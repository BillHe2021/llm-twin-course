{
    "Post_0": {
        "text": "Do you want to \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb to \ud835\uddef\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa\ud835\ude00 using \ud835\uddf4\ud835\uddfc\ud835\uddfc\ud835\uddf1 \ud835\udde6\ud835\uddea\ud835\uddd8 and \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 \ud835\uddfd\ud835\uddff\ud835\uddee\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddf0\ud835\uddf2\ud835\ude00? \u2192 I am \ud835\uddf5\ud835\uddfc\ud835\ude00\ud835\ude01\ud835\uddf6\ud835\uddfb\ud835\uddf4 a \ud835\uddd9\ud835\udde5\ud835\uddd8\ud835\uddd8 \ud835\ude04\ud835\uddf2\ud835\uddef\ud835\uddf6\ud835\uddfb\ud835\uddee\ud835\uddff \ud835\uddfb\ud835\uddf2\ud835\ude05\ud835\ude01 \ud835\ude04\ud835\uddf2\ud835\uddf2\ud835\uddf8.\nI was invited by Maven to speak in their Lighting Lesson series about how to \ud835\uddd4\ud835\uddff\ud835\uddf0\ud835\uddf5\ud835\uddf6\ud835\ude01\ud835\uddf2\ud835\uddf0\ud835\ude01 \ud835\uddec\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\udde7\ud835\ude04\ud835\uddf6\ud835\uddfb. This 30-min session is for ML & MLOps engineers who want to learn:\n\ud835\udddf\ud835\udddf\ud835\udde0 \ud835\udde6\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa \ud835\uddf1\ud835\uddf2\ud835\ude00\ud835\uddf6\ud835\uddf4\ud835\uddfb \ud835\uddfc\ud835\uddf3 \ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\udde7\ud835\ude04\ud835\uddf6\ud835\uddfb\n\u2192 Using the 3-pipeline architecture & MLOps good practices\n\ud835\uddd7\ud835\uddf2\ud835\ude00\ud835\uddf6\ud835\uddf4\ud835\uddfb \ud835\uddee \ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddee \ud835\uddf0\ud835\uddfc\ud835\uddf9\ud835\uddf9\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\n\u2192 data crawling, ETLs, CDC, AWS\n\ud835\uddd7\ud835\uddf2\ud835\ude00\ud835\uddf6\ud835\uddf4\ud835\uddfb \ud835\uddee \ud835\uddf3\ud835\uddf2\ud835\uddee\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\n\u2192 streaming engine in Python, data ingestion for fine-tuning & RAG, vector DBs\n\ud835\uddd7\ud835\uddf2\ud835\ude00\ud835\uddf6\ud835\uddf4\ud835\uddfb \ud835\uddee \ud835\ude01\ud835\uddff\ud835\uddee\ud835\uddf6\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\n\u2192 create a custom dataset, fine-tuning, model registries, experiment trackers, LLM evaluation\n\ud835\uddd7\ud835\uddf2\ud835\ude00\ud835\uddf6\ud835\uddf4\ud835\uddfb \ud835\uddee\ud835\uddfb \ud835\uddf6\ud835\uddfb\ud835\uddf3\ud835\uddf2\ud835\uddff\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf2 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\n\u2192 real-time deployment, REST API, RAG, LLM monitoring\n\u2193\u2193\u2193\nJoin LIVE on \ud835\ude0d\ud835\ude33\ud835\ude2a, \ud835\ude14\ud835\ude22\ud835\ude3a 3!\n\ud835\udde5\ud835\uddf2\ud835\uddf4\ud835\uddf6\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddff \ud835\uddf5\ud835\uddf2\ud835\uddff\ud835\uddf2 (\ud835\uddf6\ud835\ude01\u2019\ud835\ude00 \ud835\uddf3\ud835\uddff\ud835\uddf2\ud835\uddf2) \u2192\nhttps://bit.ly/44eDZTY\n.\nThe presentation is based on the FREE LLM Twin course I am creating with the\nDecoding ML\nteam.\nIf you \ud835\uddf0\ud835\uddee\ud835\uddfb'\ud835\ude01 \ud835\uddf7\ud835\uddfc\ud835\uddf6\ud835\uddfb \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\ude04\ud835\uddf2\ud835\uddef\ud835\uddf6\ud835\uddfb\ud835\uddee\ud835\uddff, consider \ud835\uddf0\ud835\uddf5\ud835\uddf2\ud835\uddf0\ud835\uddf8\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddfc\ud835\ude02\ud835\ude01 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddda\ud835\uddf6\ud835\ude01\ud835\udddb\ud835\ude02\ud835\uddef \ud835\uddff\ud835\uddf2\ud835\uddfd\ud835\uddfc\ud835\ude00\ud835\uddf6\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\ude06 and \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb \ud835\uddee\ud835\ude01 \ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\uddfc\ud835\ude04\ud835\uddfb \ud835\uddfd\ud835\uddee\ud835\uddf0\ud835\uddf2:\nhttps://lnkd.in/dzat6PB6\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
        "image": "https://media.licdn.com/dms/image/D4D22AQHqyEUYvW4E0w/feedshare-shrink_800/0/1714298598504?e=1717027200&v=beta&t=vT1j19wsV930NHkBDeECFJNuOCNe15yht_HRMeRuaQ0"
    },
    "Post_1": {
        "text": "Want to know the \ud835\uddef\ud835\uddf2\ud835\ude00\ud835\ude01 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\ude03\ud835\uddf6\ud835\ude01\ud835\ude06 \ud835\ude01\ud835\uddf6\ud835\uddfd that \ud835\ude04\ud835\uddfc\ud835\uddff\ud835\uddf8\ud835\ude00 for \ud835\uddfa\ud835\uddf2? Nothing fancy... it's quite simple to implement... Here it is \u2193\n\ud835\ude13\ud835\ude26\ud835\ude22\ud835\ude33\ud835\ude2f \ud835\ude35\ud835\ude30 \ud835\ude34\ud835\ude22\ud835\ude3a \"\ud835\ude15\ud835\ude30\" \ud835\ude35\ud835\ude30 \ud835\ude35\ud835\ude29\ud835\ude2a\ud835\ude2f\ud835\ude28\ud835\ude34 \ud835\ude2f\ud835\ude30\ud835\ude35 \ud835\ude30\ud835\ude27 \ud835\ude35\ud835\ude29\ud835\ude26 \ud835\ude29\ud835\ude2a\ud835\ude28\ud835\ude29\ud835\ude26\ud835\ude34\ud835\ude35 \ud835\ude2a\ud835\ude2e\ud835\ude31\ud835\ude30\ud835\ude33\ud835\ude35\ud835\ude22\ud835\ude2f\ud835\ude24\ud835\ude26 \ud835\ude35\ud835\ude30 \ud835\ude3a\ud835\ude30\ud835\ude36.\nEven simple things (meetings, reminders, notes, calls, etc.) add a lot of administrative overhead that doesn't add much value in the long run.\n\ud835\udde6\ud835\uddfc \ud835\uddf5\ud835\uddfc\ud835\ude04 \ud835\uddf1\ud835\uddfc \ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\uddf3\ud835\uddf6\ud835\uddf9\ud835\ude01\ud835\uddf2\ud835\uddff \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddec\ud835\uddf2\ud835\ude00'\ud835\ude00 \ud835\uddf3\ud835\uddff\ud835\uddfc\ud835\uddfa \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\udde1\ud835\uddfc'\ud835\ude00?\nTo do that, you must clearly define what is essential for you, as life will throw at you many \"temptations\" that seem important at the moment but are just distractions in the long run.\nI cannot tell you what is important to you...\n...but I have a simple formula that helps me navigate my process of decision-making:\n\ud835\ude10 \ud835\ude35\ud835\ude33\ud835\ude3a \ud835\ude35\ud835\ude30 \ud835\ude30\ud835\ude31\ud835\ude35\ud835\ude2a\ud835\ude2e\ud835\ude2a\ud835\ude3b\ud835\ude26 \ud835\ude35\ud835\ude29\ud835\ude26 \ud835\ude27\ud835\ude30\ud835\ude2d\ud835\ude2d\ud835\ude30\ud835\ude38\ud835\ude2a\ud835\ude2f\ud835\ude28:\n(\ud835\uddff\ud835\uddf2\ud835\ude04\ud835\uddee\ud835\uddff\ud835\uddf1 + \ud835\uddf0\ud835\ude02\ud835\uddff\ud835\uddf6\ud835\uddfc\ud835\ude00\ud835\uddf6\ud835\ude01\ud835\ude06) / \ud835\uddf2\ud835\uddf3\ud835\uddf3\ud835\uddfc\ud835\uddff\ud835\ude01\n\ud83d\udcad this is the ML engineer inside me...\n\ud835\ude1e\ud835\ude29\ud835\ude22\ud835\ude35 \ud835\ude25\ud835\ude30 \ud835\ude10 \ud835\ude25\ud835\ude30 \ud835\ude38\ud835\ude2a\ud835\ude35\ud835\ude29 \ud835\ude2a\ud835\ude35?\n\u2192 I sort everything based on it and pick the top 2 or 3 things (always to have the eyes on the prize).\nI will move forward to the next ones when I am done with them.\n\ud835\udde1\ud835\uddfc\ud835\ude01\ud835\uddf2: This mainly works for tangible tasks that must be executed soon (from 1 to 6 months). The big goals of your life are a different story.\nThat's it!\n\u2192 This works for everything, from learning new things to leveling up your career or building something extraordinary.\n.\nHow do you pick what is important to you or not? I would love to hear your strategy in the comments \u2193\u2193\u2193\nhashtag\n#\nproductivity\nhashtag\n#\npersonaldevelopment\nhashtag\n#\nmentalhealth",
        "image": "https://media.licdn.com/dms/image/D4D22AQEYVV5sa3VTTA/feedshare-shrink_800/0/1714284196823?e=1717027200&v=beta&t=OtnbqgncFBq7qKaXUTxCBg15wC8fk7hkKvTOzajkvt8"
    },
    "Post_2": {
        "text": "I am so excited about completely delegating my chores and writing more crappy posts about LLMs.\nhashtag\n#\nmachinelearning\nhashtag\n#\nartificialintelligence\nhashtag\n#\nautomation",
        "image": "https://media.licdn.com/dms/image/D4D22AQGgtwQzantsGg/feedshare-shrink_800/0/1714204831385?e=1717027200&v=beta&t=8DAYsLeEE29gTAbaninsDC2is8T-Up3PyMnZ0HrQxls"
    },
    "Post_3": {
        "text": "The robots are coming. \ud83d\udd25\nIf you\u2019re not yet convinced that general purpose robot helpers will be normal in the not too distant future, I highly recommend this short demo.\nIt\u2019s called \u2018Astribot\u2019, and it was just released by a Chinese research company. A lot is still unknown about their work, but the video really brings to life the incredible potential as robotic assistants come closer to the tipping points required to take off.\nThe race to develop general purpose robots has been heating up for years, but still with major limitations on speed, dexterity, and complexity of tasks.\n\u2026 but this is changing.\nThe continued convergence of advanced robotics, computer vision, and AIs with multi-model understanding will undoubtedly unlock a new global industry.\nNot if. When.\nAmazon already has close to a million robotic unit deployed in their warehouses (up from zero just 12 years ago!), and a simple use case like robotic lawn mowers is already a multi billion-dollar industry on its own.\nImagine when the industry unlocks true general purpose at a reasonable price point.\nHow quickly will it then become normal to have a robot helper in your house?\nIt\u2019s still early days, with many hurdles still to overcome.\n\u2026 but they\u2019re coming.\nFascinating future ahead.",
        "image": "https://media.licdn.com/dms/image/D4E22AQE_KUa86YBD4Q/feedshare-shrink_800/0/1714148560364?e=1717027200&v=beta&t=-YUgaoCrbaWGzYskkL7PvRtpVdy4fOomkM8guSnE7f0"
    },
    "Post_4": {
        "text": "Decoding ML\n\ud835\uddff\ud835\uddf2\ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\uddf1 an \ud835\uddee\ud835\uddff\ud835\ude01\ud835\uddf6\ud835\uddf0\ud835\uddf9\ud835\uddf2 & \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2 on building a \ud835\udde5\ud835\uddf2\ud835\uddee\ud835\uddf9-\ud835\ude01\ud835\uddf6\ud835\uddfa\ud835\uddf2 \ud835\udde1\ud835\uddf2\ud835\ude04\ud835\ude00 \ud835\udde6\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddf0\ud835\uddf5 \ud835\uddd8\ud835\uddfb\ud835\uddf4\ud835\uddf6\ud835\uddfb\ud835\uddf2 using \ud835\uddde\ud835\uddee\ud835\uddf3\ud835\uddf8\ud835\uddee, \ud835\udde9\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddfc\ud835\uddff \ud835\uddd7\ud835\uddd5\ud835\ude00 and \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf2\ud835\uddfb\ud835\uddf4\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\ude00.\n\ud835\ude0c\ud835\ude37\ud835\ude26\ud835\ude33\ud835\ude3a\ud835\ude35\ud835\ude29\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude2a\ud835\ude2f \ud835\ude17\ud835\ude3a\ud835\ude35\ud835\ude29\ud835\ude30\ud835\ude2f!\n\ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddf2\ud835\uddfb\ud835\uddf1 \ud835\uddf4\ud835\uddfc\ud835\uddee\ud835\uddf9?\nLearn to build a production-ready semantic search engine for news that is synced in real-time with multiple news sources using:\n- a streaming engine\n- Kafka\n- a vector DB.\n\ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddef\ud835\uddf9\ud835\uddf2\ud835\uddfa?\nAccording to a research study by\nearthweb.com\n, the daily influx of news articles, both online and offline, is between 2 and 3 million.\nHow would you constantly sync these data sources with your vector DB to stay in sync with the outside world?\n\ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\ude00\ud835\uddfc\ud835\uddf9\ud835\ude02\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb!\n\u2192 Here is where the streaming pipeline kicks in.\nAs soon as a new data point is available, it is:\n- ingested\n- processed\n- loaded to a vector DB\n...in real-time by the streaming pipeline \u2190\n.\n\ud835\ude0f\ud835\ude26\ud835\ude33\ud835\ude26 \ud835\ude2a\ud835\ude34 \ud835\ude38\ud835\ude29\ud835\ude22\ud835\ude35 \ud835\ude3a\ud835\ude30\ud835\ude36 \ud835\ude38\ud835\ude2a\ud835\ude2d\ud835\ude2d \ud835\ude2d\ud835\ude26\ud835\ude22\ud835\ude33\ud835\ude2f \ud835\ude27\ud835\ude33\ud835\ude30\ud835\ude2e \ud835\ude35\ud835\ude29\ud835\ude26 \ud835\ude22\ud835\ude33\ud835\ude35\ud835\ude2a\ud835\ude24\ud835\ude2d\ud835\ude26 \u2193\n\u2192 Set up your own\nUpstash\n\ud835\uddde\ud835\uddee\ud835\uddf3\ud835\uddf8\ud835\uddee & \ud835\udde9\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddfc\ud835\uddff \ud835\uddd7\ud835\uddd5 \ud835\uddf0\ud835\uddf9\ud835\ude02\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddff\ud835\ude00\n\u2192 \ud835\udde6\ud835\ude01\ud835\uddff\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2 & \ud835\ude03\ud835\uddee\ud835\uddf9\ud835\uddf6\ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddf2 your \ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddee points using Pydantic\n\u2192 \ud835\udde6\ud835\uddf6\ud835\uddfa\ud835\ude02\ud835\uddf9\ud835\uddee\ud835\ude01\ud835\uddf2 multiple \ud835\uddde\ud835\uddee\ud835\uddf3\ud835\uddf8\ud835\uddee \ud835\uddd6\ud835\uddf9\ud835\uddf6\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\ude00 using \ud835\ude1b\ud835\ude29\ud835\ude33\ud835\ude26\ud835\ude22\ud835\ude25\ud835\ude17\ud835\ude30\ud835\ude30\ud835\ude2d\ud835\ude0c\ud835\ude39\ud835\ude26\ud835\ude24\ud835\ude36\ud835\ude35\ud835\ude30\ud835\ude33 & \ud835\ude12\ud835\ude22\ud835\ude27\ud835\ude2c\ud835\ude22\ud835\ude17\ud835\ude33\ud835\ude30\ud835\ude25\ud835\ude36\ud835\ude24\ud835\ude26\ud835\ude33\n\u2192 \ud835\udde6\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf0\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf4 using\nBytewax\n- learn to \ud835\uddef\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1 \ud835\uddee \ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf9-\ud835\ude01\ud835\uddf6\ud835\uddfa\ud835\uddf2 \ud835\udde5\ud835\uddd4\ud835\uddda ingestion \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\n\u2192 \ud835\uddd5\ud835\uddee\ud835\ude01\ud835\uddf0\ud835\uddf5-\ud835\ude02\ud835\uddfd\ud835\ude00\ud835\uddf2\ud835\uddff\ud835\ude01\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf2\ud835\uddfa\ud835\uddef\ud835\uddf2\ud835\uddf1\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4\ud835\ude00 + \ud835\uddfa\ud835\uddf2\ud835\ude01\ud835\uddee\ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddee to Upstash Vector DB\n\u2192 Build a \ud835\udde4&\ud835\uddd4 \ud835\udde8I using\nStreamlit\n\u2192 \ud835\udde8\ud835\uddfb\ud835\uddf6\ud835\ude01 \ud835\udde7\ud835\uddf2\ud835\ude00\ud835\ude01\ud835\uddf6\ud835\uddfb\ud835\uddf4  - Yes, we even added unit testing!\n.\n\ud835\uddd6\ud835\ude02\ud835\uddff\ud835\uddf6\ud835\uddfc\ud835\ude02\ud835\ude00 \ud835\ude01\ud835\uddfc \ud835\uddf9\ud835\uddf2\ud835\ude03\ud835\uddf2\ud835\uddf9 \ud835\ude02\ud835\uddfd \ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\udde3\ud835\ude06\ud835\ude01\ud835\uddf5\ud835\uddfc\ud835\uddfb, \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 & \ud835\udde5\ud835\uddd4\ud835\uddda \ud835\uddf4\ud835\uddee\ud835\uddfa\ud835\uddf2 \ud83e\udef5\n\ud835\ude0a\ud835\ude29\ud835\ude26\ud835\ude24\ud835\ude2c \ud835\ude30\ud835\ude36\ud835\ude35 \ud835\ude35\ud835\ude29\ud835\ude26 \ud835\ude22\ud835\ude33\ud835\ude35\ud835\ude2a\ud835\ude24\ud835\ude2d\ud835\ude26 & \ud835\ude24\ud835\ude30\ud835\ude25\ud835\ude26. \ud835\ude10\ud835\ude34 \ud835\ude0d\ud835\ude19\ud835\ude0c\ud835\ude0c\n\u2193\u2193\u2193\n\ud83d\udd17 \ud835\uddd4\ud835\uddff\ud835\ude01\ud835\uddf6\ud835\uddf0\ud835\uddf9\ud835\uddf2:\nhttps://lnkd.in/dWW9m8ap\n\ud83d\udd17 \ud835\uddda\ud835\uddf6\ud835\ude01\ud835\udddb\ud835\ude02\ud835\uddef \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2:\nhttps://lnkd.in/dsaMb9Qk\n.\n\ud83d\udc40 Hey,\nThe post is not finished yet,\n\ud835\uddd5\ud835\uddf2 \ud835\ude00\ud835\ude02\ud835\uddff\ud835\uddf2 \ud835\ude01\ud835\uddfc \ud835\ude01\ud835\uddff\ud835\ude06 \ud835\uddfc\ud835\ude02\ud835\ude01 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2. \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f\n\u2192 It has a beautiful README & Makefile with step-by-step instructions.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily content on production ML and MLOps engineering.",
        "image": "https://media.licdn.com/dms/image/D4D22AQFYNUaoosc4Qw/feedshare-shrink_800/0/1713340994642?e=1717027200&v=beta&t=G8KA8onOlMiGfHmVEj9jO5HeDMdpmycY5NYRu_Fs2wA"
    },
    "Post_5": {
        "text": "It was also about time to add a \u201cdata card\u201d along with the \u201cmodel card\u201d that explains the data your model was trained on.\nSo excited to work at a company that innovates in the field of safety AI.\nhashtag\n#\nmachinelearning\nhashtag\n#\nartificialintelligence",
        "image": "https://media.licdn.com/dms/image/D4D22AQEi8_PB-N8SUg/feedshare-shrink_800/0/1712995216934?e=1717027200&v=beta&t=yLKrnBtFIUdxBd3tvZy-Hsbt4S8KRCThmyjcAS7V6EQ"
    },
    "Post_6": {
        "text": "Ethical and responsible innovation is paramount, especially when it comes to protecting the most vulnerable members of society. As we embrace the potential of AI, we have a vital responsibility to ensure our technologies are a force for good. That is why Metaphysic has joined forces with\nThorn\n, a nonprofit dedicated to enacting strong child safety commitments for generative AI. We've partnered with the likes of\nMicrosoft\n,\nGoogle\n, and\nOpenAI\nand together we've committed to implementing robust child safety measures throughout the development, deployment, and maintenance of generative AI technologies.\nMetaphysic, with its dedication to advancing AI research ethically and responsibly, has been an instrumental partner in its commitment to developing and designing a new type of data documentation called a \"data card.\" An innovative and groundbreaking tool, a data card is a structured form that developers must complete, detailing specific aspects of their AI models or datasets. This particular data card, the protocol used by our partners for this initiative, will require developers to provide information about how data related to minors is utilized and the measures taken to prevent its non-consensual processing.\nThis collective action underscores the tech industry's shared commitment to ethical AI development and safeguarding child welfare. We're honored that we stand alongside Thorn in setting new standards for responsible AI.\nhashtag\n#\nMetaphysic\nhashtag\n#\nGenAI\nhashtag\n#\nGenerativeAI\nhashtag\n#\nThorn\nLearn more here:\nhttps://lnkd.in/eXxgJfNv",
        "image": "https://media.licdn.com/dms/image/D4E22AQEb-Lb0RndhVQ/feedshare-shrink_800/0/1712822408881?e=1717027200&v=beta&t=IzCd5T16YL360v7H9cM_uK9-NbmkRopAbBYNn4Fm9ys"
    },
    "Post_7": {
        "text": "This is how I FAILED to \ud835\uddfc\ud835\uddfd\ud835\ude01\ud835\uddf6\ud835\uddfa\ud835\uddf6\ud835\ude07\ud835\uddf2 the \ud835\uddf6\ud835\uddfb\ud835\uddf3\ud835\uddf2\ud835\uddff\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf2 of my \ud835\uddd7\ud835\udddf \ud835\uddfa\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9\ud835\ude00 when \ud835\uddff\ud835\ude02\ud835\uddfb\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\ude01\ud835\uddf5\ud835\uddf2\ud835\uddfa on a \ud835\udde1\ud835\ude03\ud835\uddf6\ud835\uddf1\ud835\uddf6\ud835\uddee \ud835\uddda\ud835\udde3\ud835\udde8. Let me tell you \ud835\ude04\ud835\uddf5\ud835\uddee\ud835\ude01 \ud835\ude01\ud835\uddfc \ud835\uddee\ud835\ude03\ud835\uddfc\ud835\uddf6\ud835\uddf1 \u2193\nI had a simple task. To reduce the latency of the DL models used in production.\nWe had 4 DL models that were running on Nvidia GPUs.\nAfter a first look at the inference code, I saw that the inputs to the models weren't batched.\nWe were processing one sample at a time.\nI said to myself: \"Ahaa! That's it. I cracked it. We just have to batch as many samples as possible, and we are done.\"\nSo, I did just that...\nAfter 2-3 days of work adding the extra batch dimension to the PyTorch preprocessing & postprocessing code, \ud835\udddc \ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf9\ud835\uddf6\ud835\ude07\ud835\uddf2\ud835\uddf1 \ud835\udddc \ud835\uddea\ud835\uddd4\ud835\udde6 \ud835\uddea\ud835\udde5\ud835\udde2\ud835\udde1\ud835\uddda.\n\ud835\udddb\ud835\uddf2\ud835\uddff\ud835\uddf2 \ud835\uddf6\ud835\ude00 \ud835\ude04\ud835\uddf5\ud835\ude06\n\u2193\u2193\u2193\nWe were using Nvidia GPUs from the A family (A6000, A5000, etc.).\nAs these GPUs have a lot of memory  (>40GB), I managed to max out the VRAM and squash a batch of 256 images on the GPU.\nRelative to using a \"\ud835\ude23\ud835\ude22\ud835\ude35\ud835\ude24\ud835\ude29 = 1\" it was faster, but not A LOT FASTER, as I expected.\nThen I tried batches of 128, 64, 32, 16, and 8.\n...and realized that everything > batch = 16 was running slower than using a batch of 16.\n\u2192 \ud835\uddd4 \ud835\uddef\ud835\uddee\ud835\ude01\ud835\uddf0\ud835\uddf5 \ud835\uddfc\ud835\uddf3 \ud835\udfed\ud835\udff2 \ud835\ude04\ud835\uddee\ud835\ude00 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\ude00\ud835\ude04\ud835\uddf2\ud835\uddf2\ud835\ude01 \ud835\ude00\ud835\uddfd\ud835\uddfc\ud835\ude01.\nBut that is not good, as I was using only ~10% of the VRAM...\n\ud835\uddea\ud835\uddf5\ud835\ude06 \ud835\uddf6\ud835\ude00 \ud835\ude01\ud835\uddf5\ud835\uddee\ud835\ude01?\nThe Nvidia A family of GPUs are known to:\n- having a lot of VRAM\n- not being very fast (the memory transfer between the CPU & GPU + the number of CUDA cores isn't that great)\nThat being said, my program was throttled.\nEven if my GPU could handle much more memory-wise, the memory transfer & processing speeds weren't keeping up.\nIn the end, it was a good optimization: ~75% faster\n\ud835\uddd5\ud835\ude02\ud835\ude01 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddf9\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddfc\ud835\uddfb \ud835\uddfc\ud835\uddf3 \ud835\ude01\ud835\uddf5\ud835\uddf6\ud835\ude00 \ud835\ude00\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\ude06 \ud835\uddf6\ud835\ude00:\n\u2192 ALWAYS KNOW YOUR HARDWARE \u2190\nMost probably, running a bigger batch on an A100 or V100 wouldn't have the same problem.\nI plan to try that.\nBut that is why...\n\u2192 \ud835\ude6e\ud835\ude64\ud835\ude6a \ud835\ude56\ud835\ude61\ud835\ude6c\ud835\ude56\ud835\ude6e\ud835\ude68 \ud835\ude5d\ud835\ude56\ud835\ude6b\ud835\ude5a \ud835\ude69\ud835\ude64 \ud835\ude64\ud835\ude65\ud835\ude69\ud835\ude5e\ud835\ude62\ud835\ude5e\ud835\ude6f\ud835\ude5a \ud835\ude69\ud835\ude5d\ud835\ude5a \ud835\ude65\ud835\ude56\ud835\ude67\ud835\ude56\ud835\ude62\ud835\ude5a\ud835\ude69\ud835\ude5a\ud835\ude67\ud835\ude68 \ud835\ude64\ud835\ude5b \ud835\ude6e\ud835\ude64\ud835\ude6a\ud835\ude67 \ud835\ude68\ud835\ude6e\ud835\ude68\ud835\ude69\ud835\ude5a\ud835\ude62 \ud835\ude57\ud835\ude56\ud835\ude68\ud835\ude5a\ud835\ude59 \ud835\ude64\ud835\ude63 \ud835\ude6e\ud835\ude64\ud835\ude6a\ud835\ude67 \ud835\ude5d\ud835\ude56\ud835\ude67\ud835\ude59\ud835\ude6c\ud835\ude56\ud835\ude67\ud835\ude5a!\n.\nIn theory, I knew this, but it is completely different when you encounter it in production.\nLet me know in the comments if you want more similar stories on \"DO NOTs\" from my experience.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily content on production ML and MLOps engineering.",
        "image": "https://media.licdn.com/dms/image/D4D22AQHnJmu_-ZSEww/feedshare-shrink_800/0/1712736184913?e=1717027200&v=beta&t=k3XEV1BEKaXbdfA7-y-sqRORcwsM2WAnJYkkfOCay2E"
    },
    "Post_8": {
        "text": "\ud835\udde6\ud835\ude01\ud835\uddf2\ud835\uddfd-\ud835\uddef\ud835\ude06-\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfd \ud835\uddf4\ud835\ude02\ud835\uddf6\ud835\uddf1\ud835\uddf2 to \ud835\uddef\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 a \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa with \ud835\udff0 \ud835\udde3\ud835\ude06\ud835\ude01\ud835\uddf5\ud835\uddfc\ud835\uddfb \ud835\uddfa\ud835\uddf6\ud835\uddf0\ud835\uddff\ud835\uddfc\ud835\ude00\ud835\uddf2\ud835\uddff\ud835\ude03\ud835\uddf6\ud835\uddf0\ud835\uddf2\ud835\ude00 using \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 & the \ud835\udfef-\ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 \ud835\uddee\ud835\uddff\ud835\uddf0\ud835\uddf5\ud835\uddf6\ud835\ude01\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2\nAs an example, you will build a system for an LLM Twin.\nBut you can use the same strategy for most use cases.\n\ud835\ude0f\ud835\ude26\ud835\ude33\ud835\ude26 \ud835\ude2a\ud835\ude34 \ud835\ude35\ud835\ude29\ud835\ude26 \ud835\ude22\ud835\ude33\ud835\ude24\ud835\ude29\ud835\ude2a\ud835\ude35\ud835\ude26\ud835\ude24\ud835\ude35\ud835\ude36\ud835\ude33\ud835\ude26 \ud835\ude30\ud835\ude27 \ud835\ude30\ud835\ude36\ud835\ude33 \ud835\ude13\ud835\ude13\ud835\ude14 \ud835\ude1b\ud835\ude38\ud835\ude2a\ud835\ude2f \u2193\n\ud835\udfed. \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee \ud835\uddd6\ud835\uddfc\ud835\uddf9\ud835\uddf9\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\udde3\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 + \ud835\uddd6\ud835\uddd7\ud835\uddd6\nThe \ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddee \ud835\uddf0\ud835\uddfc\ud835\uddf9\ud835\uddf9\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 will crawl your digital data (only yours) from various platforms, such as Medium, Substack, LinkedIn, and GitHub.\n- An ETL pipeline for every platform built with \ud835\ude34\ud835\ude26\ud835\ude2d\ud835\ude26\ud835\ude2f\ud835\ude2a\ud835\ude36\ud835\ude2e to crawl the data and \ud835\ude09\ud835\ude26\ud835\ude22\ud835\ude36\ud835\ude35\ud835\ude2a\ud835\ude27\ud835\ude36\ud835\ude2d \ud835\ude1a\ud835\ude30\ud835\ude36\ud835\ude31 to parse the HTML\n- The data is loaded into a MongoDB normalized as posts, articles or code\nUsing \ud835\uddd6\ud835\uddd7\ud835\uddd6, a watcher will listen to changes made to the MongoDB.\nAn event will be added to a RabbitMQ queue during any CRUD operation.\n\ud835\udfee. \ud835\udde0\ud835\udddf \ud835\udde3\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\ude00\nHere, we will leverage the 3-pipeline architecture and split the ML system into 3 pipelines:\n\ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddf3\ud835\uddf2\ud835\uddee\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\n\u2192 A streaming ingestion pipeline using\nBytewax\nas its streaming engine\n\ud835\ude10\ud835\ude2f\ud835\ude31\ud835\ude36\ud835\ude35: queue\n\ud835\ude16\ud835\ude36\ud835\ude35\ud835\ude31\ud835\ude36\ud835\ude35:\nQdrant\n(feature store)\n- listens to the RabbitMQ queue\n- cleans the data and creates a snapshot into Qdrant used for fine-tuning\n- chunks and embeds the data for a second snapshot into Qdrant used for RAG\n\ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\ude01\ud835\uddff\ud835\uddee\ud835\uddf6\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\n\u2192 A fine-tuning pipeline using open-source LLMs from\nHugging Face\n\ud835\ude10\ud835\ude2f\ud835\ude31\ud835\ude36\ud835\ude35: Qdrant (feature store)\n\ud835\ude16\ud835\ude36\ud835\ude35\ud835\ude31\ud835\ude36\ud835\ude35: fine-tuned LLM stored in\nComet\n's Model registry\n- uses the retrieval client to access your articles, posts, or code\n- uses ChatGPT to create a custom Q&A dataset\n- fine-tunes the LLM using QLoRA\n- logs all the metrics using Comet 's model registry\n- picks the best LLM and publishes it to the model registry as the production candidate\n- a second evaluation step is performed using an LLM evaluation tool\n- if it passes, the LLM is flagged as accepted and deployed to the inference pipeline\n\ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddf6\ud835\uddfb\ud835\uddf3\ud835\uddf2\ud835\uddff\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf2 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\n-> Acts as the RAG real-time client\n\ud835\ude10\ud835\ude2f\ud835\ude31\ud835\ude36\ud835\ude35: fined-tuned LLM from the model registry\n\ud835\ude16\ud835\ude36\ud835\ude35\ud835\ude31\ud835\ude36\ud835\ude35: real-time predictions\n- loads & quantized the LLM from the model registry\n- accepts client requests using a REST API endpoint\n- augments the prompt using RAG\n- calls the LLM Twin & logs everything to a prompt monitoring tool\n.\nOk, ok, that sounds great...\n\ud835\uddd5\ud835\ude02\ud835\ude01 \ud835\ude04\ud835\uddf5\ud835\uddf2\ud835\uddff\ud835\uddf2 \ud835\uddf6\ud835\ude00 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2?\nCheck it out on\nDecoding ML\n's GitHub repository\n\u2193\u2193\u2193\n\ud83d\udd17 LLM Twin Course:\nhttps://lnkd.in/dzat6PB6\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
        "image": "https://media.licdn.com/dms/image/D4D22AQFYG2rRlVRLGA/feedshare-shrink_800/0/1712391432978?e=1717027200&v=beta&t=INLimx1OGUvma_uU1g3vwqvcly7jRNmLjlKY2rZd2y4"
    },
    "Post_9": {
        "text": "I hate customization. I want to be productive everywhere, anywhere.\niPhone > android\nMacbooks > Windows, Linux\nPyCharm > VS Code\nJoking.\nI was a PyCharm fanboy but switched to VS Code due to their superior SSH remote work integration.\nSince then, I have entirely ditched PyCharm.\nInteresting fact: VS Code doesn\u2019t need that much customization to make it work + you can sync all your settings to your account (so installing all the required extensions is just a one-time thing)\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\nhashtag\n#\nprogramming",
        "image": "https://media.licdn.com/dms/image/D4D22AQHuUEnqoD5sqQ/feedshare-shrink_800/0/1712304080491?e=1717027200&v=beta&t=5ByBpzEdNXt4jQvDt1pMQB_-QgkM1h5rgO2BbmZVa5Q"
    },
    "Post_10": {
        "text": "This is how I \ud835\uddff\ud835\uddf2\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\uddf2\ud835\uddf1 the \ud835\uddf9\ud835\uddee\ud835\ude01\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\ude06 of my \ud835\udde3\ud835\ude06\ud835\udde7\ud835\uddfc\ud835\uddff\ud835\uddf0\ud835\uddf5 \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2 by \ud835\udff4\ud835\udfee% \ud835\ude02\ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddfc\ud835\uddfb\ud835\uddf9\ud835\ude06 \ud835\udde3\ud835\ude06\ud835\ude01\ud835\uddf5\ud835\uddfc\ud835\uddfb & \ud835\udde3\ud835\ude06\ud835\udde7\ud835\uddfc\ud835\uddff\ud835\uddf0\ud835\uddf5. \ud835\udde1\ud835\udde2 \ud835\uddf3\ud835\uddee\ud835\uddfb\ud835\uddf0\ud835\ude06 \ud835\ude01\ud835\uddfc\ud835\uddfc\ud835\uddf9\ud835\ude00 \ud835\uddf6\ud835\uddfb\ud835\ude03\ud835\uddfc\ud835\uddf9\ud835\ude03\ud835\uddf2\ud835\uddf1!\n\ud835\ude4f\ud835\ude5d\ud835\ude5a \ud835\ude65\ud835\ude67\ud835\ude64\ud835\ude57\ud835\ude61\ud835\ude5a\ud835\ude62?\nDuring inference, I am using 5 DL at ~25k images at once.\nThe script took around ~4 hours to run.\nThe problem is that this isn't a batch job that runs over the night...\nVarious people across the company required it to run in \"real-time\" multiple times a day.\n\ud835\ude4f\ud835\ude5d\ud835\ude5a \ud835\ude68\ud835\ude64\ud835\ude61\ud835\ude6a\ud835\ude69\ud835\ude5e\ud835\ude64\ud835\ude63?\nThe first thing that might come to your mind is to start using some fancy optimizer (e.g., TensorRT).\nEven though that should be done at some point...\nFirst, you should \ud835\uddee\ud835\ude00\ud835\uddf8 \ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00\ud835\uddf2\ud835\uddf9\ud835\uddf3:\n- I/O bottlenecks: reading & writing images\n- preprocessing & postprocessing - can it be parallelized?\n- are the CUDA cores used at their maximum potential?\n- is the bandwidth between the CPU & GPU throttled?\n- can we move more computation to the GPU?\nThat being said...\n\ud835\udddb\ud835\uddf2\ud835\uddff\ud835\uddf2 is what I did I \ud835\uddf1\ud835\uddf2\ud835\uddf0\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\uddf1 the \ud835\uddf9\ud835\uddee\ud835\ude01\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\ude06 of the script by \ud835\udff4\ud835\udfee%\n\u2193\u2193\u2193\n\ud835\udfed. \ud835\uddd5\ud835\uddee\ud835\ude01\ud835\uddf0\ud835\uddf5\ud835\uddf2\ud835\uddf1 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddf6\ud835\uddfb\ud835\uddf3\ud835\uddf2\ud835\uddff\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf2 \ud835\ude00\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00\nBatching is not only valuable for training but also mighty in speeding up your inference time.\nOtherwise, you waste your GPU CUDA cores.\nInstead of passing through the models one sample at a time, I now process 64.\n\ud835\udfee. \ud835\udddf\ud835\uddf2\ud835\ude03\ud835\uddf2\ud835\uddff\ud835\uddee\ud835\uddf4\ud835\uddf2\ud835\uddf1 \ud835\udde3\ud835\ude06\ud835\udde7\ud835\uddfc\ud835\uddff\ud835\uddf0\ud835\uddf5'\ud835\ude00 \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee\ud835\udddf\ud835\uddfc\ud835\uddee\ud835\uddf1\ud835\uddf2\ud835\uddff\nThis has 2 main advantages:\n- parallel data loading & preprocessing on multiple processes (NOT threads)\n- copying your input images directly into the pinned memory (avoid a CPU -> CPU copy operation)\n\ud835\udfef. \ud835\udde0\ud835\uddfc\ud835\ude03\ud835\uddf2\ud835\uddf1 \ud835\uddee\ud835\ude00 \ud835\uddfa\ud835\ude02\ud835\uddf0\ud835\uddf5 \ud835\uddfc\ud835\uddf3 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddfd\ud835\uddfc\ud835\ude00\ud835\ude01\ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf0\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddfc\ud835\uddfb \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddda\ud835\udde3\ud835\udde8\nI saw that the tensor was moved too early on the CPU and mapped to a NumPy array.\nI refactored the code to keep it on the GPU as much as possible, which had 2 main advantages:\n- tensors are processed faster on the GPU\n- at the end of the logic, I had smaller tensors, resulting in smaller transfers between the CPU & GPU\n\ud835\udff0. \ud835\udde0\ud835\ude02\ud835\uddf9\ud835\ude01\ud835\uddf6\ud835\ude01\ud835\uddf5\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf3\ud835\uddfc\ud835\uddff \ud835\uddee\ud835\uddf9\ud835\uddf9 \ud835\uddfa\ud835\ude06 \ud835\udddc/\ud835\udde2 \ud835\ude04\ud835\uddff\ud835\uddf6\ud835\ude01\ud835\uddf2 \ud835\uddfc\ud835\uddfd\ud835\uddf2\ud835\uddff\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb\ud835\ude00\nFor I/O bottlenecks, using Python threads is extremely powerful.\nI moved all my writes under a \ud835\ude1b\ud835\ude29\ud835\ude33\ud835\ude26\ud835\ude22\ud835\ude25\ud835\ude17\ud835\ude30\ud835\ude30\ud835\ude2d\ud835\ude0c\ud835\ude39\ud835\ude26\ud835\ude24\ud835\ude36\ud835\ude35\ud835\ude30\ud835\ude33, batching my write operations.\n.\nNote that I used only good old Python & PyTorch code.\n\u2192 When the code is poorly written, no tool can save you\nOnly now is the time to add fancy tooling, such as TensorRT.\n.\nSo remember...\nTo optimize the PyTorch code by 82%:\n1. Batched the inference samples\n2. Leveraged PyTorch's DataLoader\n3. Moved as much of the postprocessing on the GPU\n4. Multithreading for all my I/O write operations\nWhat other methods do you have in mind? Leave them in the comments \u2193\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
        "image": "https://media.licdn.com/dms/image/D4E22AQHvW4CWcsUgWw/feedshare-shrink_800/0/1711011641321?e=1717027200&v=beta&t=cBhLo_L65kfCfGERTkiaov69vXje2xRCxp_AVetxf4I"
    },
    "Post_11": {
        "text": "\ud835\uddd5\ud835\uddee\ud835\ude01\ud835\uddf0\ud835\uddf5 \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa\ud835\ude00 are the \ud835\uddfd\ud835\uddee\ud835\ude00\ud835\ude01. Here is how to \ud835\ude04\ud835\uddff\ud835\uddf6\ud835\ude01\ud835\uddf2 a \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddff\ud835\uddf2\ud835\ude01\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude03\ud835\uddee\ud835\uddf9 \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa for \ud835\udde5\ud835\uddd4\ud835\uddda on \ud835\ude00\ud835\uddfc\ud835\uddf0\ud835\uddf6\ud835\uddee\ud835\uddf9 \ud835\uddfa\ud835\uddf2\ud835\uddf1\ud835\uddf6\ud835\uddee \ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddee \u2193\n\ud835\uddea\ud835\uddf5\ud835\ude06 \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddfc\ud835\ude03\ud835\uddf2\ud835\uddff \ud835\uddef\ud835\uddee\ud835\ude01\ud835\uddf0\ud835\uddf5?\nIn environments where data evolves quickly (e.g., social media platforms), the system's response time is critical for your application's user experience.\nThat is why TikTok is so addicting. Its recommender system adapts in real-time based on your interaction with the app.\nHow would it be if the recommendations were updated daily or hourly?\nWell, it would work, but you would probably get bored of the app much faster.\nThe same applies to RAG for highly intensive data sources...\n\u2192 where you must sync your source and vector DB in real time for up-to-date retrievals.\n.\nI wrote a \ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfd-\ud835\uddef\ud835\ude06-\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfd \ud835\ude01\ud835\ude02\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\uddf6\ud835\uddee\ud835\uddf9 on \ud835\udde9\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\udddb\ud835\ude02\ud835\uddef by\nSuperlinked\n(the best place to learn about VectorDBs & RAG).\n\ud835\ude43\ud835\ude5a\ud835\ude67\ud835\ude5a \ud835\ude5e\ud835\ude68 \ud835\ude56 \ud835\ude66\ud835\ude6a\ud835\ude5e\ud835\ude58\ud835\ude60 \ud835\ude57\ud835\ude67\ud835\ude5a\ud835\ude56\ud835\ude60\ud835\ude59\ud835\ude64\ud835\ude6c\ud835\ude63.\n\u2193\u2193\u2193\nThe \ud835\uddff\ud835\uddf2\ud835\ude01\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude03\ud835\uddee\ud835\uddf9 \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa is based on \ud835\udfee \ud835\uddf1\ud835\uddf2\ud835\ude01\ud835\uddee\ud835\uddf0\ud835\uddf5\ud835\uddf2\ud835\uddf1 \ud835\uddf0\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\uddfc\ud835\uddfb\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\ude00:\n- the streaming ingestion pipeline\n- the retrieval client\nThe \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf6\ud835\uddfb\ud835\uddf4\ud835\uddf2\ud835\ude00\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 runs 24/7 to keep the vector DB synced with the current raw LinkedIn posts data source.\nThe \ud835\uddff\ud835\uddf2\ud835\ude01\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude03\ud835\uddee\ud835\uddf9 \ud835\uddf0\ud835\uddf9\ud835\uddf6\ud835\uddf2\ud835\uddfb\ud835\ude01 is used in RAG applications to query the vector DB.\n\u2192 These 2 components are completely decoupled and communicate with each other through the vector DB.\n#\ud835\udfed. \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf6\ud835\uddfb\ud835\uddf4\ud835\uddf2\ud835\ude00\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\n\u2192 Implemented in\nBytewax\n- a streaming engine built in Rust (speed& reliability) that exposes a Python interface\n\ud835\ude14\ud835\ude22\ud835\ude2a\ud835\ude2f \ud835\ude27\ud835\ude2d\ud835\ude30\ud835\ude38:\n- uses CDC to add changes from the source DB to a queue\n- listens to the queue for new events\n- cleans, chunks, and embeds the LI posts\n- loads them to a Qdrant vector DB\nand... everything in real-time!\n#\ud835\udfee. \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddff\ud835\uddf2\ud835\ude01\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude03\ud835\uddee\ud835\uddf9 \ud835\uddf0\ud835\uddf9\ud835\uddf6\ud835\uddf2\ud835\uddfb\ud835\ude01\n\u2192 A standard Python module.\nThe goal is to retrieve similar posts using a variety of query types - e.g., posts, questions, sentences.\n\ud835\ude14\ud835\ude22\ud835\ude2a\ud835\ude2f \ud835\ude27\ud835\ude2d\ud835\ude30\ud835\ude38:\n- preprocess user queries (the same way as they were ingested)\n- search the Qdrant vector DB for the most similar results\n- use rerank to improve the retrieval system's accuracy\n- visualize the results on a 2D plot using UMAP\n.\n\ud835\uddea\ud835\uddee\ud835\uddfb\ud835\ude01 \ud835\ude01\ud835\uddfc \ud835\ude00\ud835\uddf2\ud835\uddf2 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2? \ud83e\udef5\n\ud835\uddd6\ud835\uddf5\ud835\uddf2\ud835\uddf0\ud835\uddf8 \ud835\uddf6\ud835\ude01 \ud835\uddfc\ud835\ude02\ud835\ude01 on \ud835\udde9\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\udddb\ud835\ude02\ud835\uddef...\n...along a step-by-step \ud835\ude01\ud835\ude02\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\uddf6\ud835\uddee\ud835\uddf9 on how to build a full-fledged \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 using only \ud835\udde3\ud835\ude06\ud835\ude01\ud835\uddf5\ud835\uddfc\ud835\uddfb.\n\u2193\u2193\u2193\n\ud83d\udd17 \ud835\ude08 \ud835\ude19\ud835\ude26\ud835\ude22\ud835\ude2d-\ud835\ude35\ud835\ude2a\ud835\ude2e\ud835\ude26 \ud835\ude19\ud835\ude26\ud835\ude35\ud835\ude33\ud835\ude2a\ud835\ude26\ud835\ude37\ud835\ude22\ud835\ude2d \ud835\ude1a\ud835\ude3a\ud835\ude34\ud835\ude35\ud835\ude26\ud835\ude2e \ud835\ude27\ud835\ude30\ud835\ude33 \ud835\ude1a\ud835\ude30\ud835\ude24\ud835\ude2a\ud835\ude22\ud835\ude2d \ud835\ude14\ud835\ude26\ud835\ude25\ud835\ude2a\ud835\ude22 \ud835\ude0b\ud835\ude22\ud835\ude35\ud835\ude22:\nhttps://lnkd.in/dVYqj8Rd\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
        "image": "https://media.licdn.com/dms/image/D4E22AQE0BIHd106C4Q/feedshare-shrink_800/0/1710875040339?e=1717027200&v=beta&t=9kwSLhLvSxsQ1XRrTLEoIwV2LBC7lZWHWjHY_Xnyqu8"
    },
    "Post_12": {
        "text": "\ud835\uddd6\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\ude02\ud835\ude01\ud835\uddf2\ud835\uddff \ud835\ude00\ud835\uddf0\ud835\uddf6\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf2 \ud835\uddf6\ud835\ude00 \ud835\uddf1\ud835\uddf2\ud835\uddee\ud835\uddf1. Do this instead.\nIn a recent talk, Jensen Huang, CEO of Nvidia, said that kids shouldn't learn programming anymore.\nHe said that until now, most of us thought that everyone should learn to program at some point.\nBut the actual opposite is the truth.\nWith the rise of AI, nobody should have or need to learn to program anymore.\nHe highlights that with AI tools, the technology divide between non-programmers and engineers is closing.\n.\n\ud835\uddd4\ud835\ude00 \ud835\uddee\ud835\uddfb \ud835\uddf2\ud835\uddfb\ud835\uddf4\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\uddf2\ud835\uddff, \ud835\uddfa\ud835\ude06 \ud835\uddf2\ud835\uddf4\ud835\uddfc \ud835\uddf6\ud835\ude00 \ud835\uddf5\ud835\ude02\ud835\uddff\ud835\ude01; \ud835\uddfa\ud835\ude06 \ud835\uddf3\ud835\uddf6\ud835\uddff\ud835\ude00\ud835\ude01 \ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddf6\ud835\ude00 \ud835\ude01\ud835\uddfc \ud835\ude00\ud835\uddee\ud835\ude06 \ud835\uddf6\ud835\ude01 \ud835\uddf6\ud835\ude00 \ud835\ude00\ud835\ude01\ud835\ude02\ud835\uddfd\ud835\uddf6\ud835\uddf1.\nBut after thinking about it more thoroughly, I tend to agree with him.\nAfter all, even now, almost anybody can work with AI.\nThis probably won't happen in the next 10 years, but at some point, 100% will do.\nAt some point, we will ask our AI companion to write a program that does X for us or whatever.\nBut, I think this is a great thing, as it will give us more time & energy to focus on what matters, such as:\n- solving real-world problems (not just tech problems)\n- moving to the next level of technology (Bioengineering, interplanetary colonization, etc.)\n- think about the grand scheme of things\n- be more creative\n- more time to connect with our family\n- more time to take care of our\nI personally think it is a significant step for humanity.\n.\nWhat do you think?\nAs an engineer, do you see your job still present in the next 10+ years?\nHere is the full talk\n\u2193\u2193\u2193\n\ud83d\udd17 A Conversation with the Founder of NVIDIA: Who Will Shape the Future of AI?\nhttps://lnkd.in/duxK_t2C\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
        "image": "https://media.licdn.com/dms/image/D4D22AQESZ9F4YqvX0Q/feedshare-shrink_800/0/1710925388727?e=1717027200&v=beta&t=T77W0fYKYSsh_t_01RRpmZG4rhZHsQDN62V1qOBUISc"
    },
    "Post_13": {
        "text": "\ud835\udff2 \ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfd\ud835\ude00 to \ud835\uddef\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1 your \ud835\uddd4\ud835\uddea\ud835\udde6 \ud835\uddf6\ud835\uddfb\ud835\uddf3\ud835\uddff\ud835\uddee\ud835\ude00\ud835\ude01\ud835\uddff\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2 (using \ud835\udddc\ud835\uddee\ud835\uddd6) and a \ud835\uddd6\ud835\udddc/\ud835\uddd6\ud835\uddd7 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 that will \ud835\ude04\ud835\uddfc\ud835\uddff\ud835\uddf8 for \ud835\udff5\ud835\udfec% of your \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf7\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\ude00 \u2193\nWe will use the data collection pipeline from our free digital twin course as an example, but it can easily be extrapolated to most of your projects.\n\ud835\ude0d\ud835\ude2a\ud835\ude33\ud835\ude34\ud835\ude35, \ud835\ude2d\ud835\ude26\ud835\ude35'\ud835\ude34 \ud835\ude34\ud835\ude26\ud835\ude26 \ud835\ude38\ud835\ude29\ud835\ude22\ud835\ude35 \ud835\ude2a\ud835\ude34 \ud835\ude2a\ud835\ude2f \ud835\ude30\ud835\ude36\ud835\ude33 \ud835\ude35\ud835\ude30\ud835\ude30\ud835\ude2d\ud835\ude23\ud835\ude26\ud835\ude2d\ud835\ude35:\n- Docker\n- AWS ECR\n- AWS Lambda\n- MongoDB\n- Pulumni\n- GitHub Actions\n\ud835\ude1a\ud835\ude26\ud835\ude24\ud835\ude30\ud835\ude2f\ud835\ude25\ud835\ude2d\ud835\ude3a, \ud835\ude2d\ud835\ude26\ud835\ude35'\ud835\ude34 \ud835\ude32\ud835\ude36\ud835\ude2a\ud835\ude24\ud835\ude2c\ud835\ude2d\ud835\ude3a \ud835\ude36\ud835\ude2f\ud835\ude25\ud835\ude26\ud835\ude33\ud835\ude34\ud835\ude35\ud835\ude22\ud835\ude2f\ud835\ude25 \ud835\ude38\ud835\ude29\ud835\ude22\ud835\ude35 \ud835\ude35\ud835\ude29\ud835\ude26 \ud835\ude25\ud835\ude22\ud835\ude35\ud835\ude22 \ud835\ude24\ud835\ude30\ud835\ude2d\ud835\ude2d\ud835\ude26\ud835\ude24\ud835\ude35\ud835\ude2a\ud835\ude30\ud835\ude2f \ud835\ude31\ud835\ude2a\ud835\ude31\ud835\ude26\ud835\ude2d\ud835\ude2a\ud835\ude2f\ud835\ude26 \ud835\ude2a\ud835\ude34 \ud835\ude25\ud835\ude30\ud835\ude2a\ud835\ude2f\ud835\ude28\nIt automates your digital data collection from LinkedIn, Medium, Substack, and GitHub. The normalized data will be loaded into MongoDB.\n\ud835\ude15\ud835\ude30\ud835\ude38, \ud835\ude2d\ud835\ude26\ud835\ude35'\ud835\ude34 \ud835\ude36\ud835\ude2f\ud835\ude25\ud835\ude26\ud835\ude33\ud835\ude34\ud835\ude35\ud835\ude22\ud835\ude2f\ud835\ude25 \ud835\ude29\ud835\ude30\ud835\ude38 \ud835\ude35\ud835\ude29\ud835\ude26 \ud835\ude08\ud835\ude1e\ud835\ude1a \ud835\ude2a\ud835\ude2f\ud835\ude27\ud835\ude33\ud835\ude22\ud835\ude34\ud835\ude35\ud835\ude33\ud835\ude36\ud835\ude24\ud835\ude35\ud835\ude36\ud835\ude33\ud835\ude26 \ud835\ude22\ud835\ude2f\ud835\ude25 \ud835\ude0a\ud835\ude10/\ud835\ude0a\ud835\ude0b \ud835\ude31\ud835\ude2a\ud835\ude31\ud835\ude26\ud835\ude2d\ud835\ude2a\ud835\ude2f\ud835\ude26 \ud835\ude38\ud835\ude30\ud835\ude33\ud835\ude2c\ud835\ude34 \u2193\n1. We wrap the application's entry point with a `\ud835\ude29\ud835\ude22\ud835\ude2f\ud835\ude25\ud835\ude2d\ud835\ude26(\ud835\ude26\ud835\ude37\ud835\ude26\ud835\ude2f\ud835\ude35, \ud835\ude24\ud835\ude30\ud835\ude2f\ud835\ude35\ud835\ude26\ud835\ude39\ud835\ude35: \ud835\ude13\ud835\ude22\ud835\ude2e\ud835\ude23\ud835\ude25\ud835\ude22\ud835\ude0a\ud835\ude30\ud835\ude2f\ud835\ude35\ud835\ude26\ud835\ude39\ud835\ude35)` function. The AWS Lambda serverless computing service will default to the `\ud835\ude29\ud835\ude22\ud835\ude2f\ud835\ude25\ud835\ude2d\ud835\ude26()` function.\n2. Build a Docker image of your application inheriting the `\ud835\ude31\ud835\ude36\ud835\ude23\ud835\ude2d\ud835\ude2a\ud835\ude24.\ud835\ude26\ud835\ude24\ud835\ude33.\ud835\ude22\ud835\ude38\ud835\ude34/\ud835\ude2d\ud835\ude22\ud835\ude2e\ud835\ude23\ud835\ude25\ud835\ude22/\ud835\ude31\ud835\ude3a\ud835\ude35\ud835\ude29\ud835\ude30\ud835\ude2f:3.11` base Docker image\n\u2192 Now, you can quickly check your AWS Lambda function locally by making HTTP requests to your Docker container.\n3. Use Pulumni IaC to create your AWS infrastructure programmatically:\n- an ECR as your Docker registry\n- an AWS Lambda service\n- a MongoDB cluster\n- the VPC for the whole infrastructure\n4. Now that we have our Docker image and infrastructure, we can build our CI/CD pipeline using GitHub Actions. The first step is to build the Docker image inside the CI and push it to ECR when a new PR is merged into the main branch.\n5. On the CD part, we will take the fresh Docker image from ECR and deploy it to AWS Lambda.\n6. Repeat the same logic with the Pulumni code \u2192 Add a CD GitHub Action that updates the infrastructure whenever the IaC changes.\nWith \ud835\ude01\ud835\uddf5\ud835\uddf6\ud835\ude00 \ud835\uddf3\ud835\uddf9\ud835\uddfc\ud835\ude04, you will do fine for \ud835\udff5\ud835\udfec% of your \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf7\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\ude00 \ud83d\udd25\n.\n\ud835\ude1b\ud835\ude30 \ud835\ude34\ud835\ude36\ud835\ude2e\ud835\ude2e\ud835\ude22\ud835\ude33\ud835\ude2a\ud835\ude3b\ud835\ude26, \ud835\ude35\ud835\ude29\ud835\ude26 \ud835\ude0a\ud835\ude10/\ud835\ude0a\ud835\ude0b \ud835\ude38\ud835\ude2a\ud835\ude2d\ud835\ude2d \ud835\ude2d\ud835\ude30\ud835\ude30\ud835\ude2c \ud835\ude2d\ud835\ude2a\ud835\ude2c\ud835\ude26 \ud835\ude35\ud835\ude29\ud835\ude2a\ud835\ude34:\nfeature PR -> merged to main -> build Docker image -> push to ECR -> deploy to AWS Lambda\n.\n\ud835\uddea\ud835\uddee\ud835\uddfb\ud835\ude01 \ud835\ude01\ud835\uddfc \ud835\uddff\ud835\ude02\ud835\uddfb \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2 \ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00\ud835\uddf2\ud835\uddf9\ud835\uddf3?\nCheck out \ud835\udddf\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddfc\ud835\uddfb \ud835\udfee from the FREE \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\udde7\ud835\ude04\ud835\uddf6\ud835\uddfb \ud835\uddf0\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00\ud835\uddf2 hosted by @company_decoding-ml\nIt contains a \ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfd-\ud835\uddef\ud835\ude06-\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfd \ud835\uddee\ud835\uddff\ud835\ude01\ud835\uddf6\ud835\uddf0\ud835\uddf9\ud835\uddf2 & \ud835\uddfc\ud835\uddfd\ud835\uddf2\ud835\uddfb-\ud835\ude00\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\uddf0\ud835\uddf2 \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2.\n\u2193\u2193\u2193\n\ud83d\udd17 \ud835\ude1b\ud835\ude29\ud835\ude26 \ud835\ude10\ud835\ude2e\ud835\ude31\ud835\ude30\ud835\ude33\ud835\ude35\ud835\ude22\ud835\ude2f\ud835\ude24\ud835\ude26 \ud835\ude30\ud835\ude27 \ud835\ude0b\ud835\ude22\ud835\ude35\ud835\ude22 \ud835\ude17\ud835\ude2a\ud835\ude31\ud835\ude26\ud835\ude2d\ud835\ude2a\ud835\ude2f\ud835\ude26\ud835\ude34 \ud835\ude2a\ud835\ude2f \ud835\ude35\ud835\ude29\ud835\ude26 \ud835\ude0c\ud835\ude33\ud835\ude22 \ud835\ude30\ud835\ude27 \ud835\ude0e\ud835\ude26\ud835\ude2f\ud835\ude26\ud835\ude33\ud835\ude22\ud835\ude35\ud835\ude2a\ud835\ude37\ud835\ude26 \ud835\ude08\ud835\ude10:\nhttps://lnkd.in/e4ssuhnM\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
        "image": "https://media.licdn.com/dms/image/D4D22AQFs2BdfvBnGBA/feedshare-shrink_2048_1536/0/1710666130386?e=1717027200&v=beta&t=gbLuU5ZwuiLj1n5Q3TnAVaoGLUFBKVqNPuoYkff74VI"
    },
    "Post_14": {
        "text": "What is the \ud835\uddef\ud835\uddf2\ud835\ude00\ud835\ude01 \ud835\ude01\ud835\uddfc\ud835\uddfc\ud835\uddf9 to \ud835\uddfc\ud835\uddff\ud835\uddf4\ud835\uddee\ud835\uddfb\ud835\uddf6\ud835\ude07\ud835\uddf2 and \ud835\uddf9\ud835\uddfc\ud835\uddee\ud835\uddf1 your \ud835\udde0\ud835\udddf \ud835\uddf0\ud835\uddfc\ud835\uddfb\ud835\uddf3\ud835\uddf6\ud835\uddf4\ud835\ude02\ud835\uddff\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddf3\ud835\uddf6\ud835\uddf9\ud835\uddf2\ud835\ude00? \ud835\udddb\ud835\ude06\ud835\uddf1\ud835\uddff\ud835\uddee \ud83d\udc09  \u2192 Here is a \ud835\uddfe\ud835\ude02\ud835\uddf6\ud835\uddf0\ud835\uddf8 \ud835\uddf4\ud835\ude02\ud835\uddf6\ud835\uddf1\ud835\uddf2 to \ud835\uddf4\ud835\uddf2\ud835\ude01 \ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\ude00\ud835\ude01\ud835\uddee\ud835\uddff\ud835\ude01\ud835\uddf2\ud835\uddf1.\n\ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddef\ud835\uddf9\ud835\uddf2\ud835\uddfa?\nThe bigger the ML projects get, the harder it gets to keep them organized.\nThat's where the magic of proper configuration management comes in.\nConfiguration files are present in every stage of your ML pipelines:\n- data\n- features\n- training\n- inference\nManaging complex configurations is a must-have rather than a nice-to-have.\nFor complex projects, it is as critical as versioning your data or models.\n\ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\ude00\ud835\uddfc\ud835\uddf9\ud835\ude02\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb?\n\u2192 \ud835\ude43\ud835\ude6e\ud835\ude59\ud835\ude67\ud835\ude56\n.\nAlexandru Razvant\ncompiled \ud835\uddee \ud835\uddfe\ud835\ude02\ud835\uddf6\ud835\uddf0\ud835\uddf8 \ud835\uddf4\ud835\ude02\ud835\uddf6\ud835\uddf1\ud835\uddf2 for you on\nDecoding ML\nthat \ud835\ude04\ud835\uddf6\ud835\uddf9\ud835\uddf9 \ud835\uddf4\ud835\uddf2\ud835\ude01 \ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\ude00\ud835\ude01\ud835\uddee\ud835\uddff\ud835\ude01\ud835\uddf2\ud835\uddf1 with \ud835\udddb\ud835\ude06\ud835\uddf1\ud835\uddff\ud835\uddee on \ud835\uddf5\ud835\uddfc\ud835\ude04 to:\n- structure your project to use it\n- define your config files\n- use it in your Python code\n- leverage the flexibility of Hydra\n\ud835\ude0c\ud835\ude37\ud835\ude26\ud835\ude33\ud835\ude3a\ud835\ude35\ud835\ude29\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude3a\ud835\ude30\ud835\ude36 \ud835\ude2f\ud835\ude26\ud835\ude26\ud835\ude25 \ud835\ude35\ud835\ude30 \ud835\ude34\ud835\ude35\ud835\ude22\ud835\ude33\ud835\ude35 \ud835\ude35\ud835\ude33\ud835\ude3a\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude2a\ud835\ude35 \ud835\ude3a\ud835\ude30\ud835\ude36\ud835\ude33\ud835\ude34\ud835\ude26\ud835\ude2d\ud835\ude27 \ud835\ude2a\ud835\ude2f \ud835\ude3a\ud835\ude30\ud835\ude36\ud835\ude33 \ud835\ude31\ud835\ude33\ud835\ude30\ud835\ude2b\ud835\ude26\ud835\ude24\ud835\ude35\ud835\ude34  \ud83e\udef5\n.\nTo find out more...\n\ud835\uddd6\ud835\uddf5\ud835\uddf2\ud835\uddf0\ud835\uddf8 \ud835\uddf6\ud835\ude01 \ud835\uddfc\ud835\ude02\ud835\ude01\n\u2193\u2193\u2193\n\ud83d\udd17 Mastering ML Configurations by leveraging OmegaConf and Hydra:\nhttps://lnkd.in/dKkE9hPD\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
        "image": "https://media.licdn.com/dms/image/D4D22AQHM5CBDO-yQsw/feedshare-shrink_800/0/1710579619175?e=1717027200&v=beta&t=pkG6S-B3R9JHuxmi6oHqBZ3syagtZuYtMGrF7hv3iDc"
    },
    "Post_15": {
        "text": "\ud835\udc07\ud835\udc28\ud835\udc30 can you \ud835\udc1b\ud835\udc2e\ud835\udc22\ud835\udc25\ud835\udc1d & \ud835\udc1d\ud835\udc1e\ud835\udc29\ud835\udc25\ud835\udc28\ud835\udc32 an \ud835\udc22\ud835\udc27\ud835\udc1f\ud835\udc1e\ud835\udc2b\ud835\udc1e\ud835\udc27\ud835\udc1c\ud835\udc1e \ud835\udc29\ud835\udc22\ud835\udc29\ud835\udc1e\ud835\udc25\ud835\udc22\ud835\udc27\ud835\udc1e for a real-time financial advisor with \ud835\udc0b\ud835\udc1a\ud835\udc27\ud835\udc20\ud835\udc02\ud835\udc21\ud835\udc1a\ud835\udc22\ud835\udc27 powered by \ud835\udc0b\ud835\udc0b\ud835\udc0c\ud835\udc2c & \ud835\udc2f\ud835\udc1e\ud835\udc1c\ud835\udc2d\ud835\udc28\ud835\udc2b \ud835\udc03\ud835\udc01\ud835\udc2c while considering \ud835\udc20\ud835\udc28\ud835\udc28\ud835\udc1d \ud835\udc0b\ud835\udc0b\ud835\udc0c\ud835\udc0e\ud835\udc29\ud835\udc2c \ud835\udc29\ud835\udc2b\ud835\udc1a\ud835\udc1c\ud835\udc2d\ud835\udc22\ud835\udc1c\ud835\udc1e\ud835\udc2c?\n.\nAs a quick reminder from previous posts, here is what we already have:\n- a\nQdrant\nvector DB populated with financial news (the output of the feature pipeline)\n- fine-tuned Falcon-7B LoRA weights stored in Comet's model registry (the output of the training pipeline)\nThe\nQdrant\nvectorDB is accessed through a Python client.\nA specific version of the Falcon-7B LoRA weights is downloaded from Comet's model registry and loaded in memory using QLoRA.\nThe goal of the inference pipeline is to use LangChain to glue the 2 components into a single `FinancialAssistant` entity.\n.\nThe `FinancialAssistant` entity is deployed in a request-response fashion under a RESTful API. We used Beam to deploy it quickly under a serverless web endpoint.\nTo deploy any model using Beam as a RESTful API is as easy as writing the following Python decorator:\n```\n@financial_bot. rest_api(keep_warm_seconds=300, loader=load_bot)\ndef run(**inputs):\n....\n```\n\u21b3\ud83d\udd17 Beam:\nhttps://lnkd.in/esEM24xG\n\ud835\udc0d\ud835\udc28\ud835\udc30 \ud835\udc25\ud835\udc1e\ud835\udc2d'\ud835\udc2c \ud835\udc2e\ud835\udc27\ud835\udc1d\ud835\udc1e\ud835\udc2b\ud835\udc2c\ud835\udc2d\ud835\udc1a\ud835\udc27\ud835\udc1d \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc1f\ud835\udc25\ud835\udc28\ud835\udc30 \ud835\udc28\ud835\udc1f \ud835\udc2d\ud835\udc21\ud835\udc1e `\ud835\udc05\ud835\udc22\ud835\udc27\ud835\udc1a\ud835\udc27\ud835\udc1c\ud835\udc22\ud835\udc1a\ud835\udc25\ud835\udc00\ud835\udc2c\ud835\udc2c\ud835\udc22\ud835\udc2c\ud835\udc2d\ud835\udc1a\ud835\udc27\ud835\udc2d` \ud835\udc1c\ud835\udc21\ud835\udc1a\ud835\udc22\ud835\udc27\u2193\n1. Clean the user's input prompt and use a pre-trained \"all-MiniLM-L6-v2\" encoder-only model to embed it (the same LM used to populate the vector DB).\n2. Using the embedded user input, query the Qdrant vector DB and extract the top 3 most similar financial news based on the cosine similarly distance\nThese 2 steps were necessary to do RAG.\n3. Build the final prompt using a \"PromptTemplate\" class (the same one used for training) that formats the following components:\n- a system prompt\n- the user's input prompt\n- the financial news context\n- the chat history\n4. Now that our prompt contains all the necessary data, we pass it to the fine-tuned Falcon-7B LLM for the final answer.\nThe input prompt and LLM answer will be logged and monitored by Comet LLMOps.\n5. You can get the answer in one shot or use the `TextIteratorStreamer` class (from HuggingFace) to stream it token-by-token.\n6. Store the user's input prompt and LLM answer in the chat history.\n7. Pass the final answer to the client.\nNote: You can use the `TextIteratorStreamer` class & wrap the `FinancialAssistant` under a WebSocket (instead of the RESTful API) to stream the answer of the bot token by token.\nSimilar to ChatGPT.\n\u21b3 To see the full code, check out our \"Hands-on LLMs\" FREE course & support us with a \u2b50: \ud83d\udd17\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
        "image": "https://media.licdn.com/dms/image/D4D22AQFbl4XRmDjGFw/feedshare-shrink_800/0/1710493275386?e=1717027200&v=beta&t=5GCJUYJu2zLQ8FO0QAAnSruww7OLqcBXpb_IyxILQIU"
    },
    "Post_16": {
        "text": "So excited to work with such an innovative company in the world of AI \ud83d\udd25\nhashtag\n#\nai\nhashtag\n#\ndatascience\nhashtag\n#\nmachinelearning",
        "image": "https://media.licdn.com/dms/image/D4E22AQEKbam-Qfei-Q/feedshare-shrink_800/0/1710406827668?e=1717027200&v=beta&t=E87O_Qc-lpJcEVn97BSLwuKrW5SR9eHoyuucbvuZe80"
    },
    "Post_17": {
        "text": "Today we're proud to announce that we have been named one of\nFast Company\n's Most Innovative Companies of 2024.\nWe are recognized for providing a revolutionary safe space, empowering users to own and control their digital identities. Our work ethically pushes the boundaries of what AI can do while upholding individual rights and consent.\nWe believe innovation is about more than just the latest features. It's about solving human problems with technology infused with creativity, vision and ethics.\nCongratulations to the entire Metaphysic team on this well-deserved honor. Their efforts to develop user-empowering AI align with our overall commitment to building a more responsible AI world.\nAs leaders, we must implement AI responsibly. The true potential of technology is realized when it enriches people's lives while safeguarding their rights.\nRead how we empower actors with a safe space for their digital selves:\nhttps://lnkd.in/eN8qqGzQ\nhashtag\n#\nmetaphysic\nhashtag\n#\nethics\nhashtag\n#\nempowerment\nhashtag\n#\ngenai\nhashtag\n#\nai",
        "image": "https://media.licdn.com/dms/image/D4D22AQHQZYxiPwvrRg/feedshare-shrink_800/0/1710320589337?e=1717027200&v=beta&t=xPSVwPg2TWEMbK8NXEg7yXmw02WYYtbLM19gl-bXfmw"
    },
    "Post_18": {
        "text": "\ud835\uddd5\ud835\uddee\ud835\ude01\ud835\uddf0\ud835\uddf5 \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa\ud835\ude00 are the \ud835\uddfd\ud835\uddee\ud835\ude00\ud835\ude01. Here is how to \ud835\ude04\ud835\uddff\ud835\uddf6\ud835\ude01\ud835\uddf2 a \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddff\ud835\uddf2\ud835\ude01\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude03\ud835\uddee\ud835\uddf9 \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa for \ud835\udde5\ud835\uddd4\ud835\uddda on \ud835\ude00\ud835\uddfc\ud835\uddf0\ud835\uddf6\ud835\uddee\ud835\uddf9 \ud835\uddfa\ud835\uddf2\ud835\uddf1\ud835\uddf6\ud835\uddee \ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddee \u2193\n\ud835\uddea\ud835\uddf5\ud835\ude06 \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddfc\ud835\ude03\ud835\uddf2\ud835\uddff \ud835\uddef\ud835\uddee\ud835\ude01\ud835\uddf0\ud835\uddf5?\nIn environments where data evolves quickly (e.g., social media platforms), the system's response time is critical for your application's user experience.\nThat is why TikTok is so addicting. Its recommender system adapts in real-time based on your interaction with the app.\nHow would it be if the recommendations were updated daily or hourly?\nWell, it would work, but you would probably get bored of the app much faster.\nThe same applies to RAG for highly intensive data sources...\n\u2192 where you must sync your source and vector DB in real time for up-to-date retrievals.\n\ud835\ude13\ud835\ude26\ud835\ude35'\ud835\ude34 \ud835\ude34\ud835\ude26\ud835\ude26 \ud835\ude29\ud835\ude30\ud835\ude38 \ud835\ude2a\ud835\ude35 \ud835\ude38\ud835\ude30\ud835\ude33\ud835\ude2c\ud835\ude34.\n\u2193\u2193\u2193\nI wrote an \ud835\uddee\ud835\uddff\ud835\ude01\ud835\uddf6\ud835\uddf0\ud835\uddf9\ud835\uddf2 on how to \ud835\uddef\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1 a \ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf9-\ud835\ude01\ud835\uddf6\ud835\uddfa\ud835\uddf2 \ud835\uddff\ud835\uddf2\ud835\ude01\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude03\ud835\uddee\ud835\uddf9 \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa for \ud835\udde5\ud835\uddd4\ud835\uddda on \ud835\udddf\ud835\uddf6\ud835\uddfb\ud835\uddf8\ud835\uddf2\ud835\uddf1\ud835\udddc\ud835\uddfb \ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddee in collaboration with\nSuperlinked\n.\nThe \ud835\uddff\ud835\uddf2\ud835\ude01\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude03\ud835\uddee\ud835\uddf9 \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa is based on \ud835\udfee \ud835\uddf1\ud835\uddf2\ud835\ude01\ud835\uddee\ud835\uddf0\ud835\uddf5\ud835\uddf2\ud835\uddf1 \ud835\uddf0\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\uddfc\ud835\uddfb\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\ude00:\n- the streaming ingestion pipeline\n- the retrieval client\nThe \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf6\ud835\uddfb\ud835\uddf4\ud835\uddf2\ud835\ude00\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 runs 24/7 to keep the vector DB synced with the current raw LinkedIn posts data source.\nThe \ud835\uddff\ud835\uddf2\ud835\ude01\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude03\ud835\uddee\ud835\uddf9 \ud835\uddf0\ud835\uddf9\ud835\uddf6\ud835\uddf2\ud835\uddfb\ud835\ude01 is used in RAG applications to query the vector DB.\n\u2192 These 2 components are completely decoupled and communicate with each other through the vector DB.\n#\ud835\udfed. \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf6\ud835\uddfb\ud835\uddf4\ud835\uddf2\ud835\ude00\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\n\u2192 Implemented in\nBytewax\n- a streaming engine built in Rust (speed& reliability) that exposes a Python interface\n\ud835\ude14\ud835\ude22\ud835\ude2a\ud835\ude2f \ud835\ude27\ud835\ude2d\ud835\ude30\ud835\ude38:\n- uses CDC to add changes from the source DB to a queue\n- listens to the queue for new events\n- cleans, chunks, and embeds the LI posts\n- loads them to a\nQdrant\nvector DB\nand... everything in real-time!\n#\ud835\udfee. \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddff\ud835\uddf2\ud835\ude01\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude03\ud835\uddee\ud835\uddf9 \ud835\uddf0\ud835\uddf9\ud835\uddf6\ud835\uddf2\ud835\uddfb\ud835\ude01\n\u2192 A standard Python module.\nThe goal is to retrieve similar posts using a variety of query types - e.g., posts, questions, sentences.\n\ud835\ude14\ud835\ude22\ud835\ude2a\ud835\ude2f \ud835\ude27\ud835\ude2d\ud835\ude30\ud835\ude38:\n- preprocess user queries (the same way as they were ingested)\n- search the Qdrant vector DB for the most similar results\n- use rerank to improve the retrieval system's accuracy\n- visualize the results on a 2D plot using UMAP\n.\nYou don't believe me? \ud83e\udef5\n\ud835\uddd6\ud835\uddf5\ud835\uddf2\ud835\uddf0\ud835\uddf8 \ud835\uddfc\ud835\ude02\ud835\ude01 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddf3\ud835\ude02\ud835\uddf9\ud835\uddf9 \ud835\uddee\ud835\uddff\ud835\ude01\ud835\uddf6\ud835\uddf0\ud835\uddf9\ud835\uddf2 & \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2 \ud835\uddfc\ud835\uddfb \ud835\uddd7\ud835\uddf2\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\udde0\ud835\udddf \u2193\n\ud83d\udd17 \ud835\ude08 \ud835\ude19\ud835\ude26\ud835\ude22\ud835\ude2d-\ud835\ude35\ud835\ude2a\ud835\ude2e\ud835\ude26 \ud835\ude19\ud835\ude26\ud835\ude35\ud835\ude33\ud835\ude2a\ud835\ude26\ud835\ude37\ud835\ude22\ud835\ude2d \ud835\ude1a\ud835\ude3a\ud835\ude34\ud835\ude35\ud835\ude26\ud835\ude2e \ud835\ude27\ud835\ude30\ud835\ude33 \ud835\ude19\ud835\ude08\ud835\ude0e \ud835\ude30\ud835\ude2f \ud835\ude1a\ud835\ude30\ud835\ude24\ud835\ude2a\ud835\ude22\ud835\ude2d \ud835\ude14\ud835\ude26\ud835\ude25\ud835\ude2a\ud835\ude22 \ud835\ude0b\ud835\ude22\ud835\ude35\ud835\ude22:\nhttps://lnkd.in/d87xfWwn\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
        "image": "https://media.licdn.com/dms/image/D4D22AQHwAbjKtfJJtA/feedshare-shrink_800/0/1708678870475?e=1717027200&v=beta&t=DbJ0FjIWpuV4Jr9ZW5TFEweJcdrRWLl_wJgkr35K5hc"
    },
    "Post_19": {
        "text": "Here are \ud835\udff3 \ud835\ude01\ud835\uddf6\ud835\uddfd\ud835\ude00 you must know to \ud835\uddff\ud835\uddf2\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\uddf2 your \ud835\udde9\ud835\udde5\ud835\uddd4\ud835\udde0 \ud835\uddf0\ud835\uddfc\ud835\uddfb\ud835\ude00\ud835\ude02\ud835\uddfa\ud835\uddfd\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb of your \ud835\udddf\ud835\udddf\ud835\udde0\ud835\ude00 during \ud835\ude01\ud835\uddff\ud835\uddee\ud835\uddf6\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 so you can \ud835\uddf3\ud835\uddf6\ud835\ude01 it on \ud835\ude05\ud835\udfed \ud835\uddda\ud835\udde3\ud835\udde8.\n\ud835\udfed. \ud835\udde0\ud835\uddf6\ud835\ude05\ud835\uddf2\ud835\uddf1-\ud835\uddfd\ud835\uddff\ud835\uddf2\ud835\uddf0\ud835\uddf6\ud835\ude00\ud835\uddf6\ud835\uddfc\ud835\uddfb: During training you use both FP32 and FP16 in the following way: \"FP32 weights\" -> \"FP16 weights\" -> \"FP16 gradients\" -> \"FP32 gradients\" -> \"Update weights\" -> \"FP32 weights\" (and repeat). As you can see, the forward & backward passes are done in FP16, and only the optimization step is done in FP32, which reduces both the VRAM and runtime.\n\ud835\udfee. \ud835\udddf\ud835\uddfc\ud835\ude04\ud835\uddf2\ud835\uddff-\ud835\uddfd\ud835\uddff\ud835\uddf2\ud835\uddf0\ud835\uddf6\ud835\ude00\ud835\uddf6\ud835\uddfc\ud835\uddfb: All your computations are done in FP16 instead of FP32. But the key is using bfloat16 (\"Brain Floating Point\"), a numerical representation Google developed for deep learning. It allows you to represent very large and small numbers, avoiding overflowing or underflowing scenarios.\n\ud835\udfef. \ud835\udde5\ud835\uddf2\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddef\ud835\uddee\ud835\ude01\ud835\uddf0\ud835\uddf5 \ud835\ude00\ud835\uddf6\ud835\ude07\ud835\uddf2: This one is straightforward. Fewer samples per training iteration result in smaller VRAM requirements. The downside of this method is that you can't go too low with your batch size without impacting your model's performance.\n\ud835\udff0. \ud835\uddda\ud835\uddff\ud835\uddee\ud835\uddf1\ud835\uddf6\ud835\uddf2\ud835\uddfb\ud835\ude01 \ud835\uddee\ud835\uddf0\ud835\uddf0\ud835\ude02\ud835\uddfa\ud835\ude02\ud835\uddf9\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb: It is a simple & powerful trick to increase your batch size virtually. You compute the gradients for \"micro\" batches (forward + backward passes). Once the accumulated gradients reach the given \"virtual\" target, the model weights are updated with the accumulated gradients. For example, you have a batch size of 4 and a micro-batch size of 1. Then, the forward & backward passes will be done using only x1 sample, and the optimization step will be done using the aggregated gradient of the 4 samples.\n\ud835\udff1. \ud835\udde8\ud835\ude00\ud835\uddf2 \ud835\uddee \ud835\ude00\ud835\ude01\ud835\uddee\ud835\ude01\ud835\uddf2\ud835\uddf9\ud835\uddf2\ud835\ude00\ud835\ude00 \ud835\uddfc\ud835\uddfd\ud835\ude01\ud835\uddf6\ud835\uddfa\ud835\uddf6\ud835\ude07\ud835\uddf2\ud835\uddff: Adam is the most popular optimizer. It is one of the most stable optimizers, but the downside is that it has 2 additional parameters (a mean & variance) for every model parameter. If you use a stateless optimizer, such as SGD, you can reduce the number of parameters by 2/3, which is significant for LLMs.\n\ud835\udff2. \ud835\uddda\ud835\uddff\ud835\uddee\ud835\uddf1\ud835\uddf6\ud835\uddf2\ud835\uddfb\ud835\ude01 (\ud835\uddfc\ud835\uddff \ud835\uddee\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\ude03\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb) \ud835\uddf0\ud835\uddf5\ud835\uddf2\ud835\uddf0\ud835\uddf8\ud835\uddfd\ud835\uddfc\ud835\uddf6\ud835\uddfb\ud835\ude01\ud835\uddf6\ud835\uddfb\ud835\uddf4: It drops specific activations during the forward pass and recomputes them during the backward pass. Thus, it eliminates the need to hold all activations simultaneously in VRAM. This technique reduces VRAM consumption but makes the training slower.\n\ud835\udff3. \ud835\uddd6\ud835\udde3\ud835\udde8 \ud835\uddfd\ud835\uddee\ud835\uddff\ud835\uddee\ud835\uddfa\ud835\uddf2\ud835\ude01\ud835\uddf2\ud835\uddff \ud835\uddfc\ud835\uddf3\ud835\uddf3\ud835\uddf9\ud835\uddfc\ud835\uddee\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4: The parameters that do not fit on your GPU's VRAM are loaded on the CPU. Intuitively, you can see it as a model parallelism between your GPU & CPU.\nMost of these methods are orthogonal, so you can combine them and drastically reduce your VRAM requirements during training.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
        "image": "https://media.licdn.com/dms/image/D4D22AQEQc1TMK7qS9w/feedshare-shrink_800/0/1708592422286?e=1717027200&v=beta&t=iVUsD__Dlj3ogEwuUH8Ve8pGUWa3PQ9LdwhW50Tb9mI"
    },
    "Post_20": {
        "text": "Want to \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb to \ud835\uddef\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\udddf\ud835\udddf\ud835\udde0\ud835\ude00 in a \ud835\ude00\ud835\ude01\ud835\uddff\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2\ud835\uddf1 \ud835\ude04\ud835\uddee\ud835\ude06? For \ud835\uddd9\ud835\udde5\ud835\uddd8\ud835\uddd8? Then \ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\ude00\ud835\uddf5\ud835\uddfc\ud835\ude02\ud835\uddf9\ud835\uddf1 \ud835\ude01\ud835\uddee\ud835\uddf8\ud835\uddf2 our \ud835\udde1\ud835\uddd8\ud835\uddea \ud835\uddf0\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00\ud835\uddf2 on how to \ud835\uddf6\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01 an \ud835\uddf2\ud835\uddfb\ud835\uddf1-\ud835\ude01\ud835\uddfc-\ud835\uddf2\ud835\uddfb\ud835\uddf1 \ud835\uddf3\ud835\uddff\ud835\uddee\ud835\uddfa\ud835\uddf2\ud835\ude04\ud835\uddfc\ud835\uddff\ud835\uddf8 for \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb-\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf1\ud835\ude06 \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa\ud835\ude00 \u2193\n\ud83e\udde0 Decoding ML and I are \ud835\ude00\ud835\ude01\ud835\uddee\ud835\uddff\ud835\ude01\ud835\uddf6\ud835\uddfb\ud835\uddf4 a \ud835\uddfb\ud835\uddf2\ud835\ude04 \ud835\uddd9\ud835\udde5\ud835\uddd8\ud835\uddd8 \ud835\uddf0\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00\ud835\uddf2 on \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 how to \ud835\uddee\ud835\uddff\ud835\uddf0\ud835\uddf5\ud835\uddf6\ud835\ude01\ud835\uddf2\ud835\uddf0\ud835\ude01 and \ud835\uddef\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1 a \ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf9-\ud835\ude04\ud835\uddfc\ud835\uddff\ud835\uddf9\ud835\uddf1 \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa by \ud835\uddef\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 an \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\udde7\ud835\ude04\ud835\uddf6\ud835\uddfb:\n\u2192 from start to finish\u200a-\u200afrom\n\u2192 from data collection to deployment\n\u2192 production-ready\n\u2192 from NO MLOps to experiment trackers, model registries, prompt monitoring, and versioning\nThe course is called: \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\udde7\ud835\ude04\ud835\uddf6\ud835\uddfb: \ud835\uddd5\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddec\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb-\ud835\udde5\ud835\uddf2\ud835\uddee\ud835\uddf1\ud835\ude06 \ud835\uddd4\ud835\udddc \ud835\udde5\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddf6\ud835\uddf0\ud835\uddee\n...and here is what you will learn to build\n\u2193\u2193\u2193\n\ud83d\udc0d 4 \ud835\ude17\ud835\ude3a\ud835\ude35\ud835\ude29\ud835\ude30\ud835\ude2f \ud835\ude2e\ud835\ude2a\ud835\ude24\ud835\ude33\ud835\ude30\ud835\ude34\ud835\ude26\ud835\ude33\ud835\ude37\ud835\ude2a\ud835\ude24\ud835\ude26\ud835\ude34:\n\u2192 \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddee \ud835\uddf0\ud835\uddfc\ud835\uddf9\ud835\uddf9\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\n- Crawl your digital data from various social media platforms.\n- Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines.\n- Send database changes to a queue using the CDC pattern.\n\u2601 Deployed on AWS.\n\u2192 \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddf3\ud835\uddf2\ud835\uddee\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\n- Consume messages from a queue through a\nBytewax\nstreaming pipeline.\n- Every message will be cleaned, chunked, embedded (using\nSuperlinked\n), and loaded into a\nQdrant\nvector DB in real-time.\n\u2601 Deployed on AWS.\n\u2192 \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\ude01\ud835\uddff\ud835\uddee\ud835\uddf6\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\n- Create a custom dataset based on your digital data.\n- Fine-tune an LLM using QLoRA.\n- Use\nComet\nML's experiment tracker to monitor the experiments.\n- Evaluate and save the best model to Comet's model registry.\n\u2601 Deployed on\nQwak\n.\n\u2192 \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddf6\ud835\uddfb\ud835\uddf3\ud835\uddf2\ud835\uddff\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf2 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\n- Load and quantize the fine-tuned LLM from Comet's model registry.\n- Deploy it as a REST API\n- Enhance the prompts using RAG\n- Generate content using your LLM twin\n- Monitor the LLM using Comet's prompt monitoring dashboard\n\u2601 Deployed on Qwak.\n.\n\ud835\ude08\ud835\ude2d\ud835\ude30\ud835\ude2f\ud835\ude28 \ud835\ude35\ud835\ude29\ud835\ude26 4 \ud835\ude2e\ud835\ude2a\ud835\ude24\ud835\ude33\ud835\ude30\ud835\ude34\ud835\ude26\ud835\ude33\ud835\ude37\ud835\ude2a\ud835\ude24\ud835\ude26\ud835\ude34, \ud835\ude3a\ud835\ude30\ud835\ude36 \ud835\ude38\ud835\ude2a\ud835\ude2d\ud835\ude2d \ud835\ude2d\ud835\ude26\ud835\ude22\ud835\ude33\ud835\ude2f \ud835\ude35\ud835\ude30 \ud835\ude2a\ud835\ude2f\ud835\ude35\ud835\ude26\ud835\ude28\ud835\ude33\ud835\ude22\ud835\ude35\ud835\ude26 3 \ud835\ude34\ud835\ude26\ud835\ude33\ud835\ude37\ud835\ude26\ud835\ude33\ud835\ude2d\ud835\ude26\ud835\ude34\ud835\ude34 \ud835\ude35\ud835\ude30\ud835\ude30\ud835\ude2d\ud835\ude34:\n- Comet as your ML Platform\n- Qdrant as your vector DB\n- Qwak as your ML infrastructure\n.\nTo stay updated on \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\udde7\ud835\ude04\ud835\uddf6\ud835\uddfb: \ud835\uddd5\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddec\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb-\ud835\udde5\ud835\uddf2\ud835\uddee\ud835\uddf1\ud835\ude06 \ud835\uddd4\ud835\udddc \ud835\udde5\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddf6\ud835\uddf0\ud835\uddee course...\n\ud835\ude3e\ud835\ude5d\ud835\ude5a\ud835\ude58\ud835\ude60 \ud835\ude5e\ud835\ude69 \ud835\ude64\ud835\ude6a\ud835\ude69 \ud835\ude42\ud835\ude5e\ud835\ude69\ud835\ude43\ud835\ude6a\ud835\ude57 \ud835\ude56\ud835\ude63\ud835\ude59 \ud835\ude68\ud835\ude6a\ud835\ude65\ud835\ude65\ud835\ude64\ud835\ude67\ud835\ude69 \ud835\ude6a\ud835\ude68 \ud835\ude6c\ud835\ude5e\ud835\ude69\ud835\ude5d \ud835\ude56 \u2b50\ufe0f\n\u2193\u2193\u2193\n\ud83d\udd17 \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\udde7\ud835\ude04\ud835\uddf6\ud835\uddfb: \ud835\uddd5\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddec\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb-\ud835\udde5\ud835\uddf2\ud835\uddee\ud835\uddf1\ud835\ude06 \ud835\uddd4\ud835\udddc \ud835\udde5\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddf6\ud835\uddf0\ud835\uddee:\nhttps://lnkd.in/dzat6PB6\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
        "image": "https://media.licdn.com/dms/image/D4D22AQGFC3Aim1ZAcg/feedshare-shrink_800/0/1708506022307?e=1717027200&v=beta&t=T_xJNCgW82k3C4fZzZaPKO6hA4cxD-5iOXPAIvhPXkE"
    },
    "Post_21": {
        "text": "\ud835\uddd7\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddfc\ud835\ude06\ud835\uddf6\ud835\uddfb\ud835\uddf4 & \ud835\uddfa\ud835\uddee\ud835\uddfb\ud835\uddee\ud835\uddf4\ud835\uddf6\ud835\uddfb\ud835\uddf4 ML models is \ud835\uddf5\ud835\uddee\ud835\uddff\ud835\uddf1, especially when running your models on GPUs.\nBut \ud835\ude00\ud835\uddf2\ud835\uddff\ud835\ude03\ud835\uddf2\ud835\uddff\ud835\uddf9\ud835\uddf2\ud835\ude00\ud835\ude00 makes things \ud835\uddf2\ud835\uddee\ud835\ude00\ud835\ude06.\nUsing Beam as your serverless provider, deploying & managing ML models can be as easy as \u2193\n\ud835\uddd7\ud835\uddf2\ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddf2 \ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\uddf6\ud835\uddfb\ud835\uddf3\ud835\uddff\ud835\uddee\ud835\ude00\ud835\ude01\ud835\uddff\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2 & \ud835\uddf1\ud835\uddf2\ud835\uddfd\ud835\uddf2\ud835\uddfb\ud835\uddf1\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf6\ud835\uddf2\ud835\ude00\nIn a few lines of code, you define the application that contains:\n- the requirements of your infrastructure, such as the CPU, RAM, and GPU\n- the dependencies of your application\n- the volumes from where you can load your data and store your artifacts\n\ud835\uddd7\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddfc\ud835\ude06 \ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\uddf7\ud835\uddfc\ud835\uddef\ud835\ude00\nUsing the Beam application, you can quickly decore your Python functions to:\n- run them once on the given serverless application\n- put your task/job in a queue to be processed or even schedule it using a CRON-based syntax\n- even deploy it as a RESTful API endpoint\n.\nAs you can see in the image below, you can have one central function for training or inference, and with minimal effort, you can switch from all these deployment methods.\nAlso, you don't have to bother at all with managing the infrastructure on which your jobs run. You specify what you need, and Beam takes care of the rest.\nBy doing so, you can directly start to focus on your application and stop carrying about the infrastructure.\nThis is the power of serverless!\n.\n\u21b3 \ud835\ude0a\ud835\ude29\ud835\ude26\ud835\ude24\ud835\ude2c \ud835\ude30\ud835\ude36\ud835\ude35 \ud835\ude09\ud835\ude26\ud835\ude22\ud835\ude2e \ud835\ude35\ud835\ude30 \ud835\ude2d\ud835\ude26\ud835\ude22\ud835\ude33\ud835\ude2f \ud835\ude2e\ud835\ude30\ud835\ude33\ud835\ude26: \ud83d\udd17\nhttps://lnkd.in/d4-pkCxc\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily content on ML and MLOps engineering.",
        "image": "https://media.licdn.com/dms/image/D4D22AQGzmneYi4CKQA/feedshare-shrink_2048_1536/0/1708419787323?e=1717027200&v=beta&t=_yVp7yxRxlskXInLwidlXMdaNCbC0L1jZH9sqBMhhS4"
    },
    "Post_22": {
        "text": "\ud835\uddea\ud835\uddf5\ud835\uddee\ud835\ude01 do you need to \ud835\uddef\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1 an \ud835\uddf6\ud835\uddfb\ud835\uddf3\ud835\uddf2\ud835\uddff\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf2 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 for a financial assistant \ud835\uddfd\ud835\uddfc\ud835\ude04\ud835\uddf2\ud835\uddff\ud835\uddf2\ud835\uddf1 by \ud835\udddf\ud835\udddf\ud835\udde0\ud835\ude00 and \ud835\ude03\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddfc\ud835\uddff \ud835\uddd7\ud835\uddd5\ud835\ude00?\nHere are its \ud835\udff3 \ud835\uddf8\ud835\uddf2\ud835\ude06 \ud835\uddf0\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\uddfc\ud835\uddfb\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\ude00 \u2193\n1. \ud835\ude03\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddfc\ud835\uddff \ud835\uddd7\ud835\uddd5 \ud835\uddfd\ud835\uddfc\ud835\uddfd\ud835\ude02\ud835\uddf9\ud835\uddee\ud835\ude01\ud835\uddf2\ud835\uddf1 \ud835\ude04\ud835\uddf6\ud835\ude01\ud835\uddf5 \ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddee\ud835\uddfb\ud835\uddf0\ud835\uddf6\ud835\uddee\ud835\uddf9 \ud835\uddfb\ud835\uddf2\ud835\ude04\ud835\ude00: This is the output of the feature pipeline. More concretely, a\nQdrant\nvector DB populated with chunks of financial news from Alpaca. During the inference pipeline, we will use it to query valuable chunks of information and do RAG.\n2. \ud835\uddf2\ud835\uddfa\ud835\uddef\ud835\uddf2\ud835\uddf1\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf9\ud835\uddee\ud835\uddfb\ud835\uddf4\ud835\ude02\ud835\uddee\ud835\uddf4\ud835\uddf2 \ud835\uddfa\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9: To embed the user question and query the vector DB, you need the same embedding model used in the feature pipeline, more concretely `\ud835\ude22\ud835\ude2d\ud835\ude2d-\ud835\ude14\ud835\ude2a\ud835\ude2f\ud835\ude2a\ud835\ude13\ud835\ude14-\ud835\ude136-\ud835\ude372` from `\ud835\ude34\ud835\ude26\ud835\ude2f\ud835\ude35\ud835\ude26\ud835\ude2f\ud835\ude24\ud835\ude26-\ud835\ude35\ud835\ude33\ud835\ude22\ud835\ude2f\ud835\ude34\ud835\ude27\ud835\ude30\ud835\ude33\ud835\ude2e\ud835\ude26\ud835\ude33\ud835\ude34`. Using the same encoder-only model is crucial, as the query vector and vector DB index vectors have to be in the same space.\n3. \ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddf2-\ud835\ude01\ud835\ude02\ud835\uddfb\ud835\uddf2\ud835\uddf1 \ud835\uddfc\ud835\uddfd\ud835\uddf2\ud835\uddfb-\ud835\ude00\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\uddf0\ud835\uddf2 \ud835\udddf\ud835\udddf\ud835\udde0: The output of the training pipeline will be a fine-tuned Falcon 7B on financial tasks.\n4. \ud835\uddfa\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9 \ud835\uddff\ud835\uddf2\ud835\uddf4\ud835\uddf6\ud835\ude00\ud835\ude01\ud835\uddff\ud835\ude06: The fine-tuned model will be shared between the training & inference pipeline through\nComet\n's model registry. By doing so, you decouple entirely the 2 components, and the model can easily be shared under specific environments (e.g., staging, prod) and versions (e.g., v1.0.1).\n5. \ud835\uddee \ud835\uddf3\ud835\uddff\ud835\uddee\ud835\uddfa\ud835\uddf2\ud835\ude04\ud835\uddfc\ud835\uddff\ud835\uddf8 \ud835\uddf3\ud835\uddfc\ud835\uddff \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\uddee\ud835\uddfd\ud835\uddfd\ud835\uddf9\ud835\uddf6\ud835\uddf0\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb\ud835\ude00: You need LangChain, as your LLM framework, to glue all the steps together, such as querying the vector DB, storing the history of the conversation, creating the prompt, and calling the LLM. LangChain provides out-of-the-box solutions to chain all these steps together quickly.\n6. \ud835\uddf1\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddfc\ud835\ude06 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\uddee\ud835\uddfd\ud835\uddfd \ud835\uddee\ud835\ude00 \ud835\uddee \ud835\udde5\ud835\uddd8\ud835\udde6\ud835\udde7\ud835\uddf3\ud835\ude02\ud835\uddf9 \ud835\uddd4\ud835\udde3\ud835\udddc: One of the final steps is to deploy your awesome LLM financial assistant under a RESTful API. You can quickly do this using Beam as your serverless infrastructure provider. Beam specializes in DL. Thus, it offers quick ways to load your LLM application on GPU machines and expose it under a RESTful API. \u21b3\ud83d\udd17 Beam:\nhttps://lnkd.in/dedCaMDh\n7. \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\ude01 \ud835\uddfa\ud835\uddfc\ud835\uddfb\ud835\uddf6\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\uddf6\ud835\uddfb\ud835\uddf4: The last step is to add eyes on top of your system. You can do this using Comet 's LLMOps features that allow you to track & monitor all the prompts & responses of the system.\n\ud835\ude1e\ud835\ude22\ud835\ude2f\ud835\ude2f\ud835\ude22 \ud835\ude34\ud835\ude26\ud835\ude26 \ud835\ude29\ud835\ude30\ud835\ude38 \ud835\ude35\ud835\ude29\ud835\ude26\ud835\ude34\ud835\ude26 \ud835\ude24\ud835\ude30\ud835\ude2e\ud835\ude31\ud835\ude30\ud835\ude2f\ud835\ude26\ud835\ude2f\ud835\ude35\ud835\ude34 \ud835\ude22\ud835\ude33\ud835\ude26 \ud835\ude38\ud835\ude30\ud835\ude33\ud835\ude2c\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude35\ud835\ude30\ud835\ude28\ud835\ude26\ud835\ude35\ud835\ude29\ud835\ude26\ud835\ude33?\n\u2193\u2193\u2193\n\u21b3 Check out our \ud835\udddb\ud835\uddee\ud835\uddfb\ud835\uddf1\ud835\ude00-\ud835\uddfc\ud835\uddfb \ud835\udddf\ud835\udddf\ud835\udde0\ud835\ude00 \ud835\uddf3\ud835\uddff\ud835\uddf2\ud835\uddf2 \ud835\uddf0\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00\ud835\uddf2: \ud83d\udd17\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily content on ML and MLOps engineering.",
        "image": "https://media.licdn.com/dms/image/D4D22AQGfZbjBrBm3cA/feedshare-shrink_2048_1536/0/1708246924272?e=1717027200&v=beta&t=RV7bmeG5WWihzuZSQ8TsdTxPPLxCkUi-l4emZvjoZ7E"
    },
    "Post_23": {
        "text": "Want to \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb an \ud835\uddf2\ud835\uddfb\ud835\uddf1-\ud835\ude01\ud835\uddfc-\ud835\uddf2\ud835\uddfb\ud835\uddf1 \ud835\uddf3\ud835\uddff\ud835\uddee\ud835\uddfa\ud835\uddf2\ud835\ude04\ud835\uddfc\ud835\uddff\ud835\uddf8 for \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb-\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf1\ud835\ude06 \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa\ud835\ude00 by \ud835\uddef\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 your \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\ude01\ud835\ude04\ud835\uddf6\ud835\uddfb?\nThen you are in luck.\n\u2193\u2193\u2193\nThe Decoding ML team and I will \ud835\uddff\ud835\uddf2\ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\ude00\ud835\uddf2 a \ud835\uddd9\ud835\udde5\ud835\uddd8\ud835\uddd8 \ud835\uddf0\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00\ud835\uddf2 called the \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\udde7\ud835\ude04\ud835\uddf6\ud835\uddfb: \ud835\uddd5\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddec\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb-\ud835\udde5\ud835\uddf2\ud835\uddee\ud835\uddf1\ud835\ude06 \ud835\uddd4\ud835\udddc \ud835\udde5\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddf6\ud835\uddf0\ud835\uddee in a few days.\nWithin the course, you will learn how to:\n- architect\n- train\n- deploy\n...a \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb-\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf1\ud835\ude06 \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\ude01\ud835\ude04\ud835\uddf6\ud835\uddfb of yourself powered by LLMs, vector DBs, and LLMOps good practices, such as:\n- experiment trackers\n- model registries\n- prompt monitoring\n- versioning\n- deploying LLMs\n...and more!\n.\nIt is an \ud835\uddf2\ud835\uddfb\ud835\uddf1-\ud835\ude01\ud835\uddfc-\ud835\uddf2\ud835\uddfb\ud835\uddf1 \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\uddf0\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00\ud835\uddf2 where you will \ud835\uddef\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1 a \ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf9-\ud835\ude04\ud835\uddfc\ud835\uddff\ud835\uddf9\ud835\uddf1 \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa:\n\u2192 from start to finish\n\u2192 from data collection to deployment\n.\n\ud835\uddea\ud835\uddf5\ud835\uddee\ud835\ude01 \ud835\uddf6\ud835\ude00 \ud835\uddee\ud835\uddfb \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\udde7\ud835\ude04\ud835\uddf6\ud835\uddfb? It is an AI character that learns to write like somebody by incorporating its style and personality into an LLM.\n.\n\ud835\uddea\ud835\uddf5\ud835\uddfc \ud835\uddf6\ud835\ude00 \ud835\ude01\ud835\uddf5\ud835\uddf6\ud835\ude00\u00a0\ud835\uddf3\ud835\uddfc\ud835\uddff?\n\ud835\ude08\ud835\ude36\ud835\ude25\ud835\ude2a\ud835\ude26\ud835\ude2f\ud835\ude24\ud835\ude26: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps good principles.\n\ud835\ude13\ud835\ude26\ud835\ude37\ud835\ude26\ud835\ude2d: intermediate\n\ud835\ude17\ud835\ude33\ud835\ude26\ud835\ude33\ud835\ude26\ud835\ude32\ud835\ude36\ud835\ude2a\ud835\ude34\ud835\ude2a\ud835\ude35\ud835\ude26\ud835\ude34: basic knowledge of Python, ML, and the cloud\n\ud835\udddb\ud835\uddfc\ud835\ude04 \ud835\ude04\ud835\uddf6\ud835\uddf9\ud835\uddf9 \ud835\ude06\ud835\uddfc\ud835\ude02\u00a0\ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb?\nThe course contains \ud835\udfed\ud835\udfed \ud835\uddf5\ud835\uddee\ud835\uddfb\ud835\uddf1\ud835\ude00-\ud835\uddfc\ud835\uddfb \ud835\ude04\ud835\uddff\ud835\uddf6\ud835\ude01\ud835\ude01\ud835\uddf2\ud835\uddfb \ud835\uddf9\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddfc\ud835\uddfb\ud835\ude00 and the \ud835\uddfc\ud835\uddfd\ud835\uddf2\ud835\uddfb-\ud835\ude00\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\uddf0\ud835\uddf2 \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2 you can access on GitHub.\nYou can read everything at your own pace.\n\ud835\uddd6\ud835\uddfc\ud835\ude00\ud835\ude01\ud835\ude00?\nThe \ud835\uddee\ud835\uddff\ud835\ude01\ud835\uddf6\ud835\uddf0\ud835\uddf9\ud835\uddf2\ud835\ude00 and \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2 are \ud835\uddf0\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude01\ud835\uddf2\ud835\uddf9\ud835\ude06 \ud835\uddf3\ud835\uddff\ud835\uddf2\ud835\uddf2. They will always remain free.\nThis time, the Medium articles won't be under any paid wall. I want to make them entirely available to everyone.\n\ud835\udde0\ud835\uddf2\ud835\uddf2\ud835\ude01 \ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\ude01\ud835\uddf2\ud835\uddee\ud835\uddf0\ud835\uddf5\ud835\uddf2\ud835\uddff\ud835\ude00!\nThe course is created under the Decoding ML umbrella by:\nPaul Iusztin\n| Senior ML & MLOps Engineer\nAlex Vesa\n| Senior AI Engineer\nAlexandru Razvant\n| Senior ML & MLOps Engineer\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud835\ude3e\ud835\ude5d\ud835\ude5a\ud835\ude58\ud835\ude60 \ud835\ude5e\ud835\ude69 \ud835\ude64\ud835\ude6a\ud835\ude69 \ud835\ude42\ud835\ude5e\ud835\ude69\ud835\ude43\ud835\ude6a\ud835\ude57 \ud835\ude56\ud835\ude63\ud835\ude59 \ud835\ude68\ud835\ude6a\ud835\ude65\ud835\ude65\ud835\ude64\ud835\ude67\ud835\ude69 \ud835\ude6a\ud835\ude68 \ud835\ude6c\ud835\ude5e\ud835\ude69\ud835\ude5d \ud835\ude56 \u2b50\ufe0f\n\u2193\u2193\u2193\n\ud83d\udd17 \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\udde7\ud835\ude04\ud835\uddf6\ud835\uddfb: \ud835\uddd5\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddec\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb-\ud835\udde5\ud835\uddf2\ud835\uddee\ud835\uddf1\ud835\ude06 \ud835\uddd4\ud835\udddc \ud835\udde5\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddf6\ud835\uddf0\ud835\uddee:\nhttps://lnkd.in/dzat6PB6",
        "image": "https://media.licdn.com/dms/image/D4D22AQH0GnGepzOn3g/feedshare-shrink_800/0/1708160419507?e=1717027200&v=beta&t=6dEP238PZLVawsFI_qwqzPChIizt_ZKDUygqzalBX34"
    },
    "Post_24": {
        "text": "Want to learn \ud835\udde0\ud835\udddf\ud835\uddd8 & \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 in a \ud835\ude00\ud835\ude01\ud835\uddff\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2\ud835\uddf1 \ud835\ude04\ud835\uddee\ud835\ude06, for \ud835\uddf3\ud835\uddff\ud835\uddf2\ud835\uddf2, and with \ud835\uddf5\ud835\uddee\ud835\uddfb\ud835\uddf1\ud835\ude00-\ud835\uddfc\ud835\uddfb \ud835\uddf2\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00?\nThen you should check out my \ud835\udde7\ud835\uddf5\ud835\uddf2 \ud835\uddd9\ud835\ude02\ud835\uddf9\ud835\uddf9 \ud835\udde6\ud835\ude01\ud835\uddee\ud835\uddf0\ud835\uddf8 \ud835\udff3-\ud835\udde6\ud835\ude01\ud835\uddf2\ud835\uddfd\ud835\ude00 \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 \ud835\uddd9\ud835\uddff\ud835\uddee\ud835\uddfa\ud835\uddf2\ud835\ude04\ud835\uddfc\ud835\uddff\ud835\uddf8 FREE course.\n.\nIn \ud835\udfee.\ud835\udff1 \ud835\uddf5\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00 \ud835\uddfc\ud835\uddf3 \ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 & \ud835\ude03\ud835\uddf6\ud835\uddf1\ud835\uddf2\ud835\uddfc \ud835\uddfa\ud835\uddee\ud835\ude01\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddee\ud835\uddf9\ud835\ude00, you will \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb \ud835\uddf5\ud835\uddfc\ud835\ude04 \ud835\ude01\ud835\uddfc:\n- design a batch-serving architecture\n- use Hopsworks as a feature store\n- design a feature engineering pipeline that reads data from an API\n- build a training pipeline with hyper-parameter tunning\n- use W&B as an ML Platform to track your experiments, models, and metadata\n- implement a batch prediction pipeline\n- use Poetry to build your own Python packages\n- deploy your own private PyPi server\n- orchestrate everything with Airflow\n- use the predictions to code a web app using FastAPI and Streamlit\n- use Docker to containerize your code\n- use Great Expectations to ensure data validation and integrity\n- monitor the performance of the predictions over time\n- deploy everything to GCP\n- build a CI/CD pipeline using GitHub Actions\n- trade-offs & future improvements discussion\n...where all the pieces are integrated into a single end-to-end ML system that forecasts hourly energy levels across Denmark.\n\ud835\uddec\ud835\uddfc\ud835\ude02 \ud835\uddf0\ud835\uddee\ud835\uddfb \ud835\uddee\ud835\uddf0\ud835\uddf0\ud835\uddf2\ud835\ude00\ud835\ude00 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddf0\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00\ud835\uddf2 \ud835\uddfc\ud835\uddfb:\n\u279d \ud835\ude14\ud835\ude26\ud835\ude25\ud835\ude2a\ud835\ude36\ud835\ude2e'\ud835\ude34 \ud835\ude1b\ud835\ude0b\ud835\ude1a \ud835\ude31\ud835\ude36\ud835\ude23\ud835\ude2d\ud835\ude2a\ud835\ude24\ud835\ude22\ud835\ude35\ud835\ude2a\ud835\ude30\ud835\ude2f: text tutorials + videos\n\u279d \ud835\ude0e\ud835\ude2a\ud835\ude35\ud835\ude0f\ud835\ude36\ud835\ude23: open-source code + docs\nThe course is on Medium's TDS publication to make it easily accessible to people worldwide. Thus \u2193\n... anyone can learn the fundamentals of MLE & MLOps.\nSo, no more excuses. Just go and build your own project \ud83d\udd25\nCheck it out \u2193\n\ud83d\udd17 \ud835\ude1b\ud835\ude29\ud835\ude26 \ud835\ude0d\ud835\ude36\ud835\ude2d\ud835\ude2d \ud835\ude1a\ud835\ude35\ud835\ude22\ud835\ude24\ud835\ude2c 7-\ud835\ude1a\ud835\ude35\ud835\ude26\ud835\ude31\ud835\ude34 \ud835\ude14\ud835\ude13\ud835\ude16\ud835\ude31\ud835\ude34 \ud835\ude0d\ud835\ude33\ud835\ude22\ud835\ude2e\ud835\ude26\ud835\ude38\ud835\ude30\ud835\ude33\ud835\ude2c:\nhttps://lnkd.in/d_GVpZ9X\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily content on ML and MLOps engineering.",
        "image": "https://media.licdn.com/dms/image/D4D22AQFM1ZvU9dRgQQ/feedshare-shrink_800/0/1708074076272?e=1717027200&v=beta&t=Eg_F6acsQ9O6ghVWTvjh477xx0JFRk27f2k4sQbFRLQ"
    },
    "Post_25": {
        "text": "The difference between \ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddff\ud835\ude00, \ud835\uddf1\ud835\uddf2\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddff\ud835\ude00 and \ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddff-\ud835\uddf1\ud835\uddf2\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddff \ud835\udddf\ud835\udddf\ud835\udde0\ud835\ude00.\nEmbeddings are everywhere... both encoders and decoders use self-attention layers to encode word tokens into embeddings.\nLet's see when to use each architecture \u2193\nThe key difference between an encoder & decoder is in how it processes its inputs & outputs.\n=== \ud835\uddd8\ud835\uddfb\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddff\ud835\ude00 ===\nThe role of an encoder is to extract relevant information from the whole input and encode it into an embedding (e.g., BERT, RoBERTa).\nWithin the \"Multi-head attention\" of the transformer, all the tokens are allowed to speak to each other.\nA token at position t can talk to all other previous tokens [0, t-1] and future tokens [t+1, T]. This means that the attention mask is computed along the whole vector.\nThus, because the encoder processes the whole input, it is helpful for classification tasks (e.g., sentiment analysis) and creates embeddings for clustering, recommender systems, vector DB indexes, etc.\n=== \ud835\uddd7\ud835\uddf2\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddff\ud835\ude00 ===\nOn the flip side, if you want to generate text, use decoder-only models (e.g., GPT family).\nOnly the current and previous tokens (not the whole input) are used to predict the next token.\nWithin the \"Masked Multi-head attention,\" the future positions are masked to maintain the autoregressive property of the decoding process.\nFor example, within the \"Masked Multi-head attention,\" instead of all the tokens talking to each other, a token at position t will have access only to previous tokens at positions t-1, t-2, t-3, ..., 0.\n=== \ud835\uddd8\ud835\uddfb\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddff-\ud835\uddf1\ud835\uddf2\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddff ===\nThis technique is used when you have to understand the entire input sequence (encoder) and the previously generated sequence (decoder -> autoregressive).\nTypical use cases are text translation & summarization (the original transformer was built for text translation), where the output heavily relies on the input.\nWhy? Because the decoding step always has to be conditioned by the encoded information. Also known as cross-attention, the decoder queries the encoded information for information to guide the decoding process.\nFor example, when translating English to Spanish, every Spanish token predicted is conditioned by the previously predicted Spanish tokens & the entire English sentence.\n.\nTo conclude...\n- a decoder takes as input previous tokens and predicts the next one (in an autoregressive way)\n- by dropping the \"Masked\" logic from the \"Masked Multi-head attention,\" you process the whole input, transforming the decoder into an encoder\n- if you hook the encoder to the decoder through a cross-attention layer, you have an encoder-decoder architecture\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily content on ML and MLOps engineering.",
        "image": "https://media.licdn.com/dms/image/D4E22AQHXIDi8QL2uSg/feedshare-shrink_800/0/1707987616806?e=1717027200&v=beta&t=sDRi8Ox5_tAzyrTSFcu0RZrRkvmoQmRUHkPjj0arq5M"
    },
    "Post_26": {
        "text": "\ud835\uddea\ud835\uddee\ud835\uddfb\ud835\ude01 to \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 but got stuck at the 100th tool you think you must know? Here is the \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 \ud835\uddff\ud835\uddfc\ud835\uddee\ud835\uddf1\ud835\uddfa\ud835\uddee\ud835\uddfd \ud835\uddf3\ud835\uddfc\ud835\uddff \ud835\udfee\ud835\udfec\ud835\udfee\ud835\udff0 \u2193\n\ud835\ude14\ud835\ude13\ud835\ude16\ud835\ude31\ud835\ude34 \ud835\ude37\ud835\ude34. \ud835\ude14\ud835\ude13 \ud835\ude26\ud835\ude2f\ud835\ude28\ud835\ude2a\ud835\ude2f\ud835\ude26\ud835\ude26\ud835\ude33\nIn theory, MLEs focus on deploying models to production while MLOps engineers build the platform used by MLEs.\nI think this is heavily dependent on the scale of the company. As the company gets smaller, these 2 roles start to overlap more.\nThis roadmap will teach you how to build such a platform, from programming skills to MLOps components and infrastructure as code.\n.\nHere is the MLOps roadmap for 2024 suggested by\nMaria Vechtomova\nfrom @company_marvelous-mlops:\n\ud835\udfed. \ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddf4\ud835\uddff\ud835\uddee\ud835\uddfa\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4\n- Python & IDEs\n- Bash basics & command line editors\n\ud835\udfee. \ud835\uddd6\ud835\uddfc\ud835\uddfb\ud835\ude01\ud835\uddee\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\ude07\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\uddde\ud835\ude02\ud835\uddef\ud835\uddf2\ud835\uddff\ud835\uddfb\ud835\uddf2\ud835\ude01\ud835\uddf2\ud835\ude00\n- Docker\n- Kubernetes\n\ud835\udfef. \ud835\udde0\ud835\uddee\ud835\uddf0\ud835\uddf5\ud835\uddf6\ud835\uddfb\ud835\uddf2 \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf3\ud835\ude02\ud835\uddfb\ud835\uddf1\ud835\uddee\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\uddee\ud835\uddf9\ud835\ude00\n...until now we laid down the fundamentals. Now let's get into MLOps \ud83d\udd25\n\ud835\udff0. \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 \ud835\uddfd\ud835\uddff\ud835\uddf6\ud835\uddfb\ud835\uddf0\ud835\uddf6\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude00\n- reproducible,\n- testable, and\n- evolvable ML-powered software\n\ud835\udff1. \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 \ud835\uddf0\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\uddfc\ud835\uddfb\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\ude00\n- Version control & CI/CD pipelines\n- Orchestration\n- Experiment tracking and model registries\n- Data lineage and feature stores\n- Model training & serving\n- Monitoring & observability\n\ud835\udff2. \ud835\udddc\ud835\uddfb\ud835\uddf3\ud835\uddff\ud835\uddee\ud835\ude00\ud835\ude01\ud835\uddff\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2 \ud835\uddee\ud835\ude00 \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2\n- Terraform\n.\nAs a self-learner, I wish I had access to this step-by-step plan when I started learning MLOps.\nRemember, you should pick up and tailor this roadmap at the level you are currently at.\nFind more details about the roadmap in\nMaria Vechtomova\n's article \u2193\n\ud83d\udd17 MLOps roadmap 2024:\nhttps://lnkd.in/dCFdhnWg\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps.",
        "image": "https://media.licdn.com/dms/image/D4D22AQHQyBWmxK7Esw/feedshare-shrink_800/0/1706254384617?e=1717027200&v=beta&t=NWeXZzotbd-gUE_aj9vKERW6HQhIzVvZfohbP58taSY"
    },
    "Post_27": {
        "text": "How to \ud835\uddef\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1 an end-to-end \ud835\udde3\ud835\ude06\ud835\udde7\ud835\uddfc\ud835\uddff\ud835\uddf0\ud835\uddf5 \u2192 \ud835\udde2\ud835\udde1\ud835\udde1\ud835\uddeb \u2192 \ud835\udde7\ud835\uddf2\ud835\uddfb\ud835\ude00\ud835\uddfc\ud835\uddff\ud835\udde5\ud835\udde7 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 for \ud835\uddec\ud835\udde2\ud835\udddf\ud835\udde2 \ud835\udde2\ud835\uddef\ud835\uddf7\ud835\uddf2\ud835\uddf0\ud835\ude01 \ud835\uddd7\ud835\uddf2\ud835\ude01\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\udde0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9\ud835\ude00 (or other DL models)\n\ud835\ude1b\ud835\ude29\ud835\ude26 \ud835\ude31\ud835\ude33\ud835\ude30\ud835\ude23\ud835\ude2d\ud835\ude26\ud835\ude2e?\nWhen compiling YOLO models to TensorRT on multiple GPUs, there are always mismatches between ONNX, ORT, TensorRT, etc.\n.\n\ud835\ude1b\ud835\ude29\ud835\ude26 \ud835\ude34\ud835\ude30\ud835\ude2d\ud835\ude36\ud835\ude35\ud835\ude2a\ud835\ude30\ud835\ude2f?\nThis is how you can automate this process and save hours of repetitive work \u2193\n1. \ud835\uddda\ud835\uddf2\ud835\uddfb\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddf0 \ud835\udde3\ud835\ude06\ud835\udde7\ud835\uddfc\ud835\uddff\ud835\uddf0\ud835\uddf5-\ud835\ude01\ud835\uddfc-\ud835\udde2\ud835\udde1\ud835\udde1\ud835\uddeb: have a single .py script that converts the .pt 2 .onnx\n2. \ud835\udde6\ud835\uddf6\ud835\uddfb\ud835\uddf4\ud835\uddf9\ud835\uddf2 \ud835\uddd6\ud835\uddfc\ud835\uddfb\ud835\uddf3\ud835\uddf6\ud835\uddf4\ud835\ude02\ud835\uddff\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddd9\ud835\uddf6\ud835\uddf9\ud835\uddf2: have a config.json file where you specify the TensorRT version I need, the ONNX Operator Set, and metadata fields to save the model\n3.  \ud835\udde2\ud835\udde1\ud835\udde1\ud835\uddeb \ud835\udde6\ud835\uddee\ud835\uddfb\ud835\uddf6\ud835\ude01\ud835\ude06 \ud835\uddd6\ud835\uddf5\ud835\uddf2\ud835\uddf0\ud835\uddf8: run several tests on the integrity of the model and its configuration\n4. Convert to \ud835\udde7\ud835\uddf2\ud835\uddfb\ud835\ude00\ud835\uddfc\ud835\uddff\ud835\udde5\ud835\udde7: start the TensorRT container, copy the .onnx model, convert the model to .engine, copy it back to the host path, then remove the container\n5. \ud835\udde2\ud835\uddfd\ud835\ude01\ud835\uddf6\ud835\uddfa\ud835\uddee\ud835\uddf9 \ud835\uddf0\ud835\uddfc\ud835\uddfb\ud835\uddf3\ud835\uddf6\ud835\uddf4\ud835\ude02\ud835\uddff\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\ude00\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddf0\ud835\uddf5 \ud835\ude04\ud835\uddf6\ud835\ude01\ud835\uddf5 \ud835\udde1\ud835\ude03\ud835\uddf6\ud835\uddf1\ud835\uddf6\ud835\uddee \ud835\udde3\ud835\uddf2\ud835\uddff\ud835\uddf3 \ud835\uddd4\ud835\uddfb\ud835\uddee\ud835\uddf9\ud835\ude06\ud835\ude07\ud835\uddf2\ud835\uddff: test various use cases, like optimal batch size, min/optimal/max load of concurrent requests, GPU stats under workload, etc.\n6. \ud835\udde0\ud835\uddf2\ud835\ude01\ud835\uddff\ud835\uddf6\ud835\uddf0\ud835\ude00 \ud835\uddd7\ud835\uddee\ud835\ude00\ud835\uddf5\ud835\uddef\ud835\uddfc\ud835\uddee\ud835\uddff\ud835\uddf1: generate charts around critical metrics like Queries-per-Second (QPS), Concurrency Analysis, and Precision Accuracy\n\ud835\uddd6\ud835\ude02\ud835\uddff\ud835\uddf6\ud835\uddfc\ud835\ude02\ud835\ude00 \ud835\ude01\ud835\uddfc \ud835\ude01\ud835\uddff\ud835\ude06 \ud835\uddf6\ud835\ude01 \ud835\uddfc\ud835\ude02\ud835\ude01?\nAlexandru Razvant\nwrote a detailed article on Decoding ML and provided the code for implementing this technique on your own models.\n\ud835\uddd6\ud835\uddf5\ud835\uddf2\ud835\uddf0\ud835\uddf8 \ud835\uddf6\ud835\ude01 \ud835\uddfc\ud835\ude02\ud835\ude01 \ud835\uddfc\ud835\uddfb \ud835\uddd7\ud835\uddf2\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\udde0\ud835\udddf \u2193\n\ud83d\udd17 \ud835\ude0f\ud835\ude30\ud835\ude38 \ud835\ude35\ud835\ude30 \ud835\ude23\ud835\ude36\ud835\ude2a\ud835\ude2d\ud835\ude25 \ud835\ude22 \ud835\ude17\ud835\ude3a\ud835\ude1b\ud835\ude30\ud835\ude33\ud835\ude24\ud835\ude29 - \ud835\ude1b\ud835\ude26\ud835\ude2f\ud835\ude34\ud835\ude30\ud835\ude33\ud835\ude19\ud835\ude1b \ud835\ude31\ud835\ude2a\ud835\ude31\ud835\ude26\ud835\ude2d\ud835\ude2a\ud835\ude2f\ud835\ude26 \ud835\ude27\ud835\ude30\ud835\ude33 \ud835\ude20\ud835\ude16\ud835\ude13\ud835\ude16 \ud835\ude16\ud835\ude23\ud835\ude2b\ud835\ude26\ud835\ude24\ud835\ude35 \ud835\ude0b\ud835\ude26\ud835\ude35\ud835\ude26\ud835\ude24\ud835\ude35\ud835\ude2a\ud835\ude30\ud835\ude2f \ud835\ude14\ud835\ude30\ud835\ude25\ud835\ude26\ud835\ude2d\ud835\ude34! :\nhttps://lnkd.in/dsf-j8nv\n\ud835\udde1\ud835\uddfc\ud835\ude01\ud835\uddf2: All the credits for the diagram go to\nAlexandru Razvant\n.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps.",
        "image": "https://media.licdn.com/dms/image/D4D22AQHXyBCNimdeqw/feedshare-shrink_800/0/1706167866754?e=1717027200&v=beta&t=KrgsR41R4PQX5b7bs7JxPwXXpAEuNBWbQ0yxXPht5DI"
    },
    "Post_28": {
        "text": "\ud835\uddea\ud835\uddf5\ud835\ude06 have \ud835\ude03\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddfc\ud835\uddff \ud835\uddd7\ud835\uddd5\ud835\ude00 become so \ud835\uddfd\ud835\uddfc\ud835\uddfd\ud835\ude02\ud835\uddf9\ud835\uddee\ud835\uddff & \ud835\ude04\ud835\uddf5\ud835\ude06 are they so \ud835\uddf0\ud835\uddff\ud835\ude02\ud835\uddf0\ud835\uddf6\ud835\uddee\ud835\uddf9 for most \ud835\udde0\ud835\udddf \ud835\uddee\ud835\uddfd\ud835\uddfd\ud835\uddf9\ud835\uddf6\ud835\uddf0\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb\ud835\ude00 (not only LLMs)?\nIn the world of ML, everything can be represented as an embedding.\nA vector DB is an intelligent way to use your data embeddings as an index and perform fast and scalable searches between unstructured data points.\nSimply put, a vector DB allows you to find matches between anything and anything (e.g., use an image as a query to find similar pieces of text, video, other images, etc.).\n.\n\ud835\ude10\ud835\ude2f \ud835\ude22 \ud835\ude2f\ud835\ude36\ud835\ude35\ud835\ude34\ud835\ude29\ud835\ude26\ud835\ude2d\ud835\ude2d, \ud835\ude35\ud835\ude29\ud835\ude2a\ud835\ude34 \ud835\ude2a\ud835\ude34 \ud835\ude29\ud835\ude30\ud835\ude38 \ud835\ude3a\ud835\ude30\ud835\ude36 \ud835\ude24\ud835\ude22\ud835\ude2f \ud835\ude2a\ud835\ude2f\ud835\ude35\ud835\ude26\ud835\ude28\ud835\ude33\ud835\ude22\ud835\ude35\ud835\ude26 \ud835\ude22 \ud835\ude37\ud835\ude26\ud835\ude24\ud835\ude35\ud835\ude30\ud835\ude33 \ud835\ude0b\ud835\ude09 \ud835\ude2a\ud835\ude2f \ud835\ude33\ud835\ude26\ud835\ude22\ud835\ude2d-\ud835\ude38\ud835\ude30\ud835\ude33\ud835\ude2d\ud835\ude25 \ud835\ude34\ud835\ude24\ud835\ude26\ud835\ude2f\ud835\ude22\ud835\ude33\ud835\ude2a\ud835\ude30\ud835\ude34 \u2193\nUsing various DL techniques, you can project your data points (images, videos, text, audio, user interactions) into the same vector space (aka the embeddings of the data).\nYou will load the embeddings along a payload (e.g., a URL to the image, date of creation, image description, properties, etc.) into the vector DB, where the data will be indexed along the:\n- vector\n- payload\n- text within the payload\nNow that the embedding indexes your data, you can query the vector DB by embedding any data point.\nFor example, you can query the vector DB with an image of your cat and use a filter to retrieve only \"black\" cats.\nTo do so, you must embed the image using the same model you used to embed the data within your vector DB. After you query the database using a given distance (e.g., cosine distance between 2 vectors) to find similar embeddings.\nThese similar embeddings have attached to them their payload that contains valuable information such as the URL to an image, a URL to a site, an ID of a user, a chapter from a book about the cat of a witch, etc.\n.\nUsing this technique, I used Qdrant to implement RAG for a financial assistant powered by LLMs.\nBut vector DBs go beyond LLMs & RAG.\n\ud835\ude0f\ud835\ude26\ud835\ude33\ud835\ude26 \ud835\ude2a\ud835\ude34 \ud835\ude22 \ud835\ude2d\ud835\ude2a\ud835\ude34\ud835\ude35 \ud835\ude30\ud835\ude27 \ud835\ude38\ud835\ude29\ud835\ude22\ud835\ude35 \ud835\ude3a\ud835\ude30\ud835\ude36 \ud835\ude24\ud835\ude22\ud835\ude2f \ud835\ude23\ud835\ude36\ud835\ude2a\ud835\ude2d\ud835\ude25 \ud835\ude36\ud835\ude34\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude37\ud835\ude26\ud835\ude24\ud835\ude35\ud835\ude30\ud835\ude33 \ud835\ude0b\ud835\ude09\ud835\ude34 (e.g., Qdrant ):\n- semantic image search:\nhttps://shorturl.at/hIKU9\n- recommender systems:\nhttps://shorturl.at/deMWZ\n- RAG framework:\nhttps://shorturl.at/uDTX1\n- anomalies detection:\nhttps://lnkd.in/dJDbat3Y\n.\n\u21b3\ud83d\udd17 \ud835\ude0a\ud835\ude29\ud835\ude26\ud835\ude24\ud835\ude2c \ud835\ude30\ud835\ude36\ud835\ude35 \ud835\ude18\ud835\ude25\ud835\ude33\ud835\ude22\ud835\ude2f\ud835\ude35'\ud835\ude34 \ud835\ude28\ud835\ude36\ud835\ude2a\ud835\ude25\ud835\ude26\ud835\ude34 \ud835\ude22\ud835\ude2f\ud835\ude25 \ud835\ude35\ud835\ude36\ud835\ude35\ud835\ude30\ud835\ude33\ud835\ude2a\ud835\ude22\ud835\ude2d\ud835\ude34 \ud835\ude35\ud835\ude30 \ud835\ude2d\ud835\ude26\ud835\ude22\ud835\ude33\ud835\ude2f \ud835\ude2e\ud835\ude30\ud835\ude33\ud835\ude26 \ud835\ude22\ud835\ude23\ud835\ude30\ud835\ude36\ud835\ude35 \ud835\ude37\ud835\ude26\ud835\ude24\ud835\ude35\ud835\ude30\ud835\ude33 \ud835\ude0b\ud835\ude09\ud835\ude34:\nhttps://lnkd.in/dJeNnUdS\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps.",
        "image": "https://media.licdn.com/dms/image/D4D22AQHi4MsHdSSpNQ/feedshare-shrink_2048_1536/0/1706081527330?e=1717027200&v=beta&t=dCuPmUSKisWblnuqqxc00vZSVp45-3t7I5jM1Z7InWs"
    },
    "Post_29": {
        "text": "This is \ud835\uddf5\ud835\uddfc\ud835\ude04 you can \ud835\uddf6\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01 a \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 to populate a \ud835\ude03\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddfc\ud835\uddff \ud835\uddd7\ud835\uddd5 to do \ud835\udde5\ud835\uddd4\ud835\uddda for a \ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddee\ud835\uddfb\ud835\uddf0\ud835\uddf6\ud835\uddee\ud835\uddf9 \ud835\uddee\ud835\ude00\ud835\ude00\ud835\uddf6\ud835\ude00\ud835\ude01\ud835\uddee\ud835\uddfb\ud835\ude01 powered by \ud835\udddf\ud835\udddf\ud835\udde0\ud835\ude00.\n\ud83d\udc1d All the following steps are wrapped in\nBytewax\nfunctions and connected in a single streaming pipeline (aka Bytewax flow) \u2193\n\ud835\uddd8\ud835\ude05\ud835\ude01\ud835\uddff\ud835\uddee\ud835\uddf0\ud835\ude01 \ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddee\ud835\uddfb\ud835\uddf0\ud835\uddf6\ud835\uddee\ud835\uddf9 \ud835\uddfb\ud835\uddf2\ud835\ude04\ud835\ude00 \ud835\uddf3\ud835\uddff\ud835\uddfc\ud835\uddfa \ud835\uddd4\ud835\uddf9\ud835\uddfd\ud835\uddee\ud835\uddf0\ud835\uddee\nYou need 2 types of inputs:\n1. A WebSocket API to listen to financial news in real time - used to listen 24/7 for new data and ingest it as soon as it is available.\n2. A RESTful API to ingest historical data in batch mode. When you deploy a fresh vector DB, you use it to populate it with older data.\nYou wrap the ingested HTML document and its metadata in a `pydantic` NewsArticle model to validate its schema.\nRegardless of the input type, the ingested data is the same. Thus, the following steps are the same for both data inputs \u2193\n\ud835\udde3\ud835\uddee\ud835\uddff\ud835\ude00\ud835\uddf2 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\udddb\ud835\udde7\ud835\udde0\ud835\udddf \ud835\uddf0\ud835\uddfc\ud835\uddfb\ud835\ude01\ud835\uddf2\ud835\uddfb\ud835\ude01\nAs the ingested financial news is in HTML, you must extract the text from particular HTML tags.\n`unstructured` makes it as easy as calling `partition_html(document)`, which will recursively return the text within all essential HTML tags.\nThe parsed NewsArticle model is mapped into another `pydantic` model to validate its new schema:\n- the headline\n- summary\n- full content.\n\ud835\uddd6\ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddfb \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\ude01\ud835\uddf2\ud835\ude05\ud835\ude01\nNow we have a bunch of text that has to be cleaned. Again, `unstructured` makes things easy. Calling a few functions we clean:\n- the dashes & bullets\n- extra whitespace & trailing punctuation\n- non ascii chars\n- invalid quotes\nFinally, we standardize everything to lowercase.\n\ud835\uddd6\ud835\uddf5\ud835\ude02\ud835\uddfb\ud835\uddf8 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\ude01\ud835\uddf2\ud835\ude05\ud835\ude01\nAs the text can exceed the context window of the embedding model, we have to chunk it.\nYet again, `unstructured` provides a valuable function that splits the text based on the tokenized text and expected input length of the embedding model.\nThis strategy is naive, as it doesn't consider the text's structure, such as chapters, paragraphs, etc. As the news is short, this is not an issue, but LangChain provides a `RecursiveCharacterTextSplitter` class that does that if required.\n\ud835\uddd8\ud835\uddfa\ud835\uddef\ud835\uddf2\ud835\uddf1 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddf0\ud835\uddf5\ud835\ude02\ud835\uddfb\ud835\uddf8\ud835\ude00\nYou pass all the chunks through an encoder-only model.\nWe have used `all-MiniLM-L6-v2` from `sentence-transformers`, a small model that can run on a CPU and outputs a 384 embedding.\nBut based on the size and complexity of your data, you might need more complex and bigger models.\n\ud835\udddf\ud835\uddfc\ud835\uddee\ud835\uddf1 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddee \ud835\uddf6\ud835\uddfb \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\udde4\ud835\uddf1\ud835\uddff\ud835\uddee\ud835\uddfb\ud835\ude01 \ud835\ude03\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddfc\ud835\uddff \ud835\uddd7\ud835\uddd5\nFinally, you insert the embedded chunks and their metadata into the Qdrant vector DB.\nThe metadata contains the embedded text, the source_url and the publish date.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
        "image": "https://media.licdn.com/dms/image/D4D22AQE5m0dyA3cZsA/feedshare-shrink_800/0/1705995067964?e=1717027200&v=beta&t=0azr0JQuIUBe-Duq4is0HLZFeo0pHLKqy_X6Ruk8AYs"
    },
    "Post_30": {
        "text": "Want to build your first \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf7\ud835\uddf2\ud835\uddf0\ud835\ude01 but don't know where to start? \u2192 If you want to \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb in a \ud835\ude00\ud835\ude01\ud835\uddff\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2\ud835\uddf1 \ud835\ude04\ud835\uddee\ud835\ude06 to \ud835\uddef\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1 \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa\ud835\ude00 using good \ud835\udddf\ud835\udddf\ud835\udde0\ud835\udde2\ud835\uddfd\ud835\ude00 principles...\nWe want to announce that we just \ud835\uddff\ud835\uddf2\ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\uddf1 \ud835\udff4 \ud835\udde0\ud835\uddf2\ud835\uddf1\ud835\uddf6\ud835\ude02\ud835\uddfa \ud835\uddf9\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddfc\ud835\uddfb\ud835\ude00 for the \ud835\udddb\ud835\uddee\ud835\uddfb\ud835\uddf1\ud835\ude00-\ud835\uddfc\ud835\uddfb \ud835\udddf\ud835\udddf\ud835\udde0\ud835\ude00 \ud835\uddf0\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00\ud835\uddf2 that will put you on the right track \u2193\n.\nWithin the \ud835\udff4 \ud835\udde0\ud835\uddf2\ud835\uddf1\ud835\uddf6\ud835\ude02\ud835\uddfa \ud835\uddf9\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddfc\ud835\uddfb\ud835\ude00, you will \ud835\uddf4\ud835\uddfc \ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfd-\ud835\uddef\ud835\ude06-\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfd through the \ud835\ude01\ud835\uddf5\ud835\uddf2\ud835\uddfc\ud835\uddff\ud835\ude06, \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa \ud835\uddf1\ud835\uddf2\ud835\ude00\ud835\uddf6\ud835\uddf4\ud835\uddfb, and \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2 to learn how to build a:\n\u2192 \ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf9-\ud835\ude01\ud835\uddf6\ud835\uddfa\ud835\uddf2 \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 (deployed on AWS) that uses Bytewax as the stream engine to listen to financial news, cleans & embeds the documents, and loads them to a vector DB\n\u2192 \ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddf2-\ud835\ude01\ud835\ude02\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registry\n\u2192 \ud835\uddf6\ud835\uddfb\ud835\uddf3\ud835\uddf2\ud835\uddff\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf2 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news)\nWe will also show you how to \ud835\uddf6\ud835\uddfb\ud835\ude01\ud835\uddf2\ud835\uddf4\ud835\uddff\ud835\uddee\ud835\ude01\ud835\uddf2 various \ud835\ude00\ud835\uddf2\ud835\uddff\ud835\ude03\ud835\uddf2\ud835\uddff\ud835\uddf9\ud835\uddf2\ud835\ude00\ud835\ude00 \ud835\ude01\ud835\uddfc\ud835\uddfc\ud835\uddf9\ud835\ude00, such as:\n\u2022 Comet ML as your ML Platform;\n\u2022 Qdrant as your vector DB;\n\u2022 Beam as your infrastructure.\n.\n\ud835\uddea\ud835\uddf5\ud835\uddfc \ud835\uddf6\ud835\ude00 \ud835\ude01\ud835\uddf5\ud835\uddf6\ud835\ude00 \ud835\uddf3\ud835\uddfc\ud835\uddff?\nThe series targets MLE, DE, DS, or SWE who want to learn to engineer LLM systems using LLMOps good principles.\n\ud835\udddb\ud835\uddfc\ud835\ude04 \ud835\ude04\ud835\uddf6\ud835\uddf9\ud835\uddf9 \ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb?\nThe series contains 4 hands-on video lessons and the open-source code you can access on GitHub.\n\ud835\uddd6\ud835\ude02\ud835\uddff\ud835\uddf6\ud835\uddfc\ud835\ude02\ud835\ude00?\nCheck out the 8 Medium lessons of the Hands-on LLMs course and start building your own LLMs system:\n\ud83d\udd17 \ud835\ude1b\ud835\ude29\ud835\ude26 \ud835\ude0f\ud835\ude22\ud835\ude2f\ud835\ude25\ud835\ude34-\ud835\ude30\ud835\ude2f \ud835\ude13\ud835\ude13\ud835\ude14\ud835\ude34 \ud835\ude14\ud835\ude26\ud835\ude25\ud835\ude2a\ud835\ude36\ud835\ude2e \ud835\ude1a\ud835\ude26\ud835\ude33\ud835\ude2a\ud835\ude26\ud835\ude34:\nhttps://lnkd.in/d8HWJiHg\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps.",
        "image": "https://media.licdn.com/dms/image/D4D22AQFtmazUCPllqA/feedshare-shrink_800/0/1705908608103?e=1717027200&v=beta&t=7kPGuFIgxFEo6KOZXoOimIsxb3HI8Zv4YL70ex_YFo0"
    },
    "Post_31": {
        "text": "\ud835\udddb\ud835\uddfc\ud835\ude04 \ud835\ude01\ud835\uddfc \ud835\uddee\ud835\uddf1\ud835\uddf1 \ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf9-\ud835\ude01\ud835\uddf6\ud835\uddfa\ud835\uddf2 \ud835\uddfa\ud835\uddfc\ud835\uddfb\ud835\uddf6\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\uddfa\ud835\uddf2\ud835\ude01\ud835\uddff\ud835\uddf6\ud835\uddf0\ud835\ude00 to your ML system.\nYour model is exposed to performance degradation after it is deployed to production.\nThat is why you need to monitor it constantly.\nThe most common way to monitor an ML model is to compute its metrics.\nBut for that, you need the ground truth.\n\ud835\udddc\ud835\uddfb \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb, \ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\uddf0\ud835\uddee\ud835\uddfb \ud835\uddee\ud835\ude02\ud835\ude01\ud835\uddfc\ud835\uddfa\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddf0\ud835\uddee\ud835\uddf9\ud835\uddf9\ud835\ude06 \ud835\uddee\ud835\uddf0\ud835\uddf0\ud835\uddf2\ud835\ude00\ud835\ude00 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddf4\ud835\uddff\ud835\uddfc\ud835\ude02\ud835\uddfb\ud835\uddf1 \ud835\ude01\ud835\uddff\ud835\ude02\ud835\ude01\ud835\uddf5 \ud835\uddf6\ud835\uddfb \ud835\udfef \ud835\uddfa\ud835\uddee\ud835\uddf6\ud835\uddfb \ud835\ude00\ud835\uddf0\ud835\uddf2\ud835\uddfb\ud835\uddee\ud835\uddff\ud835\uddf6\ud835\uddfc\ud835\ude00:\n1. near real-time: you can access it quite quickly\n2. delayed: you can access it after a considerable amount of time (e.g., one month)\n3. never: you have to label the data manually\n.\n\ud835\uddd9\ud835\uddfc\ud835\uddff \ud835\ude02\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\ude00 \ud835\udfee. \ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\udfef. \ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\uddf0\ud835\uddee\ud835\uddfb \ud835\uddfe\ud835\ude02\ud835\uddf6\ud835\uddf0\ud835\uddf8\ud835\uddf9\ud835\ude06 \ud835\uddf0\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\ude02\ud835\ude01\ud835\uddf2 \ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\uddfa\ud835\uddfc\ud835\uddfb\ud835\uddf6\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 \ud835\uddf6\ud835\uddfb \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddf3\ud835\uddfc\ud835\uddf9\ud835\uddf9\ud835\uddfc\ud835\ude04\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\ude04\ud835\uddee\ud835\ude06:\n- store the model predictions and GT as soon as they are available (these 2 will be out of sync -> you can't compute the metrics right away)\n- build a DAG (e.g., using Airflow) that extracts the predictions & GT computes the metrics in batch mode and loads them into another storage (e.g., GCS)\n- use an orchestration tool to run the DAG in the following scenarios:\n1. scheduled: if the GT is available in near real-time (e.g., hourly), then it makes sense to run your monitoring pipeline based on the known frequency\n2. triggered: if the GT is delayed and you don't know when it may come up, then you can implement a webhook to trigger your monitoring pipeline\n- attach a consumer to your storage to use and display the metrics (e.g., trigger alarms and display them in a dashboard)\n.\nIf you want to see how to implement a monitoring pipeline using Airflow and GCS, check out my article:\n\u21b3\ud83d\udd17 \ud835\ude0c\ud835\ude2f\ud835\ude34\ud835\ude36\ud835\ude33\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude1b\ud835\ude33\ud835\ude36\ud835\ude34\ud835\ude35\ud835\ude38\ud835\ude30\ud835\ude33\ud835\ude35\ud835\ude29\ud835\ude3a \ud835\ude14\ud835\ude13 \ud835\ude1a\ud835\ude3a\ud835\ude34\ud835\ude35\ud835\ude26\ud835\ude2e\ud835\ude34 \ud835\ude1e\ud835\ude2a\ud835\ude35\ud835\ude29 \ud835\ude0b\ud835\ude22\ud835\ude35\ud835\ude22 \ud835\ude1d\ud835\ude22\ud835\ude2d\ud835\ude2a\ud835\ude25\ud835\ude22\ud835\ude35\ud835\ude2a\ud835\ude30\ud835\ude2f \ud835\ude22\ud835\ude2f\ud835\ude25 \ud835\ude19\ud835\ude26\ud835\ude22\ud835\ude2d-\ud835\ude1b\ud835\ude2a\ud835\ude2e\ud835\ude26 \ud835\ude14\ud835\ude30\ud835\ude2f\ud835\ude2a\ud835\ude35\ud835\ude30\ud835\ude33\ud835\ude2a\ud835\ude2f\ud835\ude28:\nhttps://lnkd.in/eM7kHb8E\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps.",
        "image": "https://media.licdn.com/dms/image/D4D22AQEberYBV8_hKQ/feedshare-shrink_800/0/1705735874533?e=1717027200&v=beta&t=lIa6oPhyXYaW1mi-lm8wDU27z76uBALg0Ym2OeyK8BY"
    },
    "Post_32": {
        "text": "If you want to break into MLE or MLOps, check out this short interview where I briefly discussed my journey's ups and downs.\nThese kinds of videos are great to see that we are all people.\nWe are not perfect.\nWe make mistakes.\nWe have to experience many things before finding what we really enjoy in our lives.\nThank you,\nHopsworks\nand\nRik Van Bruggen\n, for inviting me. It was a great experience \ud83d\ude4f\nCheck it out to find out more about my journey \u2193\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
        "image": "https://media.licdn.com/dms/image/D4D22AQEKGOuFD_ZjgQ/feedshare-shrink_800/0/1705649585867?e=1717027200&v=beta&t=SViHipuxZpvlgw9Y0rtWgGEo8Z6qi-APoTNnNt2Dhrs"
    },
    "Post_33": {
        "text": "Happy Friday!\nHere's another\nHopsworks\n5-minute\nhashtag\n#\ninterview\nfor you, with some great insights, commentary and content from our\nhashtag\n#\ncommunity\n. This time I am speaking to\nPaul Iusztin\n- who is sharing his experience with\nhashtag\n#\nmachinelearning\nand\nhashtag\n#\nartificialintelligence\nat\nMetaphysic.ai\n. Hope you like the chat - and comments always welcome!\nhttps://lnkd.in/gEKpcjCG",
        "image": "https://media.licdn.com/dms/image/D4D22AQGUsi3lpqdeWQ/feedshare-shrink_800/0/1704353488146?e=1717027200&v=beta&t=UzKoFYEUi5fwk5S_bORw_6TbliAqEPe3IiyhCluqAdg"
    },
    "Post_34": {
        "text": "RAG systems are far from perfect \u2192 This free course teaches you how to improve your RAG system.\nI recently finished the \ud835\uddd4\ud835\uddf1\ud835\ude03\ud835\uddee\ud835\uddfb\ud835\uddf0\ud835\uddf2\ud835\uddf1 \ud835\udde5\ud835\uddf2\ud835\ude01\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude03\ud835\uddee\ud835\uddf9 \ud835\uddf3\ud835\uddfc\ud835\uddff \ud835\uddd4\ud835\udddc \ud835\ude04\ud835\uddf6\ud835\ude01\ud835\uddf5 \ud835\uddd6\ud835\uddf5\ud835\uddff\ud835\uddfc\ud835\uddfa\ud835\uddee free course from\nDeepLearning.AI\nIf you are into RAG, I find it among the most valuable learning sources.\nThe course already assumes you know what RAG is.\nIts primary focus is to show you all the current issues of RAG and why it is far from perfect.\nAfterward, it shows you the latest SoTA techniques to improve your RAG system, such as:\n- query expansion\n- cross-encoder re-ranking\n- embedding adaptors\nI am not affiliated with\nDeepLearning.AI\n(I wouldn't mind though).\nThis is a great course you should take if you are into RAG systems.\nThe good news is that it is free and takes only 1 hour.\nCheck it out \u2193\n\u21b3\ud83d\udd17 Advanced Retrieval for AI with Chroma:\nhttps://lnkd.in/dH4TZi8Q\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps.",
        "image": "https://media.licdn.com/dms/image/D4D22AQEmn1alF-aNGw/feedshare-shrink_800/0/1704267126276?e=1717027200&v=beta&t=xVyIS0yFG8fj2wfxzr9D-oPqQWvEZ-GfP0WaDaz6BUo"
    },
    "Post_35": {
        "text": "LLMOps here, LLMOps there, but did you take the time to see how it differs from MLOps?\nIf not, here is a 2-min LLMOps vs. MLOps summary \u2193\n\ud835\uddea\ud835\uddf5\ud835\uddee\ud835\ude01 \ud835\uddf6\ud835\ude00 \ud835\udddf\ud835\udddf\ud835\udde0\ud835\udde2\ud835\uddfd\ud835\ude00?\nWell, everything revolves around the idea that \"Size matters.\"\nLLMOps is about best practices for efficient deployment, monitoring, and maintenance, but this time, it is for large language models.\nLLMOps is a subset of MLOps, focusing on training & deploying large models trained on big data.\nIntuitive right?\n\ud835\uddd5\ud835\ude02\ud835\ude01 \ud835\uddf5\ud835\uddf2\ud835\uddff\ud835\uddf2 \ud835\uddee\ud835\uddff\ud835\uddf2 \ud835\udff1 \ud835\udddf\ud835\udddf\ud835\udde0\ud835\udde2\ud835\uddfd\ud835\ude00 \ud835\ude02\ud835\uddfb\ud835\uddf6\ud835\uddfe\ud835\ude02\ud835\uddf2 \ud835\uddf3\ud835\uddee\ud835\uddf0\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\ude00 \ud835\ude01\ud835\uddf5\ud835\uddee\ud835\ude01 \ud835\ude00\ud835\uddf2\ud835\ude01 \ud835\uddf6\ud835\ude01 \ud835\uddee\ud835\uddfd\ud835\uddee\ud835\uddff\ud835\ude01 \ud835\uddf3\ud835\uddff\ud835\uddfc\ud835\uddfa \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 \u2193\n\ud835\udfed. \ud835\uddd6\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\ude02\ud835\ude01\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb\ud835\uddee\ud835\uddf9 \ud835\uddff\ud835\uddf2\ud835\ude00\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\uddf0\ud835\uddf2\ud835\ude00: training your models on CUDA-enabled GPUs is more critical than ever, along with knowing how to run your jobs on a cluster of GPUs leveraging data & model parallelism using techniques such as ZeRO from DeepSpeed. Also, the high cost of inference makes model compression techniques essential for deployment.\n\ud835\udfee. \ud835\udde7\ud835\uddff\ud835\uddee\ud835\uddfb\ud835\ude00\ud835\uddf3\ud835\uddf2\ud835\uddff \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4: training models from scratch is a thing of the past. In most use cases, you will fine-tune the model on specific tasks, leveraging techniques such as LLaMA-Adapters or QLora.\n\ud835\udfef. \ud835\udddb\ud835\ude02\ud835\uddfa\ud835\uddee\ud835\uddfb \ud835\uddf3\ud835\uddf2\ud835\uddf2\ud835\uddf1\ud835\uddef\ud835\uddee\ud835\uddf0\ud835\uddf8: reinforcement learning from human feedback (RLHF) showed much potential in improving the quality of generated outputs. But to do RLHF, you have to introduce a feedback loop within your ML system that lets you evaluate the generated results based on human feedback, which are even further used to fine-tune your LLMs.\n\ud835\udff0. \ud835\uddda\ud835\ude02\ud835\uddee\ud835\uddff\ud835\uddf1\ud835\uddff\ud835\uddee\ud835\uddf6\ud835\uddf9\ud835\ude00: to create safe systems, you must protect your systems against harmful or violent inputs and outputs. Also, when designing your prompt templates, you must consider hallucinations and prompt hacking.\n\ud835\udff1. \ud835\udde0\ud835\uddfc\ud835\uddfb\ud835\uddf6\ud835\ude01\ud835\uddfc\ud835\uddff\ud835\uddf6\ud835\uddfb\ud835\uddf4 & \ud835\uddee\ud835\uddfb\ud835\uddee\ud835\uddf9\ud835\ude06\ud835\ude07\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\ude01\ud835\ude00: most ML platforms (e.g.,\nComet\nML) introduced specialized logging tools to debug and monitor your LLMs to help you find better prompt templates and protect against hallucination and hacking.\n.\nTo conclude...\nLLMOps isn't anything new for those familiar with MLOps and Deep Learning.\nFor example, training deep learning models on clusters of GPUs or fine-tuning them isn't new, but now it is more important than ever to master these skills as models get bigger.\nBut it indeed introduced novel techniques to fine-tune models (e.g., QLora), to merge the fields of RL and DL, and a plethora of tools around prompt manipulation & storing, such as:\n- vector DBs (e.g.,\nQdrant\n)\n- prompt chaining (e.g.,\nLangChain\n)\n- prompt logging & analytics (e.g., Comet LLMOps)\nWhat do you think? Is the term of LLMOps going to stick around?\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps.",
        "image": "https://media.licdn.com/dms/image/D4D22AQHhZmHkyXrrsQ/feedshare-shrink_800/0/1704180681017?e=1717027200&v=beta&t=iglKTCwtgRYWT8Trf_3HyUnLgLmPJ5dRRTwwFO57e2Q"
    },
    "Post_36": {
        "text": "Ever wondered \ud835\uddf5\ud835\uddfc\ud835\ude04 to \ud835\uddf1\ud835\uddf2\ud835\uddfd\ud835\uddf9\ud835\uddfc\ud835\ude06 in <\ud835\udfef\ud835\udfec \ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\ude02\ud835\ude01\ud835\uddf2\ud835\ude00 \ud835\uddfc\ud835\uddfd\ud835\uddf2\ud835\uddfb-\ud835\ude00\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\uddf0\ud835\uddf2 \ud835\udddf\ud835\udddf\ud835\udde0\ud835\ude00, such as \ud835\udddf\ud835\uddf9\ud835\uddee\ud835\uddfa\ud835\uddee\ud835\udfee, on \ud835\uddd4\ud835\uddea\ud835\udde6 \ud835\udde6\ud835\uddee\ud835\uddf4\ud835\uddf2\ud835\udde0\ud835\uddee\ud835\uddf8\ud835\uddf2\ud835\uddff? Then wonder no more \u2193\nThe sweet thing about SageMaker is that it accelerates the development process, enabling a more efficient and rapid transition to the production stage.\nAlex Vesa\nsmashed with his first article on DML about showing step-by-step how to deploy an LLM from HuggingFace to AWS SageMaker using good practices, such as:\n- designing a config class for the deployment of the LLM\n- set up AWS and deploy the LLM to SageMaker\n- implement an inference class to call the deployed LLM in real time through a web endpoint\n- define a prompt template function to ensure reproducibility & consistency\n...and, ultimately, how to play yourself with your freshly deployed LLM.\n.\nIf that is something for you, check it out for FREE on Decoding ML \u2193\n\ud83d\udd17  \ud835\ude10\ud835\ude2f\ud835\ude35\ud835\ude33\ud835\ude30\ud835\ude25\ud835\ude36\ud835\ude24\ud835\ude35\ud835\ude2a\ud835\ude30\ud835\ude2f \ud835\ude35\ud835\ude30 \ud835\ude0b\ud835\ude26\ud835\ude31\ud835\ude2d\ud835\ude30\ud835\ude3a\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude17\ud835\ude33\ud835\ude2a\ud835\ude37\ud835\ude22\ud835\ude35\ud835\ude26 \ud835\ude13\ud835\ude13\ud835\ude14\ud835\ude34 \ud835\ude38\ud835\ude2a\ud835\ude35\ud835\ude29 \ud835\ude08\ud835\ude1e\ud835\ude1a \ud835\ude1a\ud835\ude22\ud835\ude28\ud835\ude26\ud835\ude14\ud835\ude22\ud835\ude2c\ud835\ude26\ud835\ude33: \ud835\ude0d\ud835\ude30\ud835\ude24\ud835\ude36\ud835\ude34 \ud835\ude30\ud835\ude2f \ud835\ude13\ud835\ude2d\ud835\ude22\ud835\ude2e\ud835\ude222-7\ud835\ude23-\ud835\ude24\ud835\ude29\ud835\ude22\ud835\ude35:\nhttps://lnkd.in/d6vjXHda\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps.",
        "image": "https://media.licdn.com/dms/image/D4D10AQH2uw3kdUp74Q/image-shrink_800/0/1703835003861?e=1715004000&v=beta&t=im-4LjMcFusR2W7wTcJblbxVX_UntuNDYqGZpxhoWeI"
    },
    "Post_37": {
        "text": "Here is a step-by-step guide on designing the architecture of a financial assistant powered by LLMs, vector DBs and MLOps.\nThe 3-pipeline design, also known as the FTI architecture, makes things simple \u2193\n\ud835\uddd9\ud835\uddf2\ud835\uddee\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2 \ud835\udde3\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\nWe build a streaming pipeline that listens to real-time financial news, embeds the news, and loads everything in a vector DB. The goal is to add up-to-date news to the user's questions using RAG to avoid retraining.\n1. We listen 24/7 to financial news from Alpaca through a WebSocket wrapped over a\nBytewax\nconnector\n2. Once any financial news is received, these are passed to the Bytewax flow that:\n- extracts & cleans the information from the news HTML document\n- chunks the text based on the LLM's max context window\n- embeds all the chunks using the \"all-MiniLM-L6-v2\" encoder-only model from sentence-transformers\n- inserts all the embeddings + metadata to\nQdrant\n3. The streaming pipeline is deployed to an EC2 that runs multiple Bytewax processes (or to K8s to scale up).\n\ud835\udde7\ud835\uddff\ud835\uddee\ud835\uddf6\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\udde3\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\nWe fine-tune a pretrained LLM to specialize the model to answer financial-based questions.\n1. Manually fill ~100 financial questions.\n2. Use RAG to enrich the questions using the financial news from the Qdrant vector DB.\n3. Use a powerful LLM (e.g., GPT-4) to answer them, or hire an expert if you have more time and resources.\n4. Load Falcon from HuggingFace using QLoRA to fit on a single GPU.\n5. Preprocess the Q&A dataset into prompts.\n6. Fine-tune the LLM and log all the artifacts to\nComet\n's experiment tracker (loss, model weights, etc.)\n7. For every epoch, run the LLM on your test set, log the prompts to Comet's prompt logging feature and compute the metrics.\n8. Send the best LoRA weights to the model registry as the next production candidate.\n9. Deploy steps 4-8 to Beam to run the training on an A10G or A100\n\ud835\udddc\ud835\uddfb\ud835\uddf3\ud835\uddf2\ud835\uddff\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf2 \ud835\udde3\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\nWe hook the financial news stored in the Qdrant Vector DB and the Falcon fine-tuned model into a single entity exposed under a RESTful API.\nSteps 1-7 are all chained together using LangChain.\n1. Use the \"all-MiniLM-L6-v2\" model to embed the user's question.\n2. Using the question embedding, query the vector DB to find the top 3 related financial news.\n3. Attach the news's text (stored as metadata) to the prompt (aka RAG).\n4. Download Falcon's pretrained weights from HF & LoRA's fine-tuned weights from Comet's model registry.\n5. Load the LLM and pass the prompt to it.\n6. Store the conversation in LangChain's memory.\n7. Deploy steps 1-7 under a RESTful API using Beam.\n.\nCheck out the \ud835\udddb\ud835\uddee\ud835\uddfb\ud835\uddf1\ud835\ude00-\ud835\uddfc\ud835\uddfb \ud835\udddf\ud835\udddf\ud835\udde0\ud835\ude00 course to see this in action:\n\u21b3\ud83d\udd17\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
        "image": "https://media.licdn.com/dms/image/D4D10AQGGrorLqsPGiQ/image-shrink_800/0/1703748604581?e=1715004000&v=beta&t=NEqmaUD3au04uROLG-x9-0lVM9cO1Ry4S3PhGNV53XU"
    },
    "Post_38": {
        "text": "This is the \ud835\uddfc\ud835\uddfb\ud835\uddf9\ud835\ude06 \ud835\uddff\ud835\uddf2\ud835\ude00\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\uddf0\ud835\uddf2 \ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\uddfb\ud835\uddf2\ud835\uddf2\ud835\uddf1 to \ud835\uddfd\ud835\uddf6\ud835\uddf0\ud835\uddf8 the \ud835\uddff\ud835\uddf6\ud835\uddf4\ud835\uddf5\ud835\ude01 \ud835\ude03\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddfc\ud835\uddff \ud835\uddd7\ud835\uddd5 for your exact \ud835\ude02\ud835\ude00\ud835\uddf2 \ud835\uddf0\ud835\uddee\ud835\ude00\ud835\uddf2.\nSince ChatGPT made AI cool, besides the millions of ChatGPT posts you got tired of and blocked, you realized that a new type of tool started to hit the scene: Vector DBs.\nAs vector DBs play a crucial role in most LLM applications, they popped out everywhere.\nOn this day, there are 37 vector DB solutions, that are constantly changing and adding features.\n\ud835\ude15\ud835\ude30\ud835\ude38, \ud835\ude29\ud835\ude30\ud835\ude38 \ud835\ude35\ud835\ude29\ud835\ude26 \ud835\ude29**\ud835\ude2d \ud835\ude34\ud835\ude29\ud835\ude30\ud835\ude36\ud835\ude2d\ud835\ude25 \ud835\ude10 \ud835\ude31\ud835\ude2a\ud835\ude24\ud835\ude2c \ud835\ude30\ud835\ude2f\ud835\ude26?\n\ud835\ude43\ud835\ude5a\ud835\ude67\ud835\ude5a \ud835\ude5e\ud835\ude68 \ud835\ude6c\ud835\ude5d\ud835\ude5a\ud835\ude67\ud835\ude5a \ud835\ude69\ud835\ude5d\ud835\ude5a \"\ud835\ude51\ud835\ude5a\ud835\ude58\ud835\ude69\ud835\ude64\ud835\ude67 \ud835\ude3f\ud835\ude3d \ud835\ude3e\ud835\ude64\ud835\ude62\ud835\ude65\ud835\ude56\ud835\ude67\ud835\ude5e\ud835\ude68\ud835\ude64\ud835\ude63\" \ud835\ude60\ud835\ude5e\ud835\ude58\ud835\ude60\ud835\ude68 \ud835\ude5e\ud835\ude63.\nIt is an effort managed by\nSuperlinked\n, where they carefully compared all these 37 vector DBs across 29 features, such as:\n- License\n- GitHub \u2b50\n- support for text, image or struct models\n- RAG, RecSys, LangChain or LllamaIndex APIs\n- pricing\n- sharding\n- document size\n- vector dims\n...and more!\nI won't list all 29 features.\nYou have to check it out to see them for yourself \u2193\n\u21b3 \ud83d\udd17 Vector DB Comparison:\nhttps://lnkd.in/d2v3S2Jp\n\ud835\udde1\ud835\uddfc\ud835\ude01\ud835\uddf2: To keep the table updated or add more features, you can contribute to it yourself.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps.",
        "image": "https://media.licdn.com/dms/image/D4D10AQFJgKkXXDKT6g/image-shrink_800/0/1703575807389?e=1715004000&v=beta&t=KkSMco4q8cCJo09y01jhDqb1n_jjnJSFzXlBA_Iz32I"
    },
    "Post_39": {
        "text": "Here are \ud835\udfef \ud835\uddf4\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\ude01 \ud835\ude01\ud835\uddf6\ud835\uddfd\ud835\ude00 from \ud835\udde6\ud835\uddee\ud835\uddfa \ud835\uddd4\ud835\uddf9\ud835\ude01\ud835\uddfa\ud835\uddee\ud835\uddfb about \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 & \ud835\uddfa\ud835\uddee\ud835\uddf8\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf1\ud835\uddf2\ud835\uddf0\ud835\uddf6\ud835\ude00\ud835\uddf6\ud835\uddfc\ud835\uddfb\ud835\ude00 that can benefit you to \ud835\uddf9\ud835\uddf2\ud835\ude03\ud835\uddf2\ud835\uddf9 \ud835\ude02\ud835\uddfd your \ud835\udde0\ud835\udddf\ud835\uddd8 or \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 game.\nHe mainly referred to \"business\" decisions, but I think they can significantly be transferred to pursuing any goal.\n\u2192 \ud835\ude4e\ud835\ude6a\ud835\ude58\ud835\ude5d \ud835\ude56\ud835\ude68 \ud835\ude57\ud835\ude67\ud835\ude5a\ud835\ude56\ud835\ude60\ud835\ude5e\ud835\ude63\ud835\ude5c \ud835\ude5e\ud835\ude63\ud835\ude69\ud835\ude64 \ud835\ude48\ud835\ude47\ud835\ude40 \ud835\ude64\ud835\ude67 \ud835\ude61\ud835\ude5a\ud835\ude6b\ud835\ude5a\ud835\ude61\ud835\ude5e\ud835\ude63\ud835\ude5c \ud835\ude6a\ud835\ude65 \ud835\ude6e\ud835\ude64\ud835\ude6a\ud835\ude67 \ud835\ude48\ud835\ude47\ud835\ude4a\ud835\ude4b\ud835\ude4e \ud835\ude5c\ud835\ude56\ud835\ude62\ud835\ude5a.\n\ud835\udfed. \ud835\uddd6\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\uddfc\ud835\ude02\ud835\uddfb\ud835\uddf1 \ud835\udddc\ud835\uddfb\ud835\ude01\ud835\uddf2\ud835\uddff\ud835\uddf2\ud835\ude00\ud835\ude01 \ud835\uddf6\ud835\uddfb \ud835\uddec\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\uddd6\ud835\uddee\ud835\uddff\ud835\uddf2\ud835\uddf2\ud835\uddff\nThink about it like this - your career is much like your retirement fund. The more you put in early, you'll have to chill on later. It's not just adding up; it's multiplying.\n- Early career efforts have a compounding effect.\n- It's about working smart and persistently.\n- This stage sets your career trajectory.\n\ud835\udfee. \ud835\uddde\ud835\uddfb\ud835\uddfc\ud835\ude04\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddea\ud835\uddf5\ud835\uddf2\ud835\uddfb \ud835\ude01\ud835\uddfc \ud835\udde6\ud835\ude01\ud835\uddf6\ud835\uddf0\ud835\uddf8 \ud835\uddfc\ud835\uddff \ud835\udde4\ud835\ude02\ud835\uddf6\ud835\ude01\nSometimes, it's hard to tell if you need to keep pushing or if it's time to change things up. But don't let all the noise around you make that decision. Listen to your gut and remember, it's okay to pivot when you're out of ideas and nothing works.\n- Quitting too soon is a common mistake.\n- The decision to continue should be internal.\n- Pivot when you're out of ideas, not just because of external opinions.\n\ud835\udfef. \ud835\udde6\ud835\ude02\ud835\ude00\ud835\ude01\ud835\uddee\ud835\uddf6\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\udde0\ud835\uddfc\ud835\ude01\ud835\uddf6\ud835\ude03\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\udde2\ud835\ude03\ud835\uddf2\ud835\uddff \ud835\udde7\ud835\uddf6\ud835\uddfa\ud835\uddf2\nFind what gets you up in the morning and keeps you grinding late into the night. Trust me, it's better to be fueled by your passion for your work rather than dreams of fame and fortune.\n- Intrinsic motivation is critical for longevity.\n- Success needs belief in your work and enjoying it.\n- Initial motives (fame, money) often evolve into deeper missions.\n.\nSo, what about you? How do you see these principles playing out in your life?\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    "Post_40": {
        "text": "Do you want to \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb to \ud835\uddef\ud835\ude02\ud835\uddf6\ud835\uddf9\ud835\uddf1 \ud835\uddf5\ud835\uddee\ud835\uddfb\ud835\uddf1\ud835\ude00-\ud835\uddfc\ud835\uddfb \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\ude00\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa\ud835\ude00 using good \ud835\udddf\ud835\udddf\ud835\udde0\ud835\udde2\ud835\uddfd\ud835\ude00 practices? A \ud835\uddfb\ud835\uddf2\ud835\ude04 \ud835\udde0\ud835\uddf2\ud835\uddf1\ud835\uddf6\ud835\ude02\ud835\uddfa \ud835\ude00\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude00 is \ud835\uddf0\ud835\uddfc\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 up for the \ud835\udddb\ud835\uddee\ud835\uddfb\ud835\uddf1\ud835\ude00-\ud835\uddfc\ud835\uddfb \ud835\udddf\ud835\udddf\ud835\udde0\ud835\ude00 \ud835\uddf0\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00\ud835\uddf2\n.\nBy finishing the \ud835\udddb\ud835\uddee\ud835\uddfb\ud835\uddf1\ud835\ude00-\ud835\udde2\ud835\uddfb \ud835\udddf\ud835\udddf\ud835\udde0\ud835\ude00 \ud835\uddf3\ud835\uddff\ud835\uddf2\ud835\uddf2 course, you will learn how to use the 3-pipeline architecture & LLMOps good practices to design, build, and deploy a real-time financial advisor powered by LLMs & vector DBs.\nWe will primarily focus on the engineering & MLOps aspects.\nThus, by the end of this series, you will know how to build & deploy a real ML system, not some isolated code in Notebooks.\n\ud835\ude1b\ud835\ude29\ud835\ude26\ud835\ude33\ud835\ude26 \ud835\ude22\ud835\ude33\ud835\ude26 3 \ud835\ude24\ud835\ude30\ud835\ude2e\ud835\ude31\ud835\ude30\ud835\ude2f\ud835\ude26\ud835\ude2f\ud835\ude35\ud835\ude34 \ud835\ude3a\ud835\ude30\ud835\ude36 \ud835\ude38\ud835\ude2a\ud835\ude2d\ud835\ude2d \ud835\ude2d\ud835\ude26\ud835\ude22\ud835\ude33\ud835\ude2f \ud835\ude35\ud835\ude30 \ud835\ude23\ud835\ude36\ud835\ude2a\ud835\ude2d\ud835\ude25 \ud835\ude25\ud835\ude36\ud835\ude33\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude35\ud835\ude29\ud835\ude26 \ud835\ude24\ud835\ude30\ud835\ude36\ud835\ude33\ud835\ude34\ud835\ude26:\n- a real-time streaming pipeline\n- a fine-tuning pipeline\n- an inference pipeline\n.\nWe have already released the code and video lessons of the Hands-on LLM course.\nBut we are excited to announce an \ud835\udff4-\ud835\uddf9\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddfc\ud835\uddfb \ud835\udde0\ud835\uddf2\ud835\uddf1\ud835\uddf6\ud835\ude02\ud835\uddfa \ud835\ude00\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude00 that will \ud835\uddf1\ud835\uddf6\ud835\ude03\ud835\uddf2 \ud835\uddf1\ud835\uddf2\ud835\uddf2\ud835\uddfd into the \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2 and \ud835\uddf2\ud835\ude05\ud835\uddfd\ud835\uddf9\ud835\uddee\ud835\uddf6\ud835\uddfb everything \ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfd-\ud835\uddef\ud835\ude06-\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfd.\n\ud835\uddea\ud835\uddf2 \ud835\uddf5\ud835\uddee\ud835\ude03\ud835\uddf2 \ud835\uddee\ud835\uddf9\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf1\ud835\ude06 \ud835\uddff\ud835\uddf2\ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\ude00\ud835\uddf2\ud835\uddf1 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddf3\ud835\uddf6\ud835\uddff\ud835\ude00\ud835\ude01 \ud835\uddf9\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddfc\ud835\uddfb \ud835\uddfc\ud835\uddf3 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\ude00\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude00 \u2193\n\ud835\ude1b\ud835\ude29\ud835\ude26 \ud835\ude13\ud835\ude13\ud835\ude14\ud835\ude34 \ud835\ude2c\ud835\ude2a\ud835\ude35: \ud835\ude09\ud835\ude36\ud835\ude2a\ud835\ude2d\ud835\ude25 \ud835\ude22 \ud835\ude31\ud835\ude33\ud835\ude30\ud835\ude25\ud835\ude36\ud835\ude24\ud835\ude35\ud835\ude2a\ud835\ude30\ud835\ude2f-\ud835\ude33\ud835\ude26\ud835\ude22\ud835\ude25\ud835\ude3a \ud835\ude33\ud835\ude26\ud835\ude22\ud835\ude2d-\ud835\ude35\ud835\ude2a\ud835\ude2e\ud835\ude26 \ud835\ude27\ud835\ude2a\ud835\ude2f\ud835\ude22\ud835\ude2f\ud835\ude24\ud835\ude2a\ud835\ude22\ud835\ude2d \ud835\ude22\ud835\ude25\ud835\ude37\ud835\ude2a\ud835\ude34\ud835\ude30\ud835\ude33 \ud835\ude34\ud835\ude3a\ud835\ude34\ud835\ude35\ud835\ude26\ud835\ude2e \ud835\ude36\ud835\ude34\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude34\ud835\ude35\ud835\ude33\ud835\ude26\ud835\ude22\ud835\ude2e\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude31\ud835\ude2a\ud835\ude31\ud835\ude26\ud835\ude2d\ud835\ude2a\ud835\ude2f\ud835\ude26\ud835\ude34, \ud835\ude19\ud835\ude08\ud835\ude0e, \ud835\ude22\ud835\ude2f\ud835\ude25 \ud835\ude13\ud835\ude13\ud835\ude14\ud835\ude16\ud835\ude31\ud835\ude34: \ud83d\udd17\nhttps://lnkd.in/e_msvXah\n\u21b3 In \ud835\udddf\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddfc\ud835\uddfb \ud835\udfed, you will \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb how to \ud835\uddf1\ud835\uddf2\ud835\ude00\ud835\uddf6\ud835\uddf4\ud835\uddfb a \ud835\uddf3\ud835\uddf6\ud835\uddfb\ud835\uddee\ud835\uddfb\ud835\uddf0\ud835\uddf6\ud835\uddee\ud835\uddf9 \ud835\uddee\ud835\ude00\ud835\ude00\ud835\uddf6\ud835\ude00\ud835\ude01\ud835\uddee\ud835\uddfb\ud835\ude01 using the \ud835\udfef-\ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 \ud835\uddee\ud835\uddff\ud835\uddf0\ud835\uddf5\ud835\uddf6\ud835\ude01\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2 (also known as the FTI architecture), powered by:\n- LLMs\n- vector DBs\n- a streaming engine\n- LLMOps\n.\n\u2192 The rest of the articles will be released by the end of January 2024.\nFollow us on Medium's Decoding ML publication to get notified when we publish the other lessons: \ud83d\udd17\nhttps://lnkd.in/dTizqHG7\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience"
    },
    "Post_41": {
        "text": "Ever thought about how \ud835\uddee \ud835\ude06\ud835\uddf2\ud835\uddee\ud835\uddff can \ud835\uddf0\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude01\ud835\uddf2\ud835\uddf9\ud835\ude06 \ud835\uddf0\ud835\uddf5\ud835\uddee\ud835\uddfb\ud835\uddf4\ud835\uddf2 your \ud835\ude03\ud835\uddf6\ud835\ude00\ud835\uddf6\ud835\uddfc\ud835\uddfb? \ud835\udfee\ud835\udfec\ud835\udfee\ud835\udfef did just that for me...\nI don't really care about New Year's Eve. It is just another trip around the sun.\n...but it is a great moment to pause and think about the last year and the future.\nPersonally, I love planning things and learning from my past. It helps me gain perspective and prioritize the important things in my life.\n.\n\ud835\ude1a\ud835\ude30 \ud835\ude29\ud835\ude26\ud835\ude33\ud835\ude26 \ud835\ude22\ud835\ude33\ud835\ude26 6 \ud835\ude35\ud835\ude29\ud835\ude2a\ud835\ude2f\ud835\ude28\ud835\ude34 \ud835\ude35\ud835\ude29\ud835\ude22\ud835\ude35 \ud835\ude29\ud835\ude22\ud835\ude31\ud835\ude31\ud835\ude26\ud835\ude2f\ud835\ude26\ud835\ude25 \ud835\ude2a\ud835\ude2f 2023 \ud835\ude35\ud835\ude29\ud835\ude22\ud835\ude35 \ud835\ude10 \ud835\ude22\ud835\ude2e \ud835\ude28\ud835\ude33\ud835\ude22\ud835\ude35\ud835\ude26\ud835\ude27\ud835\ude36\ud835\ude2d \ud835\ude27\ud835\ude30\ud835\ude33:\n1. Enough discipline to work out almost daily & eat healthy.\n2. Grew my MLE & MLOps content creation business.\n3. Created x2 open-source MLE & MLOps free courses that accumulated >1800 stars on GitHub.\n4. Met many incredible MLE & MLOps people from Europe and the US.\n5. Had the chance to work on awesome deep fake projects in production at Metaphysic\n6. Adopted my second cat: Arthur. I love this little fellow. Now we are a happy 2 people - 2 cat family.\n...\ud835\ude22\ud835\ude2f\ud835\ude25 \ud835\ude2e\ud835\ude3a \ud835\ude35\ud835\ude30\ud835\ude31 6 \ud835\ude35\ud835\ude29\ud835\ude2a\ud835\ude2f\ud835\ude28\ud835\ude34 \ud835\ude10 \ud835\ude31\ud835\ude2d\ud835\ude22\ud835\ude2f \ud835\ude35\ud835\ude30 \ud835\ude25\ud835\ude30 \ud835\ude2a\ud835\ude2f 2024:\n1. Grow the \ud835\uddd7\ud835\uddf2\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\udde0\ud835\udddf \ud835\uddfd\ud835\ude02\ud835\uddef\ud835\uddf9\ud835\uddf6\ud835\uddf0\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb. I have some exciting news here that I will share in the following days.\n2. Travel & work from a different country every 3-4 months to level up my social and emotional skills while exploring the world.\n3. Level up my MLE & MLOps production-ready skills on real-world projects to share better insights with you.\n4. Better understand myself and be 100% true to myself.\n5. Start creating video content.\n6. This is a grand one: Move to the city center to leave my house more.\n.\nMy final take is that you should always take care of your mind & body as much as your tech skills.\nIf you degrade, your skills degrade with you.\nLet the games begin!\nWhat are your main goals for 2024, or what are you grateful for from last year?\n.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\npersonaldevelopment\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    "Post_42": {
        "text": "Do you want to \ud835\uddf9\ud835\uddf2\ud835\ude03\ud835\uddf2\ud835\uddf9 \ud835\ude02\ud835\uddfd your \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb-\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddf1\ud835\ude06 \ud835\udde0\ud835\udddf\ud835\uddd8 & \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 game?\nThen I have some great news \ud83d\udd25\nSince I started creating content, I learned one crucial thing: \"\ud835\ude0c\ud835\ude37\ud835\ude26\ud835\ude33\ud835\ude3a\ud835\ude23\ud835\ude30\ud835\ude25\ud835\ude3a \ud835\ude2d\ud835\ude2a\ud835\ude2c\ud835\ude26\ud835\ude34 \ud835\ude35\ud835\ude30 \ud835\ude33\ud835\ude26\ud835\ude22\ud835\ude25 \ud835\ude22\ud835\ude2f\ud835\ude25 \ud835\ude2d\ud835\ude26\ud835\ude22\ud835\ude33\ud835\ude2f \ud835\ude25\ud835\ude2a\ud835\ude27\ud835\ude27\ud835\ude26\ud835\ude33\ud835\ude26\ud835\ude2f\ud835\ude35\ud835\ude2d\ud835\ude3a.\"\nThat is why I \ud835\ude00\ud835\ude01\ud835\uddee\ud835\uddff\ud835\ude01\ud835\uddf2\ud835\uddf1 my own \ud835\udde0\ud835\uddf2\ud835\uddf1\ud835\uddf6\ud835\ude02\ud835\uddfa \ud835\uddfd\ud835\ude02\ud835\uddef\ud835\uddf9\ud835\uddf6\ud835\uddf0\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb, called after my newsletter: \"\ud835\ude0b\ud835\ude26\ud835\ude24\ud835\ude30\ud835\ude25\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude14\ud835\ude13\"\nStarting from now, all my articles can be found under this Medium publication: \ud83d\udd17\nhttps://lnkd.in/dTizqHG7\nCurrently, it is empty, but in January 2024, I plan to drop \ud835\udff5 \ud835\ude00\ud835\ude02\ud835\uddff\ud835\uddfd\ud835\uddff\ud835\uddf6\ud835\ude00\ud835\uddf2\ud835\ude00 in my new \ud835\uddd7\ud835\uddf2\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\udde0\ud835\udddf \ud835\udde0\ud835\uddf2\ud835\uddf1\ud835\uddf6\ud835\ude02\ud835\uddfa \ud835\uddfd\ud835\ude02\ud835\uddef\ud835\uddf9\ud835\uddf6\ud835\uddf0\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb.\nIf you have liked my content so far...\n-> Support me and follow my new publication as you will enjoy what I prepared: \ud83d\udd17\nhttps://lnkd.in/dTizqHG7\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    "Post_43": {
        "text": "2023 was crazy. Here are my \ud835\uddff\ud835\uddf2\ud835\ude00\ud835\ude02\ud835\uddf9\ud835\ude01\ud835\ude00 after \ud835\uddfc\ud835\uddfb\ud835\uddf2 \ud835\ude06\ud835\uddf2\ud835\uddee\ud835\uddff of \ud835\ude04\ud835\uddff\ud835\uddf6\ud835\ude01\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf0\ud835\uddfc\ud835\uddfb\ud835\ude01\ud835\uddf2\ud835\uddfb\ud835\ude01 about \ud835\udde0\ud835\udddf\ud835\uddd8 and \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 and starting as a nobody.\n...not that now I am somebody \ud83d\ude02\nI had some humble goals. At the beginning of 2022, I hadn't imagined I would plan to make a living out of creating content:\n- \ud835\udddf\ud835\uddf6\ud835\uddfb\ud835\uddf8\ud835\uddf2\ud835\uddf1\ud835\udddc\ud835\uddfb: 3k -> 22.5k followers (goal 10k)\n- \ud835\udde0\ud835\uddf2\ud835\uddf1\ud835\uddf6\ud835\ude02\ud835\uddfa: 200 -> 1.3k followers (goal 1k)\n...but I stumbled on some great sources of inspiration and people who helped me grow more than I could have imagined.\nThus, during 2023, I got excited and experimented with quite a few things, such as:\n- \ud835\ude05\ud835\udfee \ud835\uddfc\ud835\uddfd\ud835\uddf2\ud835\uddfb-\ud835\ude00\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\uddf0\ud835\uddf2 \ud835\uddf0\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00\ud835\uddf2\ud835\ude00 about MLOps, LLMOps, and MLE that accumulated over 1800 GitHub stars:\n-> \ud835\ude0f\ud835\ude22\ud835\ude2f\ud835\ude25\ud835\ude34-\ud835\ude30\ud835\ude2f \ud835\ude13\ud835\ude13\ud835\ude14\ud835\ude34:\nhttps://lnkd.in/dZgqtf8f\n-> \ud835\ude1b\ud835\ude29\ud835\ude26 \ud835\ude0d\ud835\ude36\ud835\ude2d\ud835\ude2d \ud835\ude1a\ud835\ude35\ud835\ude22\ud835\ude24\ud835\ude2c 7-\ud835\ude1a\ud835\ude35\ud835\ude26\ud835\ude31\ud835\ude34 \ud835\ude14\ud835\ude13\ud835\ude16\ud835\ude31\ud835\ude34 \ud835\ude0d\ud835\ude33\ud835\ude22\ud835\ude2e\ud835\ude26\ud835\ude38\ud835\ude30\ud835\ude33\ud835\ude2c:\nhttps://lnkd.in/d5HUN39Q\n- I started my newsletter: \"\ud835\uddd7\ud835\uddf2\ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\udde0\ud835\udddf\", which grew to 2.8k followers\n- I tried to grow my \ud835\udde7\ud835\ude04\ud835\uddf6\ud835\ude01\ud835\ude01\ud835\uddf2\ud835\uddff/\ud835\uddeb account (only 233 followers): this was quite a mess as I haven't    tweaked my posts well enough to satisfy the\n.\n2023 \ud835\ude6c\ud835\ude56\ud835\ude68 \ud835\ude56\ud835\ude63 \ud835\ude5a\ud835\ude6d\ud835\ude58\ud835\ude5e\ud835\ude69\ud835\ude5e\ud835\ude63\ud835\ude5c \ud835\ude6e\ud835\ude5a\ud835\ude56\ud835\ude67, \ud835\ude56\ud835\ude63\ud835\ude59 \ud835\ude44 \ud835\ude5d\ud835\ude56\ud835\ude6b\ud835\ude5a \ud835\ude68\ud835\ude64\ud835\ude62\ud835\ude5a \ud835\ude69\ud835\ude5d\ud835\ude67\ud835\ude5e\ud835\ude61\ud835\ude61\ud835\ude5e\ud835\ude63\ud835\ude5c \ud835\ude65\ud835\ude61\ud835\ude56\ud835\ude63\ud835\ude68 \ud835\ude5b\ud835\ude64\ud835\ude67 2024.\nBut...\nI want to thank everybody who followed me and engaged with my content. You are one of the first drivers that keep me going. So... Thank you \ud83d\ude4f\nSecondly, setting such goals isn't that essential. What if I haven't reached out to any of them? That would have demoralized me entirely. But, I think they are critical in giving a clear direction.\nRemember: \"Direction is more important than... anything.\"\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\npersonaldevelopment\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    "Post_44": {
        "text": "If anyone told you that \ud835\udde0\ud835\udddf or \ud835\udde0\ud835\udddf\ud835\udde2\ud835\uddfd\ud835\ude00 is \ud835\uddf2\ud835\uddee\ud835\ude00\ud835\ude06, they were \ud835\uddff\ud835\uddf6\ud835\uddf4\ud835\uddf5\ud835\ude01.\nHere is a simple trick that I learned the hard way \u2193\nIf you are in this domain, you already know that everything changes fast:\n- a new tool every month\n- a new model every week\n- a new project every day\nYou know what I did? I stopped caring about all these changes and switched my attention to the real gold.\nWhich is \u2192 \"\ud835\uddd9\ud835\uddfc\ud835\uddf0\ud835\ude02\ud835\ude00 \ud835\uddfc\ud835\uddfb \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddf3\ud835\ude02\ud835\uddfb\ud835\uddf1\ud835\uddee\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\uddee\ud835\uddf9\ud835\ude00.\"\n.\nLet me explain \u2193\nWhen you constantly chase the latest models (aka FOMO), you will only have a shallow understanding of that new information (except if you are a genius or already deep into that niche).\nBut the joke's on you. In reality, most of what you think you need to know, you don't.\nSo you won't use what you learned and forget most of it after 1-2 months.\nWhat a waste of time, right?\n.\nBut...\nIf you master the fundamentals of the topic, you want to learn.\nFor example, for deep learning, you have to know:\n- how models are built\n- how they are trained\n- groundbreaking architectures (Resnet, UNet, Transformers, etc.)\n- parallel training\n- deploying a model, etc.\n...when in need (e.g., you just moved on to a new project), you can easily pick up the latest research.\nThus, after you have laid the foundation, it is straightforward to learn SoTA approaches when needed (if needed).\nMost importantly, what you learn will stick with you, and you will have the flexibility to jump from one project to another quickly.\n.\nI am also guilty. I used to FOMO into all kinds of topics until I was honest with myself and admitted I am no Leonardo Da Vinci.\nBut here is what I did and worked well:\n- building projects\n- replicating the implementations of famous papers\n- teaching the subject I want to learn\n... and most importantly, take my time to relax and internalize the information.\n.\nTo conclude:\n- learn ahead only the fundamentals\n- learn the latest trend only when needed\nWhat is your learning strategy? Let me know in the comments \u2193\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\npersonaldevelopment\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    "Post_45": {
        "text": "Here is one thing that I do that sets me apart from the crowd:\n\"\ud835\ude10 \ud835\ude22\ud835\ude2e \ud835\ude30\ud835\ude2c\ud835\ude22\ud835\ude3a \ud835\ude38\ud835\ude2a\ud835\ude35\ud835\ude29 \ud835\ude23\ud835\ude26\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude35\ud835\ude29\ud835\ude26 \ud835\ude25\ud835\ude36\ud835\ude2e\ud835\ude31 \ud835\ude30\ud835\ude2f\ud835\ude26 \ud835\ude35\ud835\ude29\ud835\ude22\ud835\ude35 \ud835\ude22\ud835\ude34\ud835\ude2c\ud835\ude34 \ud835\ude2e\ud835\ude22\ud835\ude2f\ud835\ude3a \ud835\ude32\ud835\ude36\ud835\ude26\ud835\ude34\ud835\ude35\ud835\ude2a\ud835\ude30\ud835\ude2f\ud835\ude34.\"\n\ud835\udc07\ud835\udc26\ud835\udc26... \ud835\udc16\ud835\udc21\ud835\udc32?\nThe reality is that even the brightest minds cannot understand everything from the first shot.\nIt is not necessarily that you cannot understand the concepts.\nThere are other factors, such as:\n- you are tired\n- you haven't paid enough attention\n- the concept wasn't explained at your level\n- the presenter wasn't clear enough, etc.\nAlso, the truth is that many of us don't understand everything from the first shot when presented with a new concept.\nBut because of our ego, we are afraid to come out and ask something because we are worried that we will sound stupid.\nThe jokes are on you.\nMost people will be grateful you broke the ice and asked to explain the concept again.\n\ud835\udc16\ud835\udc21\ud835\udc32?\nIt will help the team to learn the new concepts better.\nIt will start a discussion to dig deeper into the subject.\nIt will piss off or annoy the people you don't like.\nIt will help other people ask questions next time.\nIt will open up new perspectives on the problem.\nTo conclude...\nIgnore your ego and what people think of you. Own your curiosity and ask questions when you feel like it.\nIt is ok not to know everything.\nIt is better to be stupid for 5 minutes than your entire life.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlop\nhashtag\n#\npersonaldevelopment\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    "Post_46": {
        "text": "The whole field of prompt engineering can be reduced to these 6 techniques I use almost daily when using ChatGPT (or other LLMs).\nHere they are \u2193\n#1. \ud835\udc05\ud835\udc1e\ud835\udc30 \ud835\udc2c\ud835\udc21\ud835\udc28\ud835\udc2d \ud835\udc29\ud835\udc2b\ud835\udc28\ud835\udc26\ud835\udc29\ud835\udc2d\ud835\udc22\ud835\udc27\ud835\udc20\nAdd in your prompt 2 or 3 high-quality demonstrations, each consisting of both input and desired output, on the target task.\nThe LLM will better understand your intention and what kind of answers you expect based on concrete examples.\n#2. \ud835\udc12\ud835\udc1e\ud835\udc25\ud835\udc1f-\ud835\udc1c\ud835\udc28\ud835\udc27\ud835\udc2c\ud835\udc22\ud835\udc2c\ud835\udc2d\ud835\udc1e\ud835\udc27\ud835\udc1c\ud835\udc32 \ud835\udc2c\ud835\udc1a\ud835\udc26\ud835\udc29\ud835\udc25\ud835\udc22\ud835\udc27\ud835\udc20\nSample multiple outputs with \"temperature > 0\" and select the best one out of these candidates.\nHow to pick the best candidate?\nIt will vary from task to task, but here are  2 primary scenarios \u2193\n1. Some tasks are easy to validate, such as programming questions. In this case, you can write unit tests to verify the correctness of the generated code.\n2. For more complicated tasks, you can manually inspect them or use another LLM (or another specialized model) to rank them.\n#3. \ud835\udc02\ud835\udc21\ud835\udc1a\ud835\udc22\ud835\udc27-\ud835\udc28\ud835\udc1f-\ud835\udc13\ud835\udc21\ud835\udc28\ud835\udc2e\ud835\udc20\ud835\udc21\ud835\udc2d (\ud835\udc02\ud835\udc28\ud835\udc13)\nYou want to force the LLM to explain its thought process, which eventually leads to the final answer, step by step.\nThis will help the LLM to reason complex tasks better.\nYou want to use CoT for complicated reasoning tasks + large models (e.g., with more than 50B parameters). Simple tasks only benefit slightly from CoT prompting.\nHere are a few methods to achieve CoT:\n- provide a list of bullet points with all the steps you expect the LLM to take\n- use \"Few shot prompt\" to teach the LLM to think in steps\n... or my favorite: use sentences such as \"Let's think step by step.\"\n#4. \ud835\udc00\ud835\udc2e\ud835\udc20\ud835\udc26\ud835\udc1e\ud835\udc27\ud835\udc2d\ud835\udc1e\ud835\udc1d \ud835\udc0f\ud835\udc2b\ud835\udc28\ud835\udc26\ud835\udc29\ud835\udc2d\ud835\udc2c\nThe LLM's internal knowledge is limited to the data it was trained on. Also, often, it forgets specific details of older training datasets.\nThe most common use case is Retrieval-Augmented Generation (RAG).\nThat is why using the LLM as a reasoning engine is beneficial to parse and extract information from a reliable source of information given as context in the prompt.\n\ud835\ude1e\ud835\ude29\ud835\ude3a?\n- avoid retraining the model on new data\n- avoid hallucinating\n- access to references on the source\n#5. \ud835\udc00 \ud835\udc2c\ud835\udc22\ud835\udc27\ud835\udc20\ud835\udc25\ud835\udc1e \ud835\udc2b\ud835\udc1e\ud835\udc2c\ud835\udc29\ud835\udc28\ud835\udc27\ud835\udc2c\ud835\udc22\ud835\udc1b\ud835\udc22\ud835\udc25\ud835\udc22\ud835\udc2d\ud835\udc32 \ud835\udc29\ud835\udc1e\ud835\udc2b \ud835\udc29\ud835\udc2b\ud835\udc28\ud835\udc26\ud835\udc29\ud835\udc2d\nQuite self-explanatory. It is similar to the DRY principle in SWE.\nHaving only x1 task/prompt is good practice to avoid confusing the LLM.\nIf you have more complex tasks, split them into granular ones and merge the results later in a different prompt.\n#6. \ud835\udc01\ud835\udc1e \ud835\udc1a\ud835\udc2c \ud835\udc1e\ud835\udc31\ud835\udc29\ud835\udc25\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc2d \ud835\udc1a\ud835\udc2c \ud835\udc29\ud835\udc28\ud835\udc2c\ud835\udc2c\ud835\udc22\ud835\udc1b\ud835\udc25\ud835\udc1e\nThe LLM cannot read your mind. To maximize the probability of getting precisely what you want, you can imagine the LLM as a 7-year-old to whom you must explain everything step-by-step to be sure he understood.\n\ud835\ude15\ud835\ude30\ud835\ude35\ud835\ude26: The level of detail in the prompt is inversely proportional to the size & complexity of the model.\n.\nThe truth is that prompt engineering is quite intuitive, and we don't have to overthink it too much.\nWhat would you add to this list?\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience"
    },
    "Post_47": {
        "text": "Want to build your first \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf7\ud835\uddf2\ud835\uddf0\ud835\ude01 but don't know where to start?\nHere are \ud835\udff0 \ud835\uddd9\ud835\udde5\ud835\uddd8\ud835\uddd8 \ud835\uddf9\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2\ud835\ude00 to put you on the right track \u2193\n#1. \ud835\udc05\ud835\udc22\ud835\udc27\ud835\udc1e-\ud835\udc2d\ud835\udc2e\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc29\ud835\udc22\ud835\udc29\ud835\udc1e\ud835\udc25\ud835\udc22\ud835\udc27\ud835\udc1e \ud835\udc1f\ud835\udc28\ud835\udc2b \ud835\udc28\ud835\udc29\ud835\udc1e\ud835\udc27-\ud835\udc2c\ud835\udc28\ud835\udc2e\ud835\udc2b\ud835\udc1c\ud835\udc1e \ud835\udc0b\ud835\udc0b\ud835\udc0c\ud835\udc2c\nYou will learn:\n- What is model fine-tuning?\n- Why is it useful?\n- When to use it?\n- Why to fine-tune an LLM using QLoRA\n- How to architect a fine-tuning pipeline in a real-world project\n\ud83c\udfac  Video Lecture 1: Fine-tuning pipeline (Part 1) - \ud83d\udd17\nhttps://lnkd.in/dAKnwjK2\n#2. \ud835\udc07\ud835\udc1a\ud835\udc27\ud835\udc1d\ud835\udc2c-\ud835\udc28\ud835\udc27 \ud835\udc1f\ud835\udc22\ud835\udc27\ud835\udc1e-\ud835\udc2d\ud835\udc2e\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20\nLet's apply what we learned in lesson 1 to build our first fine-tuning pipeline.\n\ud83c\udfac Video Lecture 2: Fine-tuning pipeline (Part 2) - \ud83d\udd17\nhttps://lnkd.in/dJxyFY4D\n#3. \ud835\udc01\ud835\udc2e\ud835\udc22\ud835\udc25\ud835\udc1d & \ud835\udc1d\ud835\udc1e\ud835\udc29\ud835\udc25\ud835\udc28\ud835\udc32 \ud835\udc1a \ud835\udc2b\ud835\udc1e\ud835\udc1a\ud835\udc25-\ud835\udc2d\ud835\udc22\ud835\udc26\ud835\udc1e \ud835\udc2c\ud835\udc2d\ud835\udc2b\ud835\udc1e\ud835\udc1a\ud835\udc26\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc29\ud835\udc22\ud835\udc29\ud835\udc1e\ud835\udc25\ud835\udc22\ud835\udc27\ud835\udc1e\nYou will learn:\n- How to transform HTML docs into vector embeddings.\n- How to process data in real-time\n- How to store & retrieve embeddings from a vector DB\n- How to deploy it to AWS.\n\ud83c\udfac Video Lecture 3: Streaming pipeline  - \ud83d\udd17\nhttps://lnkd.in/d7t3kwMc\n#4. \ud835\udc08\ud835\udc27\ud835\udc1f\ud835\udc1e\ud835\udc2b\ud835\udc1e\ud835\udc27\ud835\udc1c\ud835\udc1e \ud835\udc29\ud835\udc22\ud835\udc29\ud835\udc1e\ud835\udc25\ud835\udc22\ud835\udc27\ud835\udc1e\nFinally, you will learn how to use LangChain to glue together your fine-tuned LLM and your financial news stored as embeddings in a vector DB to serve predictions behind a RESTful API.\n\ud83c\udfac  Video Lecture 4: Inference pipeline  - \ud83d\udd17\nhttps://lnkd.in/de3mVezq\n.\nThese 4 lectures are part of the open-source and FREE Hands-on LLM Course by Pau Labarta, Alexandru R\u0103zvan\u021b \ud83d\udc4b and myself.\n\u21b3 Check out the Hands-on LLMs course on GitHub and support us with a \u2b50:  \ud83d\udd17\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n\ud83d\udca1 Follow me for daily lessons about ML engineering and MLOps."
    },
    "Post_48": {
        "text": "To successfully use \ud835\udde5\ud835\uddd4\ud835\uddda in your \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\uddee\ud835\uddfd\ud835\uddfd\ud835\uddf9\ud835\uddf6\ud835\uddf0\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb\ud835\ude00, your \ud835\ude03\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddfc\ud835\uddff \ud835\uddd7\ud835\uddd5 must constantly be updated with the latest data.\nHere is how you can implement a \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddfd\ud835\uddf6\ud835\uddfd\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 to keep your vector DB in sync with your datasets \u2193\n.\n\ud835\udde5\ud835\uddd4\ud835\uddda is a popular strategy when building LLMs to add context to your prompt about your private datasets.\nLeveraging your domain data using RAG provides 2 significant benefits:\n- you don't need to fine-tune your model as often (or at all)\n- avoid hallucinations\n.\nOn the \ud835\uddef\ud835\uddfc\ud835\ude01 \ud835\ude00\ud835\uddf6\ud835\uddf1\ud835\uddf2, to implement RAG, you have to:\n3. Embed the user's question using an embedding model (e.g., BERT). Use the embedding to query your vector DB and find the most similar vectors using a distance function (e.g., cos similarity).\n4. Get the top N closest vectors and their metadata.\n5. Attach the extracted top N vectors metadata + the chat history to the input prompt.\n6. Pass the prompt to the LLM.\n7. Insert the user question + assistant answer to the chat history.\n.\nBut the question is, \ud835\uddf5\ud835\uddfc\ud835\ude04 do you \ud835\uddf8\ud835\uddf2\ud835\uddf2\ud835\uddfd \ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff \ud835\ude03\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddfc\ud835\uddff \ud835\uddd7\ud835\uddd5 \ud835\ude02\ud835\uddfd \ud835\ude01\ud835\uddfc \ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddf2 \ud835\ude04\ud835\uddf6\ud835\ude01\ud835\uddf5 \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddf9\ud835\uddee\ud835\ude01\ud835\uddf2\ud835\ude00\ud835\ude01 \ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddee?\n\u21b3 You need a real-time streaming pipeline.\nHow do you implement it?\nYou need 2 components:\n\u21b3 A streaming processing framework. For example, Bytewax is built in Rust for efficiency and exposes a Python interface for ease of use - you don't need Java to implement real-time pipelines anymore.\n\ud83d\udd17 Bytewax:\nhttps://lnkd.in/dWJytkZ5\n\u21b3 A vector DB. For example, Qdrant provides a rich set of features and a seamless experience.\n\ud83d\udd17 Qdrant:\nhttps://lnkd.in/dtZuyiBp\n.\nHere is an example of how to implement a streaming pipeline for financial news \u2193\n#\ud835\udfed. Financial news data source (e.g., Alpaca):\nTo populate your vector DB, you need a historical API (e.g., RESTful API) to add data to your vector DB in batch mode between a desired [start_date, end_date] range. You can tweak the number of workers to parallelize this step as much as possible.\n\u2192 You run this once in the beginning.\nYou need the data exposed under a web socket to ingest news in real-time. So, you'll be able to listen to the news and ingest it in your vector DB as soon as they are available.\n\u2192 Listens 24/7 for financial news.\n#\ud835\udfee. Build the streaming pipeline using Bytewax:\nImplement 2 input connectors for the 2 different types of APIs: RESTful API & web socket.\nThe rest of the steps can be shared between both connectors \u2193\n- Clean financial news documents.\n- Chunk the documents.\n- Embed the documents (e.g., using Bert).\n- Insert the embedded documents + their metadata to the vector DB (e.g., Qdrant).\n#\ud835\udfef-\ud835\udff3. When the users ask a financial question, you can leverage RAG with an up-to-date vector DB to search for the latest news in the industry.\nBytewax and Qdrant make this easy \ud83d\udd25\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning"
    }
}